{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import hnswlib\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from heapq import heappush, heappop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors of dimension 100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Path to your GloVe file (update this based on your downloaded version)\n",
    "glove_path = '/Users/tanishqchaudhari/Desktop/DataScience Proj  datasets/Dataset/glove.6B.100d.txt'\n",
    "\n",
    "# Load GloVe vectors\n",
    "word_to_vec = {}\n",
    "words = []\n",
    "vectors = []\n",
    "\n",
    "with open(glove_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]  # First token is the word\n",
    "        vector = np.array(values[1:], dtype=np.float32)  # Rest are vector values\n",
    "        word_to_vec[word] = vector\n",
    "        words.append(word)\n",
    "        vectors.append(vector)\n",
    "\n",
    "# Convert to numpy array\n",
    "vectors = np.array(vectors, dtype=np.float32)\n",
    "print(f\"Loaded {len(words)} word vectors of dimension {vectors.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding nearest neighbors: 100%|██████████| 40000/40000 [00:00<00:00, 54418.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored nearest neighbors for 38055 queries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Generate 10^ random query indices\n",
    "num_queries = 40000\n",
    "query_indices = np.random.randint(0, len(words), size=num_queries)\n",
    "\n",
    "# Step 2: Get the corresponding query vectors\n",
    "query_vectors = vectors[query_indices]\n",
    "\n",
    "# Step 3: Use NearestNeighbors to find 100 nearest neighbors\n",
    "k = 100\n",
    "nn = NearestNeighbors(n_neighbors=k, algorithm='auto', metric='l2')\n",
    "nn.fit(vectors)\n",
    "\n",
    "# Step 4: For each query, get the indices of the 100 nearest neighbors\n",
    "distances, neighbor_indices = nn.kneighbors(query_vectors)\n",
    "\n",
    "# Step 5: Store results in a dictionary {query_word: [neighbor_words]}\n",
    "query_to_neighbors = {}\n",
    "\n",
    "\n",
    "\n",
    "# Step 5: Store results in a dictionary {query_word: [neighbor_words]}\n",
    "query_to_neighbors = {}\n",
    "for i in tqdm(range(len(query_indices)), desc=\"Finding nearest neighbors\"):\n",
    "    query_idx = query_indices[i]\n",
    "    query_word = words[query_idx]\n",
    "    neighbor_words = [words[idx] for idx in neighbor_indices[i]]\n",
    "    query_to_neighbors[query_word] = neighbor_words\n",
    "\n",
    "print(f\"Stored nearest neighbors for {len(query_to_neighbors)} queries.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding nearest neighbors: 100%|██████████| 40000/40000 [00:00<00:00, 2222442.18it/s]\n"
     ]
    }
   ],
   "source": [
    "query_to_neighbors_vectors = {}\n",
    "\n",
    "for i in tqdm(range(len(query_indices)), desc=\"Finding nearest neighbors\"):\n",
    "    query_idx = query_indices[i]\n",
    "    neighbor_indices_list = neighbor_indices[i]\n",
    "    query_to_neighbors_vectors[query_idx] = neighbor_indices_list  # All indices, not vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# Flatten all neighbor indices into a single list\n",
    "all_neighbors = [idx for neighbors in query_to_neighbors_vectors.values() for idx in neighbors]\n",
    "\n",
    "# Count frequency of each neighbor index\n",
    "freq_counter = Counter(all_neighbors)\n",
    "\n",
    "# Create DataFrame and sort\n",
    "freq_df = pd.DataFrame(freq_counter.items(), columns=[\"Index\", \"Frequency\"]).sort_values(\"Frequency\", ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average frequency of neighbors: 11.96701876420995\n"
     ]
    }
   ],
   "source": [
    "#printing the average\n",
    "average = freq_df[\"Frequency\"].mean()\n",
    "print(f\"Average frequency of neighbors: {average}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hubs(\n",
    "    neighbor_indices: np.ndarray,\n",
    "    distances:        np.ndarray,\n",
    "    freq_df:          pd.DataFrame,\n",
    "    alpha:            float = 0.5,\n",
    "    method:           str   = \"percentile\",\n",
    "    threshold:        float = 95.0,\n",
    "    top_k:            int   = None\n",
    ") -> (pd.Series, set):\n",
    "    \"\"\"\n",
    "    Compute combined hub‐scores f_i and select hub items H.\n",
    "\n",
    "    Inputs:\n",
    "      - neighbor_indices: shape (5000,100) np.int array of item‑IDs\n",
    "      - distances:        shape (5000,100) np.float array of distances\n",
    "      - freq_df:          pd.DataFrame with columns [\"Index\",\"Frequency\"]\n",
    "                          where Frequency = c_i = #queries whose top‑100 includes item i\n",
    "      - alpha:            weight in [0,1] between freq and distance\n",
    "      - method:           \"percentile\", \"top_k\", or \"stddev\"\n",
    "      - threshold:        percentile p (if method=\"percentile\"), or\n",
    "                          k (if method=\"stddev\" meaning μ + k·σ)\n",
    "      - top_k:            required if method=\"top_k\"\n",
    "\n",
    "    Returns:\n",
    "      - combined:   pd.Series indexed by item‑ID, values f_i ∈ [0,1]\n",
    "      - hub_set:    set of item‑IDs chosen as hubs\n",
    "    \"\"\"\n",
    "\n",
    "    # --- 1) Normalize frequency via min–max over c_i ---\n",
    "    # freq_df[\"Index\"] holds item‑IDs, freq_df[\"Frequency\"] holds c_i\n",
    "    freq_series = freq_df.set_index(\"Index\")[\"Frequency\"].astype(float)\n",
    "    c_min, c_max = freq_series.min(), freq_series.max()\n",
    "    if c_max > c_min:\n",
    "        f_freq = (freq_series - c_min) / (c_max - c_min)\n",
    "    else:\n",
    "        f_freq = pd.Series(0.0, index=freq_series.index)\n",
    "\n",
    "    # --- 2) Compute average distance d̄_i and normalize/flip to get f_dist ---\n",
    "    df = pd.DataFrame({\n",
    "        \"Index\":    neighbor_indices.ravel(),\n",
    "        \"Distance\": distances.ravel()\n",
    "    })\n",
    "    d_bar = df.groupby(\"Index\")[\"Distance\"].mean()\n",
    "    d_min, d_max = d_bar.min(), d_bar.max()\n",
    "    if d_max > d_min:\n",
    "        d_tilde = (d_bar - d_min) / (d_max - d_min)\n",
    "    else:\n",
    "        d_tilde = pd.Series(0.0, index=d_bar.index)\n",
    "    f_dist = 1.0 - d_tilde\n",
    "\n",
    "    # --- 3) Combine scores on the intersection of items ---\n",
    "    common = f_freq.index.intersection(f_dist.index)\n",
    "    combined = alpha * f_freq.loc[common] + (1 - alpha) * f_dist.loc[common]\n",
    "    combined.name = \"f_i\"\n",
    "    print(f_dist.loc[common])\n",
    "    # --- 4) Select hubs ---\n",
    "    if method == \"percentile\":\n",
    "        cutoff = np.percentile(combined.values, threshold)\n",
    "        hub_set = set(combined[combined >= cutoff].index)\n",
    "\n",
    "    elif method == \"stddev\":\n",
    "        mu, sigma = combined.mean(), combined.std()\n",
    "        cutoff = mu + threshold * sigma\n",
    "        hub_set = set(combined[combined >= cutoff].index)\n",
    "\n",
    "    elif method == \"top_k\":\n",
    "        if top_k is None:\n",
    "            raise ValueError(\"top_k must be provided when method='top_k'\")\n",
    "        hub_set = set(combined.nlargest(top_k).index)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"unknown method {method!r}\")\n",
    "    \n",
    "    return combined, hub_set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index\n",
      "154957    0.732232\n",
      "289249    0.738054\n",
      "364978    0.739057\n",
      "224673    0.732303\n",
      "262208    0.744023\n",
      "            ...   \n",
      "103030    0.548729\n",
      "85487     0.540763\n",
      "71104     0.523096\n",
      "271552    0.510760\n",
      "49582     1.000000\n",
      "Name: Distance, Length: 317999, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "combined_scores, hubs = compute_hubs(\n",
    "    neighbor_indices,\n",
    "    distances,\n",
    "    freq_df,\n",
    "    alpha=0.5,\n",
    "    method=\"percentile\",\n",
    "    threshold=95.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from heapq import heappush, heappop\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HNSW CLASS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obj Of HNSW CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _distance(self, i, j):\n",
    "    return np.linalg.norm(self.vectors[i] - self.vectors[j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from heapq import heappush, heappop\n",
    "import random\n",
    "import math\n",
    "\n",
    "class HNSW:\n",
    "    \"\"\"\n",
    "    Standard HNSW implementation based on the provided code [1],\n",
    "    with hub-related modifications removed.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, max_elements, M=16, ef_construction=200):\n",
    "        \"\"\"\n",
    "        Initializes the HNSW index.\n",
    "\n",
    "        Args:\n",
    "            dim (int): Dimensionality of the vectors.\n",
    "            max_elements (int): Estimated maximum number of elements (informational).\n",
    "            M (int): Maximum number of connections per node per layer.\n",
    "            ef_construction (int): Size of the dynamic candidate list during construction.\n",
    "        \"\"\"\n",
    "        self.dim = dim\n",
    "        self.max_elements = max_elements # Informational\n",
    "        self.M = M\n",
    "        self.ef_construction = ef_construction\n",
    "\n",
    "        # Store graph layers: layers[l] is a dict {node_idx: [neighbor_indices]}\n",
    "        self.layers = []\n",
    "        # Store the actual vectors, index corresponds to node_idx\n",
    "        self.vectors = []\n",
    "        # Track the entry point (index of the node in the highest layer)\n",
    "        self.entry_point = None\n",
    "        # Precompute layer multiplier\n",
    "        self.ml = 1 / math.log(self.M) if self.M > 1 else 1\n",
    "\n",
    "\n",
    "    def _get_layer(self):\n",
    "        \"\"\"\n",
    "        Determine the layer for a new node based on exponential decay\n",
    "        using the precomputed multiplier (mL).\n",
    "\n",
    "        Returns:\n",
    "            int: The selected layer index (>= 0).\n",
    "        \"\"\"\n",
    "        # Calculate layer using the standard HNSW formula\n",
    "        layer = max(0, int(-math.log(random.random()) * self.ml))\n",
    "        return layer\n",
    "\n",
    "    # --- _distance method remains unchanged ---\n",
    "    def _distance(self, idx1, idx2):\n",
    "        \"\"\"Calculate Euclidean distance between two vectors by index.\"\"\"\n",
    "        if not (0 <= idx1 < len(self.vectors) and 0 <= idx2 < len(self.vectors)):\n",
    "            # Handle cases where one or both indices are out of bounds\n",
    "            # This might happen during pruning if a node was considered but\n",
    "            # doesn't actually exist in the current vector list state.\n",
    "            return float('inf')\n",
    "        try:\n",
    "            return np.linalg.norm(self.vectors[idx1] - self.vectors[idx2])\n",
    "        except IndexError:\n",
    "             # Should ideally not happen if bounds check passes, but as extra safety\n",
    "             return float('inf')\n",
    "\n",
    "\n",
    "    # --- _search_layer method remains unchanged ---\n",
    "    # (Includes robustness checks from original code [1])\n",
    "    def _search_layer(self, query_vec, layer_idx, ep_idx, ef):\n",
    "        \"\"\"Search within a specific layer starting from an entry point.\"\"\"\n",
    "        # Basic layer validity check\n",
    "        if layer_idx < 0 or layer_idx >= len(self.layers):\n",
    "            return []\n",
    "        layer_graph = self.layers[layer_idx]\n",
    "        if not layer_graph: # Layer exists but is empty\n",
    "            return []\n",
    "\n",
    "        # Robustness: Check if entry point is valid for this layer and vector list\n",
    "        if ep_idx not in layer_graph or ep_idx >= len(self.vectors):\n",
    "            # If invalid, try to find a random valid node in the layer as a fallback EP\n",
    "            try:\n",
    "                valid_indices = [idx for idx in layer_graph if idx < len(self.vectors)]\n",
    "                if not valid_indices:\n",
    "                    return [] # No valid nodes in this layer\n",
    "                ep_idx = random.choice(valid_indices) # Choose a random valid node\n",
    "            except IndexError: # Should not happen if valid_indices check passed\n",
    "                return []\n",
    "\n",
    "        visited = set()\n",
    "        candidates = [] # Min-heap: (distance, node_idx)\n",
    "        results = []    # Max-heap: (-distance, node_idx)\n",
    "\n",
    "        # Initialize search with the entry point\n",
    "        try:\n",
    "             # Ensure vector exists before calculating distance\n",
    "             if ep_idx >= len(self.vectors):\n",
    "                 # This case should be caught by the initial check, but for safety:\n",
    "                 return []\n",
    "             initial_dist = np.linalg.norm(query_vec - self.vectors[ep_idx])\n",
    "             heappush(candidates, (initial_dist, ep_idx))\n",
    "             heappush(results, (-initial_dist, ep_idx))\n",
    "             visited.add(ep_idx)\n",
    "        except IndexError:\n",
    "             # Fallback if vector access fails unexpectedly\n",
    "             # print(f\"Warning: IndexError accessing vector for entry point {ep_idx}...\") # Optional warning\n",
    "             return []\n",
    "\n",
    "        # Greedy search loop\n",
    "        while candidates:\n",
    "            try:\n",
    "                dist_candidate, current_idx = heappop(candidates)\n",
    "            except IndexError: # Heap is empty\n",
    "                break\n",
    "\n",
    "            # Get distance of the farthest node found so far\n",
    "            farthest_dist_in_results = -results[0][0] if results else float('inf')\n",
    "\n",
    "            # Early termination condition\n",
    "            if dist_candidate > farthest_dist_in_results and len(results) >= ef:\n",
    "                 break # All remaining candidates are farther than the worst result\n",
    "\n",
    "            # Check if current node is still valid (might be needed with concurrent modifications, less so here)\n",
    "            # Also check if it exists in the current layer graph\n",
    "            if current_idx not in layer_graph or current_idx >= len(self.vectors):\n",
    "                continue # Skip if node became invalid or is not in this layer\n",
    "\n",
    "            # Explore neighbors\n",
    "            for neighbor_idx in layer_graph.get(current_idx, []):\n",
    "                 # Check if neighbor is valid and not visited\n",
    "                 if neighbor_idx not in visited and neighbor_idx < len(self.vectors):\n",
    "                    visited.add(neighbor_idx)\n",
    "                    try:\n",
    "                        dist_neighbor = np.linalg.norm(query_vec - self.vectors[neighbor_idx])\n",
    "                    except IndexError:\n",
    "                        continue # Skip if neighbor vector access fails\n",
    "\n",
    "                    # Get updated farthest distance\n",
    "                    farthest_dist_in_results = -results[0][0] if results else float('inf')\n",
    "\n",
    "                    # If neighbor is closer than farthest result or results list is not full\n",
    "                    if dist_neighbor < farthest_dist_in_results or len(results) < ef:\n",
    "                        # Add to results (maintaining max-heap property)\n",
    "                        heappush(results, (-dist_neighbor, neighbor_idx))\n",
    "                        # If results exceed ef, remove the farthest\n",
    "                        if len(results) > ef:\n",
    "                            heappop(results)\n",
    "                        # Add neighbor to candidates for further exploration\n",
    "                        heappush(candidates, (dist_neighbor, neighbor_idx))\n",
    "\n",
    "        # Convert max-heap results to sorted list (distance, idx)\n",
    "        final_results = sorted([(-d, idx) for d, idx in results])\n",
    "        return final_results[:ef] # Return top ef results\n",
    "\n",
    "\n",
    "    def insert(self, vector):\n",
    "        \"\"\"Insert a vector into the HNSW index.\"\"\"\n",
    "        new_idx = len(self.vectors) # Index for the new vector\n",
    "        self.vectors.append(np.array(vector)) # Store the vector\n",
    "\n",
    "        # Determine the layers the new node will belong to\n",
    "        node_max_layer = self._get_layer() # Use standard layer selection\n",
    "\n",
    "        current_ep_idx = self.entry_point\n",
    "\n",
    "        # Ensure enough layers exist in the graph structure\n",
    "        while len(self.layers) <= node_max_layer:\n",
    "            self.layers.append(dict()) # Add new empty layer dictionaries\n",
    "\n",
    "        # --- Rest of insert method unchanged from original code [1] ---\n",
    "        # (Includes robustness checks)\n",
    "        current_insert_vec = self.vectors[new_idx] # Use the newly added vector\n",
    "\n",
    "        # Phase 1: Find entry points in upper layers (down to node_max_layer + 1)\n",
    "        for l in range(len(self.layers) - 1, node_max_layer, -1):\n",
    "             if current_ep_idx is None: break # Cannot proceed without an entry point\n",
    "             if not self.layers[l]: continue # Skip empty layers\n",
    "\n",
    "             # Ensure entry point validity before search in this layer\n",
    "             if current_ep_idx not in self.layers[l] or current_ep_idx >= len(self.vectors):\n",
    "                 try:\n",
    "                    # Fallback: Find a random valid node in the layer\n",
    "                    valid_indices = [idx for idx in self.layers[l] if idx < len(self.vectors)]\n",
    "                    if not valid_indices: continue # Skip layer if no valid nodes\n",
    "                    current_ep_idx = random.choice(valid_indices)\n",
    "                 except IndexError: continue # Should not happen\n",
    "\n",
    "             # Search for the closest node in layer l to the new vector (ef=1)\n",
    "             search_results = self._search_layer(current_insert_vec, l, current_ep_idx, ef=1)\n",
    "             if search_results:\n",
    "                 # Update the entry point for the next lower layer\n",
    "                 found_ep_idx = search_results[0][1]\n",
    "                 # Check validity before assignment\n",
    "                 if found_ep_idx < len(self.vectors):\n",
    "                      current_ep_idx = found_ep_idx\n",
    "\n",
    "        # Phase 2: Insert node in layers node_max_layer down to 0\n",
    "        for l in range(min(node_max_layer, len(self.layers) - 1), -1, -1):\n",
    "            layer_graph = self.layers[l]\n",
    "\n",
    "            # Ensure valid entry point for the search in this layer\n",
    "            if current_ep_idx is None or current_ep_idx not in layer_graph or current_ep_idx >= len(self.vectors):\n",
    "                 if layer_graph: # If the layer is not empty\n",
    "                     try:\n",
    "                         # Fallback: Find a random valid node\n",
    "                         valid_indices = [idx for idx in layer_graph if idx < len(self.vectors)]\n",
    "                         if not valid_indices: current_ep_idx = None # No valid EPs possible\n",
    "                         else: current_ep_idx = random.choice(valid_indices)\n",
    "                     except IndexError: current_ep_idx = None\n",
    "                 else: current_ep_idx = None # Layer is empty, no EP\n",
    "\n",
    "            # Find neighbors using search_layer with ef_construction\n",
    "            if current_ep_idx is None:\n",
    "                 neighbors = [] # No entry point, cannot search\n",
    "            else:\n",
    "                 # Search for ef_construction nearest neighbors\n",
    "                 neighbors = self._search_layer(current_insert_vec, l, current_ep_idx, self.ef_construction)\n",
    "                 # Update entry point for next layer (closest neighbor found)\n",
    "                 if neighbors:\n",
    "                      found_ep_idx = neighbors[0][1]\n",
    "                      # Check validity before assignment\n",
    "                      if found_ep_idx < len(self.vectors):\n",
    "                           current_ep_idx = found_ep_idx\n",
    "\n",
    "            # Select M best neighbors based on distance\n",
    "            connections = [idx for dist, idx in neighbors[:self.M]]\n",
    "            # Add connections for the new node in this layer\n",
    "            layer_graph[new_idx] = connections\n",
    "\n",
    "            # Add backlinks from neighbors to the new node, maintaining M limit\n",
    "            for neighbor_idx in connections:\n",
    "                 # Ensure neighbor exists and is valid before modifying its connections\n",
    "                 if neighbor_idx in layer_graph and neighbor_idx < len(self.vectors):\n",
    "                     neighbor_connections = layer_graph[neighbor_idx]\n",
    "                     if new_idx not in neighbor_connections: # Avoid duplicate connections\n",
    "                          neighbor_connections.append(new_idx)\n",
    "                          # Prune connections if exceeding M\n",
    "                          if len(neighbor_connections) > self.M:\n",
    "                               # Ensure all connections are valid before distance calculation for pruning\n",
    "                               valid_conns = [cidx for cidx in neighbor_connections if cidx < len(self.vectors)]\n",
    "                               if len(valid_conns) < len(neighbor_connections):\n",
    "                                   # If some connections became invalid, just update the list\n",
    "                                   layer_graph[neighbor_idx] = valid_conns\n",
    "                                   # Re-check length; might not need pruning anymore\n",
    "                                   if len(valid_conns) <= self.M:\n",
    "                                        continue # Skip pruning if count is now okay\n",
    "\n",
    "                               # Calculate distances only for valid connections\n",
    "                               distances = []\n",
    "                               for conn_idx in valid_conns:\n",
    "                                   dist = self._distance(neighbor_idx, conn_idx)\n",
    "                                   if dist != float('inf'): # Only consider valid distances\n",
    "                                       distances.append((dist, conn_idx))\n",
    "\n",
    "                               # Sort by distance and keep the M closest\n",
    "                               distances.sort()\n",
    "                               layer_graph[neighbor_idx] = [idx for dist, idx in distances[:self.M]]\n",
    "\n",
    "        # Update the global entry point if the new node is in a higher layer\n",
    "        current_ep_layer = self._get_node_layer(self.entry_point) # Find current EP's highest layer\n",
    "        if self.entry_point is None or node_max_layer > current_ep_layer:\n",
    "            self.entry_point = new_idx\n",
    "\n",
    "\n",
    "    # --- Helper method to find the highest layer a node exists in ---\n",
    "    def _get_node_layer(self, node_idx):\n",
    "        \"\"\"Helper to find the highest layer index a node exists in.\"\"\"\n",
    "        if node_idx is None or node_idx < 0: return -1\n",
    "        for l in range(len(self.layers) - 1, -1, -1):\n",
    "             # Check if layer exists and node is in the layer's keys\n",
    "             if l < len(self.layers) and node_idx in self.layers[l]:\n",
    "                 return l\n",
    "        return -1 # Node not found in any layer\n",
    "\n",
    "\n",
    "    # --- search method remains unchanged ---\n",
    "    # (Includes robustness checks from original code [1])\n",
    "    def search(self, query_vec, k=10):\n",
    "        \"\"\"Search for the k nearest neighbors of query_vec.\"\"\"\n",
    "        # Determine ef for search (at least k)\n",
    "        ef_search = max(self.ef_construction, k)\n",
    "        current_ep_idx = self.entry_point\n",
    "\n",
    "        # Check if entry point is valid at the start\n",
    "        if current_ep_idx is None or current_ep_idx >= len(self.vectors):\n",
    "            #print(\"Warning: Entry point is None or invalid at search start.\") # Optional warning\n",
    "            # Potentially try to find *any* valid node if layers exist?\n",
    "            # For now, return empty if no valid starting point.\n",
    "            return []\n",
    "\n",
    "        query_vec = np.array(query_vec) # Ensure query is a numpy array\n",
    "\n",
    "        # Phase 1: Navigate upper layers (down to layer 1) to find good entry point for layer 0\n",
    "        for l in range(len(self.layers) - 1, 0, -1): # Stop at layer 1\n",
    "            if not self.layers[l]: continue # Skip empty layers\n",
    "\n",
    "            # Ensure current entry point is valid for this layer\n",
    "            if current_ep_idx not in self.layers[l] or current_ep_idx >= len(self.vectors):\n",
    "                 try:\n",
    "                      # Fallback: Find a random valid node in the layer\n",
    "                      valid_indices = [idx for idx in self.layers[l] if idx < len(self.vectors)]\n",
    "                      if not valid_indices: continue # Skip layer if no valid nodes\n",
    "                      current_ep_idx = random.choice(valid_indices)\n",
    "                 except IndexError: continue # Should not happen\n",
    "\n",
    "            # Search layer l for the closest node (ef=1)\n",
    "            search_results = self._search_layer(query_vec, l, current_ep_idx, ef=1)\n",
    "            if search_results:\n",
    "                 # Update entry point for the next lower layer\n",
    "                 found_ep_idx = search_results[0][1]\n",
    "                 # Check validity before assignment\n",
    "                 if found_ep_idx < len(self.vectors):\n",
    "                      current_ep_idx = found_ep_idx\n",
    "\n",
    "        # Phase 2: Search layer 0 using the refined entry point\n",
    "        # Ensure the final entry point for layer 0 is valid before the final search\n",
    "        if current_ep_idx >= len(self.vectors) or \\\n",
    "           (0 < len(self.layers) and current_ep_idx not in self.layers[0]):\n",
    "             # If layer 0 exists and has nodes, try to find a fallback EP within it\n",
    "             if 0 < len(self.layers) and self.layers[0]:\n",
    "                  try:\n",
    "                      valid_indices_l0 = [idx for idx in self.layers[0] if idx < len(self.vectors)]\n",
    "                      if not valid_indices_l0: return [] # No valid nodes in layer 0\n",
    "                      current_ep_idx = random.choice(valid_indices_l0)\n",
    "                  except IndexError: return [] # Should not happen\n",
    "             else: return [] # Layer 0 doesn't exist or is empty\n",
    "\n",
    "        # Perform final search in layer 0 with ef_search\n",
    "        neighbors = self._search_layer(query_vec, 0, current_ep_idx, ef_search)\n",
    "\n",
    "        # Return the top k results by index\n",
    "        return [idx for dist, idx in neighbors[:k]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from heapq import heappush, heappop\n",
    "import random\n",
    "import math\n",
    "\n",
    "class HNSWfreqdistance:\n",
    "    \"\"\"\n",
    "    HNSW implementation based on paste.txt [1], modified to boost layer\n",
    "    probability for specified 'hub' nodes during insertion.\n",
    "    Search performance remains identical to the base HNSW implementation.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, max_elements, hubs, boost_const, M=16, ef_construction=200):\n",
    "        \"\"\"\n",
    "        Initializes the HNSWfreqdistance index.\n",
    "\n",
    "        Args:\n",
    "            dim (int): Dimensionality of the vectors.\n",
    "            max_elements (int): Estimated maximum number of elements (informational).\n",
    "            hubs (set): A set of integer indices representing 'hub' nodes.\n",
    "                        Nodes whose *future* index is in this set will have\n",
    "                        their layer assignment probability boosted.\n",
    "            boost_const (float): A small constant added to the layer calculation\n",
    "                                 multiplier (mL) for hub nodes.\n",
    "            M (int): Maximum number of connections per node per layer.\n",
    "            ef_construction (int): Size of the dynamic candidate list during construction.\n",
    "        \"\"\"\n",
    "        self.dim = dim\n",
    "        self.max_elements = max_elements # Informational\n",
    "        self.M = M\n",
    "        self.ef_construction = ef_construction\n",
    "        self.hubs = hubs if hubs is not None else set() # Ensure hubs is a set\n",
    "        self.boost_const = boost_const # Constant for boosting hub layer probability\n",
    "\n",
    "        # Store graph layers: layers[l] is a dict {node_idx: [neighbor_indices]}\n",
    "        self.layers = []\n",
    "        # Store the actual vectors, index corresponds to node_idx\n",
    "        self.vectors = []\n",
    "        # Track the entry point (index of the node in the highest layer)\n",
    "        self.entry_point = None\n",
    "        # Precompute layer multiplier\n",
    "        self.ml = 1 / math.log(self.M) if self.M > 1 else 1\n",
    "\n",
    "\n",
    "    # *** MODIFIED to accept node_idx and apply boost ***\n",
    "    def _get_layer(self, node_idx):\n",
    "        \"\"\"\n",
    "        Determine the layer for a new node based on exponential decay.\n",
    "        If node_idx is in self.hubs, boost the probability of higher layers.\n",
    "\n",
    "        Args:\n",
    "            node_idx (int): The index the new node *will* have upon insertion.\n",
    "\n",
    "        Returns:\n",
    "            int: The selected layer index (>= 0).\n",
    "        \"\"\"\n",
    "        # Use precomputed base multiplier\n",
    "        current_ml = self.ml\n",
    "\n",
    "        # Apply boost if the node index is designated as a hub\n",
    "        if node_idx in self.hubs:\n",
    "            current_ml += self.boost_const\n",
    "\n",
    "        # Calculate layer using the (potentially boosted) multiplier\n",
    "        layer = max(0, int(-math.log(random.random()) * current_ml))\n",
    "        return layer\n",
    "\n",
    "    # --- _distance method remains unchanged from paste.txt [1] ---\n",
    "    def _distance(self, idx1, idx2):\n",
    "        if not (0 <= idx1 < len(self.vectors) and 0 <= idx2 < len(self.vectors)):\n",
    "            return float('inf')\n",
    "        return np.linalg.norm(self.vectors[idx1] - self.vectors[idx2])\n",
    "\n",
    "    # --- _search_layer method remains unchanged from paste.txt [1] ---\n",
    "    # (Includes robustness checks from paste.txt [1])\n",
    "    def _search_layer(self, query_vec, layer_idx, ep_idx, ef):\n",
    "        if layer_idx < 0 or layer_idx >= len(self.layers): return []\n",
    "        layer_graph = self.layers[layer_idx]\n",
    "        if not layer_graph: return []\n",
    "\n",
    "        if ep_idx not in layer_graph or ep_idx >= len(self.vectors):\n",
    "            try:\n",
    "                valid_indices = [idx for idx in layer_graph if idx < len(self.vectors)]\n",
    "                if not valid_indices: return []\n",
    "                ep_idx = random.choice(valid_indices)\n",
    "            except IndexError: return []\n",
    "\n",
    "        visited = set()\n",
    "        candidates = []\n",
    "        results = []\n",
    "\n",
    "        try:\n",
    "            if ep_idx >= len(self.vectors): return []\n",
    "            initial_dist = np.linalg.norm(query_vec - self.vectors[ep_idx])\n",
    "            heappush(candidates, (initial_dist, ep_idx))\n",
    "            heappush(results, (-initial_dist, ep_idx))\n",
    "            visited.add(ep_idx)\n",
    "        except IndexError:\n",
    "             # print(f\"Warning: IndexError accessing vector for entry point {ep_idx}...\") # Optional warning\n",
    "             return []\n",
    "\n",
    "        while candidates:\n",
    "            try:\n",
    "                dist_candidate, current_idx = heappop(candidates)\n",
    "            except IndexError: break\n",
    "\n",
    "            farthest_dist_in_results = -results[0][0] if results else float('inf')\n",
    "            if dist_candidate > farthest_dist_in_results and len(results) >= ef:\n",
    "                 break\n",
    "\n",
    "            if current_idx not in layer_graph or current_idx >= len(self.vectors):\n",
    "                continue\n",
    "\n",
    "            for neighbor_idx in layer_graph.get(current_idx, []):\n",
    "                if neighbor_idx not in visited and neighbor_idx < len(self.vectors):\n",
    "                    visited.add(neighbor_idx)\n",
    "                    dist_neighbor = np.linalg.norm(query_vec - self.vectors[neighbor_idx])\n",
    "                    farthest_dist_in_results = -results[0][0] if results else float('inf')\n",
    "\n",
    "                    if dist_neighbor < farthest_dist_in_results or len(results) < ef:\n",
    "                        heappush(results, (-dist_neighbor, neighbor_idx))\n",
    "                        if len(results) > ef: heappop(results)\n",
    "                        heappush(candidates, (dist_neighbor, neighbor_idx))\n",
    "\n",
    "        final_results = sorted([(-d, idx) for d, idx in results])\n",
    "        return final_results[:ef]\n",
    "\n",
    "    # *** MODIFIED to pass new_idx to _get_layer ***\n",
    "    def insert(self, vector):\n",
    "        \"\"\"Insert a vector into the HNSW index.\"\"\"\n",
    "        new_idx = len(self.vectors) # Determine index before layer calculation\n",
    "        self.vectors.append(np.array(vector))\n",
    "\n",
    "        # *** MODIFIED CALL ***: Pass the node's future index to _get_layer\n",
    "        node_max_layer = self._get_layer(new_idx)\n",
    "\n",
    "        current_ep_idx = self.entry_point\n",
    "\n",
    "        while len(self.layers) <= node_max_layer:\n",
    "            self.layers.append(dict())\n",
    "\n",
    "        # --- Rest of insert method unchanged from paste.txt [1] ---\n",
    "        # (Includes robustness checks)\n",
    "        current_insert_vec = self.vectors[new_idx] # Use stored vector for search\n",
    "\n",
    "        for l in range(len(self.layers) - 1, node_max_layer, -1):\n",
    "             if current_ep_idx is None: break\n",
    "             if not self.layers[l]: continue\n",
    "\n",
    "             # Ensure entry point validity before search\n",
    "             if current_ep_idx not in self.layers[l] or current_ep_idx >= len(self.vectors):\n",
    "                 try:\n",
    "                    valid_indices = [idx for idx in self.layers[l] if idx < len(self.vectors)]\n",
    "                    if not valid_indices: continue\n",
    "                    current_ep_idx = random.choice(valid_indices)\n",
    "                 except IndexError: continue\n",
    "\n",
    "             search_results = self._search_layer(current_insert_vec, l, current_ep_idx, ef=1)\n",
    "             if search_results:\n",
    "                 found_ep_idx = search_results[0][1]\n",
    "                 if found_ep_idx < len(self.vectors): # Check validity before assignment\n",
    "                      current_ep_idx = found_ep_idx\n",
    "\n",
    "        for l in range(min(node_max_layer, len(self.layers) - 1), -1, -1):\n",
    "            layer_graph = self.layers[l]\n",
    "\n",
    "            # Ensure valid entry point for search\n",
    "            if current_ep_idx is None or current_ep_idx not in layer_graph or current_ep_idx >= len(self.vectors):\n",
    "                 if layer_graph:\n",
    "                     try:\n",
    "                         valid_indices = [idx for idx in layer_graph if idx < len(self.vectors)]\n",
    "                         if not valid_indices: current_ep_idx = None\n",
    "                         else: current_ep_idx = random.choice(valid_indices)\n",
    "                     except IndexError: current_ep_idx = None\n",
    "                 else: current_ep_idx = None\n",
    "\n",
    "            if current_ep_idx is None:\n",
    "                 neighbors = []\n",
    "            else:\n",
    "                 neighbors = self._search_layer(current_insert_vec, l, current_ep_idx, self.ef_construction)\n",
    "                 if neighbors:\n",
    "                      found_ep_idx = neighbors[0][1]\n",
    "                      if found_ep_idx < len(self.vectors): # Check validity\n",
    "                           current_ep_idx = found_ep_idx\n",
    "\n",
    "            connections = [idx for dist, idx in neighbors[:self.M]]\n",
    "            layer_graph[new_idx] = connections\n",
    "\n",
    "            for neighbor_idx in connections:\n",
    "                 # Check validity before adding backlink\n",
    "                 if neighbor_idx in layer_graph and neighbor_idx < len(self.vectors):\n",
    "                     neighbor_connections = layer_graph[neighbor_idx]\n",
    "                     if new_idx not in neighbor_connections:\n",
    "                          neighbor_connections.append(new_idx)\n",
    "                          if len(neighbor_connections) > self.M:\n",
    "                               # Check connection validity before distance calc for pruning\n",
    "                               valid_conns = [cidx for cidx in neighbor_connections if cidx < len(self.vectors)]\n",
    "                               if len(valid_conns) < len(neighbor_connections):\n",
    "                                   layer_graph[neighbor_idx] = valid_conns\n",
    "                                   continue\n",
    "\n",
    "                               distances = [(self._distance(neighbor_idx, conn_idx), conn_idx) for conn_idx in valid_conns]\n",
    "                               distances.sort()\n",
    "                               layer_graph[neighbor_idx] = [idx for dist, idx in distances[:self.M]]\n",
    "\n",
    "        # Update global entry point logic (unchanged from paste.txt [1])\n",
    "        current_ep_layer = self._get_node_layer(self.entry_point) # Use helper\n",
    "        if self.entry_point is None or node_max_layer > current_ep_layer:\n",
    "            self.entry_point = new_idx\n",
    "\n",
    "\n",
    "    # --- Helper method from paste-2.txt [2] added for clarity ---\n",
    "    def _get_node_layer(self, node_idx):\n",
    "        # Helper to find highest layer a node exists in\n",
    "        if node_idx is None or node_idx < 0: return -1\n",
    "        for l in range(len(self.layers) - 1, -1, -1):\n",
    "             if node_idx in self.layers[l]:\n",
    "                 return l\n",
    "        return -1 # Node not found\n",
    "\n",
    "    # --- search method remains unchanged from paste.txt [1] ---\n",
    "    # (Includes robustness checks)\n",
    "    def search(self, query_vec, k=10):\n",
    "        ef_search = max(self.ef_construction, k)\n",
    "        current_ep_idx = self.entry_point\n",
    "\n",
    "        if current_ep_idx is None or current_ep_idx >= len(self.vectors):\n",
    "            #print(\"Warning: Entry point is None or invalid...\") # Optional\n",
    "            return []\n",
    "\n",
    "        query_vec = np.array(query_vec)\n",
    "\n",
    "        for l in range(len(self.layers) - 1, 0, -1): # Down to layer 1\n",
    "            if not self.layers[l]: continue\n",
    "\n",
    "            # Ensure entry point validity\n",
    "            if current_ep_idx not in self.layers[l] or current_ep_idx >= len(self.vectors):\n",
    "                 try:\n",
    "                     valid_indices = [idx for idx in self.layers[l] if idx < len(self.vectors)]\n",
    "                     if not valid_indices: continue\n",
    "                     current_ep_idx = random.choice(valid_indices)\n",
    "                 except IndexError: continue\n",
    "\n",
    "            search_results = self._search_layer(query_vec, l, current_ep_idx, ef=1)\n",
    "            if search_results:\n",
    "                 found_ep_idx = search_results[0][1]\n",
    "                 if found_ep_idx < len(self.vectors): # Check validity\n",
    "                      current_ep_idx = found_ep_idx\n",
    "\n",
    "        # Ensure final entry point for layer 0 is valid\n",
    "        if current_ep_idx >= len(self.vectors) or (0 < len(self.layers) and current_ep_idx not in self.layers[0]):\n",
    "             if 0 < len(self.layers) and self.layers[0]:\n",
    "                  try:\n",
    "                      valid_indices_l0 = [idx for idx in self.layers[0] if idx < len(self.vectors)]\n",
    "                      if not valid_indices_l0: return []\n",
    "                      current_ep_idx = random.choice(valid_indices_l0)\n",
    "                  except IndexError: return []\n",
    "             else: return []\n",
    "\n",
    "        neighbors = self._search_layer(query_vec, 0, current_ep_idx, ef_search)\n",
    "        return [idx for dist, idx in neighbors[:k]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import time\n",
    "# from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# # --- Assume these are pre-loaded/defined ---\n",
    "# # vectors: np.ndarray (N x Dim) - Your dataset (e.g., GloVe)\n",
    "# # hubs: set - Set of integer indices designated as hubs\n",
    "# # HNSWfreqdistance: class - Your custom class definition (from previous step) must be available\n",
    "\n",
    "# # --- Parameters ---\n",
    "# K = 10                   # Number of neighbors for recall\n",
    "# NUM_QUERIES = 5000       # Number of queries to use\n",
    "# # Ensure vectors is defined before accessing shape\n",
    "# if 'vectors' not in globals() or not isinstance(vectors, np.ndarray):\n",
    "#     raise NameError(\"The 'vectors' numpy array is not defined.\")\n",
    "# DIMENSION = vectors.shape[1]\n",
    "# NUM_ELEMENTS = vectors.shape[0]\n",
    "\n",
    "# # HNSW parameters for your custom class\n",
    "# M = 16\n",
    "# # M_HUB is not used in the provided HNSWfreqdistance class, only boost_const\n",
    "# EF_CONSTRUCTION = 200\n",
    "# BOOST_CONST = 0.1        # For HNSWfreqdistance\n",
    "\n",
    "# # Select query vectors (first N)\n",
    "# query_vectors = vectors[:min(NUM_QUERIES, NUM_ELEMENTS)]\n",
    "# num_actual_queries = len(query_vectors)\n",
    "# all_indices = np.arange(NUM_ELEMENTS)\n",
    "\n",
    "# # --- Helper Function ---\n",
    "# def calculate_recall(ground_truth, found_neighbors, k):\n",
    "#     total_found = 0\n",
    "#     num_queries = len(ground_truth)\n",
    "#     if num_queries == 0: return 0.0\n",
    "#     expected_total = num_queries * k\n",
    "#     for i in range(num_queries):\n",
    "#         # Ensure ground truth has k elements for comparison if possible\n",
    "#         gt_set = set(ground_truth[i][:k])\n",
    "#         # Handle cases where ANN search returns less than k\n",
    "#         found_set = set(found_neighbors[i][:k]) if found_neighbors[i] is not None else set()\n",
    "#         total_found += len(gt_set.intersection(found_set))\n",
    "#     if expected_total == 0: return 1.0\n",
    "#     return total_found / expected_total\n",
    "\n",
    "# # --- Ground Truth Calculation ---\n",
    "# def calculate_ground_truth(all_vectors, query_vectors, k):\n",
    "#     num_actual_queries = len(query_vectors)\n",
    "#     print(f\"Calculating ground truth for {num_actual_queries} queries (k={k})...\")\n",
    "#     start_time = time.perf_counter()\n",
    "#     # Ensure k is not larger than the number of elements fit\n",
    "#     nn_k = min(k, all_vectors.shape[0])\n",
    "#     if nn_k != k:\n",
    "#         print(f\"Warning: Requested k={k}, but only {all_vectors.shape[0]} elements exist. Using k={nn_k} for NearestNeighbors.\")\n",
    "#     bf_index = NearestNeighbors(n_neighbors=nn_k, algorithm='brute', metric='euclidean')\n",
    "#     bf_index.fit(all_vectors)\n",
    "#     # Request the original k, kneighbors handles k > n_samples_fit\n",
    "#     _, ground_truth_indices = bf_index.kneighbors(query_vectors, n_neighbors=k)\n",
    "#     time_taken = time.perf_counter() - start_time\n",
    "#     print(f\"Ground truth calculated in {time_taken:.4f} s.\")\n",
    "#     return ground_truth_indices\n",
    "\n",
    "# # --- HNSWfreqdistance Evaluation Function ---\n",
    "# def evaluate_custom_hnsw(\n",
    "#     HNSWClass, # Pass the class itself\n",
    "#     all_vectors,\n",
    "#     query_vectors,\n",
    "#     ground_truth_indices,\n",
    "#     k,\n",
    "#     hubs,        # Specific to HNSWfreqdistance\n",
    "#     boost_const, # Specific to HNSWfreqdistance\n",
    "#     M,\n",
    "#     ef_construction):\n",
    "#     \"\"\"Builds and evaluates the custom HNSW class.\"\"\"\n",
    "\n",
    "#     print(f\"\\n--- Evaluating {HNSWClass.__name__} ---\")\n",
    "#     dim = all_vectors.shape[1]\n",
    "#     num_elements = all_vectors.shape[0]\n",
    "\n",
    "#     # Instantiate the custom class\n",
    "#     custom_index = HNSWClass(dim=dim, max_elements=num_elements,\n",
    "#                              hubs=hubs, boost_const=boost_const,\n",
    "#                              M=M, ef_construction=ef_construction)\n",
    "\n",
    "#     # --- Indexing ---\n",
    "#     print(\"Indexing...\")\n",
    "#     start_time_idx = time.perf_counter()\n",
    "#     for i in range(num_elements):\n",
    "#         custom_index.insert(all_vectors[i])\n",
    "#     index_time = time.perf_counter() - start_time_idx\n",
    "#     print(f\"Indexing time: {index_time:.4f} s\")\n",
    "\n",
    "#     # --- Querying ---\n",
    "#     print(\"Querying...\")\n",
    "#     custom_results = []\n",
    "#     start_time_query = time.perf_counter()\n",
    "#     for i in range(len(query_vectors)):\n",
    "#         neighbors = custom_index.search(query_vectors[i], k=k)\n",
    "#         custom_results.append(neighbors)\n",
    "#     query_time = time.perf_counter() - start_time_query\n",
    "#     avg_query_time = query_time / len(query_vectors) if len(query_vectors) > 0 else 0\n",
    "#     print(f\"Query time: {query_time:.4f} s ({avg_query_time*1000:.4f} ms/query avg)\")\n",
    "\n",
    "#     # --- Recall ---\n",
    "#     recall = calculate_recall(ground_truth_indices, custom_results, k)\n",
    "#     print(f\"Recall@{k}: {recall:.4f}\")\n",
    "\n",
    "#     return recall, index_time, query_time, avg_query_time\n",
    "\n",
    "# # --- Main Execution ---\n",
    "\n",
    "# # 1. Calculate Ground Truth\n",
    "# # Ensure the custom class definition is available before calling this script\n",
    "# if 'HNSWfreqdistance' not in globals():\n",
    "#      raise NameError(\"The HNSWfreqdistance class is not defined in the current scope.\")\n",
    "\n",
    "# gt_indices = calculate_ground_truth(vectors, query_vectors, K)\n",
    "\n",
    "# # 2. Evaluate your HNSWfreqdistance class\n",
    "# recall_cust, index_time_cust, query_time_cust, avg_query_time_cust = evaluate_custom_hnsw(\n",
    "#     HNSWClass=HNSWfreqdistance,\n",
    "#     all_vectors=vectors,\n",
    "#     query_vectors=query_vectors,\n",
    "#     ground_truth_indices=gt_indices,\n",
    "#     k=K,\n",
    "#     hubs=hubs,\n",
    "#     boost_const=BOOST_CONST,\n",
    "#     M=M,\n",
    "#     ef_construction=EF_CONSTRUCTION\n",
    "# )\n",
    "\n",
    "# # --- Final Results ---\n",
    "# print(\"\\n--- Custom HNSW Performance Summary ---\")\n",
    "# print(f\"Class Name:         {HNSWfreqdistance.__name__}\")\n",
    "# print(f\"Indexing Time:      {index_time_cust:.4f} s\")\n",
    "# print(f\"Total Query Time:   {query_time_cust:.4f} s\")\n",
    "# print(f\"Avg Query Time:     {avg_query_time_cust*1000:.4f} ms\")\n",
    "# print(f\"Recall@{K}:           {recall_cust:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import time\n",
    "# from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# # --- Ensure your HNSW and HNSWfreqdistance classes are in scope ---\n",
    "# # from your_module import HNSW, HNSWfreqdistance\n",
    "\n",
    "# # --- User parameters ---\n",
    "# K = 10                   # Number of neighbors for recall\\ nNUM_QUERIES = 5000       # Number of queries to use\n",
    "\n",
    "# # --- Load your data ---\n",
    "# # Replace with actual loading logic, e.g.: vectors = np.load('vectors.npy')\n",
    "# vectors = np.random.random((400000, 128))  # placeholder: 400k vectors of dim 128\n",
    "# hubs = set(np.random.choice(np.arange(vectors.shape[0]), size=1000, replace=False))  # placeholder hubs\n",
    "\n",
    "# DIMENSION = vectors.shape[1]\n",
    "# NUM_ELEMENTS = vectors.shape[0]\n",
    "\n",
    "# # --- Select query vectors ---\n",
    "# idxs = np.random.choice(NUM_ELEMENTS, size=min(40000, NUM_ELEMENTS), replace=False)\n",
    "# query_vectors = vectors[idxs]\n",
    "# num_actual_queries = len(query_vectors)\n",
    "\n",
    "# # --- Helper functions ---\n",
    "# def calculate_recall(ground_truth, found, k):\n",
    "#     total_found = 0\n",
    "#     num_q = len(ground_truth)\n",
    "#     expected = num_q * k\n",
    "\n",
    "#     for i in range(num_q):\n",
    "#         gt_set = set(ground_truth[i][:k])\n",
    "#         res = found[i]\n",
    "#         found_set = set(res[:k]) if res is not None else set()\n",
    "#         total_found += len(gt_set & found_set)\n",
    "\n",
    "#     return total_found / expected if expected > 0 else 1.0\n",
    "\n",
    "# # --- Ground Truth Calculation ---\n",
    "# def compute_ground_truth(data, queries, k):\n",
    "#     print(f\"Computing ground truth with brute-force for {len(queries)} queries...\")\n",
    "#     t0 = time.perf_counter()\n",
    "#     nn = NearestNeighbors(n_neighbors=k, algorithm='brute', metric='euclidean')\n",
    "#     nn.fit(data)\n",
    "#     _, gt = nn.kneighbors(queries, n_neighbors=k)\n",
    "#     t1 = time.perf_counter()\n",
    "#     print(f\"Done in {t1 - t0:.2f}s\")\n",
    "#     return gt\n",
    "\n",
    "# # --- Evaluation for base HNSW ---\n",
    "# def evaluate_hnsw_base(all_vecs, queries, gt, k, M=16, ef_const=200):\n",
    "#     print(\"\\nEvaluating HNSW (base)\")\n",
    "#     index = HNSW(dim=all_vecs.shape[1], max_elements=all_vecs.shape[0], M=M, ef_construction=ef_const)\n",
    "\n",
    "#     # Indexing\n",
    "#     t0 = time.perf_counter()\n",
    "#     for v in all_vecs:\n",
    "#         index.insert(v)\n",
    "#     t1 = time.perf_counter()\n",
    "\n",
    "#     # Querying\n",
    "#     results = []\n",
    "#     t_q0 = time.perf_counter()\n",
    "#     for q in queries:\n",
    "#         results.append(index.search(q, k=k))\n",
    "#     t_q1 = time.perf_counter()\n",
    "\n",
    "#     recall = calculate_recall(gt, results, k)\n",
    "#     total_q = t_q1 - t_q0\n",
    "#     avg_q = total_q / len(queries)\n",
    "\n",
    "#     print(f\"Index time: {t1 - t0:.2f}s\")\n",
    "#     print(f\"Total query time: {total_q:.2f}s, Avg/query: {avg_q*1000:.2f}ms\")\n",
    "#     print(f\"Recall@{k}: {recall:.4f}\")\n",
    "\n",
    "#     return (t1 - t0), total_q, avg_q, recall\n",
    "\n",
    "# # --- Evaluation for HNSWfreqdistance ---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_hnsw_freq(all_vecs, queries, gt, k, hubs, boost, M=16, ef_const=200):\n",
    "#     print(\"\\nEvaluating HNSWfreqdistance\")\n",
    "#     index = HNSWfreqdistance(dim=all_vecs.shape[1], max_elements=all_vecs.shape[0],\n",
    "#                               hubs=hubs, boost_const=boost,\n",
    "#                               M=M, ef_construction=ef_const)\n",
    "\n",
    "#     # Indexing\n",
    "#     t0 = time.perf_counter()\n",
    "#     for v in all_vecs:\n",
    "#         index.insert(v)\n",
    "#     t1 = time.perf_counter()\n",
    "\n",
    "#     # Querying\n",
    "#     results = []\n",
    "#     t_q0 = time.perf_counter()\n",
    "#     for q in queries:\n",
    "#         results.append(index.search(q, k=k))\n",
    "#     t_q1 = time.perf_counter()\n",
    "\n",
    "#     recall = calculate_recall(gt, results, k)\n",
    "#     total_q = t_q1 - t_q0\n",
    "#     avg_q = total_q / len(queries)\n",
    "\n",
    "#     print(f\"Index time: {t1 - t0:.2f}s\")\n",
    "#     print(f\"Total query time: {total_q:.2f}s, Avg/query: {avg_q*1000:.2f}ms\")\n",
    "#     print(f\"Recall@{k}: {recall:.4f}\")\n",
    "\n",
    "#     return (t1 - t0), total_q, avg_q, recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- User parameters ---\n",
    "K = 10                   # Number of neighbors for recall\n",
    "NUM_QUERIES = 5000       # Number of queries to use\n",
    "\n",
    "\n",
    "DIMENSION = vectors.shape[1]\n",
    "NUM_ELEMENTS = vectors.shape[0]\n",
    "\n",
    "# --- Select query vectors ---\n",
    "idxs = np.random.choice(NUM_ELEMENTS, size=min(40000, NUM_ELEMENTS), replace=False)\n",
    "query_vectors = vectors[idxs]\n",
    "num_actual_queries = len(query_vectors)\n",
    "\n",
    "# --- Helper functions ---\n",
    "def calculate_recall(ground_truth, found, k):\n",
    "    total_found = 0\n",
    "    num_q = len(ground_truth)\n",
    "    expected = num_q * k\n",
    "\n",
    "    for i in range(num_q):\n",
    "        gt_set = set(ground_truth[i][:k])\n",
    "        res = found[i]\n",
    "        found_set = set(res[:k]) if res is not None else set()\n",
    "        total_found += len(gt_set & found_set)\n",
    "\n",
    "    return total_found / expected if expected > 0 else 1.0\n",
    "\n",
    "# --- Ground Truth Calculation ---\n",
    "def compute_ground_truth(data, queries, k):\n",
    "    print(f\"Computing ground truth with brute-force for {len(queries)} queries...\")\n",
    "    t0 = time.perf_counter()\n",
    "    nn = NearestNeighbors(n_neighbors=k, algorithm='brute', metric='euclidean')\n",
    "    nn.fit(data)\n",
    "    _, gt = nn.kneighbors(queries, n_neighbors=k)\n",
    "    t1 = time.perf_counter()\n",
    "    print(f\"Done in {t1 - t0:.2f}s\")\n",
    "    return gt\n",
    "\n",
    "# --- Evaluation for base HNSW ---\n",
    "def evaluate_hnsw_base(all_vecs, queries, gt, k, M=16, ef_const=200):\n",
    "    print(\"\\nEvaluating HNSW (base)\")\n",
    "    index = HNSW(dim=all_vecs.shape[1], max_elements=all_vecs.shape[0], M=M, ef_construction=ef_const)\n",
    "\n",
    "    # Indexing with progress bar\n",
    "    t0 = time.perf_counter()\n",
    "    for v in tqdm(all_vecs, total=all_vecs.shape[0], desc=\"HNSW (base) indexing\"):\n",
    "        index.insert(v)\n",
    "    t1 = time.perf_counter()\n",
    "\n",
    "    # Querying with progress bar\n",
    "    results = []\n",
    "    t_q0 = time.perf_counter()\n",
    "    for q in tqdm(queries, total=len(queries), desc=\"HNSW (base) querying\"):\n",
    "        results.append(index.search(q, k=k))\n",
    "    t_q1 = time.perf_counter()\n",
    "\n",
    "    recall = calculate_recall(gt, results, k)\n",
    "    total_q = t_q1 - t_q0\n",
    "    avg_q = total_q / len(queries)\n",
    "\n",
    "    print(f\"Index time: {t1 - t0:.2f}s\")\n",
    "    print(f\"Total query time: {total_q:.2f}s, Avg/query: {avg_q*1000:.2f}ms\")\n",
    "    print(f\"Recall@{k}: {recall:.4f}\")\n",
    "\n",
    "    return (t1 - t0), total_q, avg_q, recall\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluation for HNSWfreqdistance ---\n",
    "def evaluate_hnsw_freq(all_vecs, queries, gt, k, hubs, boost, M=16, ef_const=200):\n",
    "    print(\"\\nEvaluating HNSWfreqdistance\")\n",
    "    index = HNSWfreqdistance(dim=all_vecs.shape[1], max_elements=all_vecs.shape[0],\n",
    "                              hubs=hubs, boost_const=boost,\n",
    "                              M=M, ef_construction=ef_const)\n",
    "\n",
    "    # Indexing with progress bar\n",
    "    t0 = time.perf_counter()\n",
    "    for v in tqdm(all_vecs, total=all_vecs.shape[0], desc=\"HNSWfreqdistance indexing\"):\n",
    "        index.insert(v)\n",
    "    t1 = time.perf_counter()\n",
    "\n",
    "    # Querying with progress bar\n",
    "    results = []\n",
    "    t_q0 = time.perf_counter()\n",
    "    for q in tqdm(queries, total=len(queries), desc=\"HNSWfreqdistance querying\"):\n",
    "        results.append(index.search(q, k=k))\n",
    "    t_q1 = time.perf_counter()\n",
    "\n",
    "    recall = calculate_recall(gt, results, k)\n",
    "    total_q = t_q1 - t_q0\n",
    "    avg_q = total_q / len(queries)\n",
    "\n",
    "    print(f\"Index time: {t1 - t0:.2f}s\")\n",
    "    print(f\"Total query time: {total_q:.2f}s, Avg/query: {avg_q*1000:.2f}ms\")\n",
    "    print(f\"Recall@{k}: {recall:.4f}\")\n",
    "\n",
    "    return (t1 - t0), total_q, avg_q, recall\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing ground truth with brute-force for 40000 queries...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 7.38s\n",
      "\n",
      "Evaluating HNSW (base)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HNSW (base) indexing: 100%|██████████| 400000/400000 [42:57<00:00, 155.20it/s]   \n",
      "HNSW (base) querying: 100%|██████████| 40000/40000 [02:24<00:00, 276.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index time: 1575.04s\n",
      "Total query time: 144.92s, Avg/query: 3.62ms\n",
      "Recall@10: 0.4918\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "evaluate_hnsw_freq() got an unexpected keyword argument 'ef_construction'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[135], line 12\u001b[0m\n\u001b[1;32m      6\u001b[0m idx_time_base, q_time_base, avg_q_base, recall_base \u001b[38;5;241m=\u001b[39m evaluate_hnsw_base(\n\u001b[1;32m      7\u001b[0m     vectors, query_vectors, gt_indices, K,\n\u001b[1;32m      8\u001b[0m     M\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, ef_const\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# HNSW with hub boosting\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m idx_time_freq, q_time_freq, avg_q_freq, recall_freq \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_hnsw_freq\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhubs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhubs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mM\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mef_construction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\n\u001b[1;32m     15\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Summary\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Summary ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: evaluate_hnsw_freq() got an unexpected keyword argument 'ef_construction'"
     ]
    }
   ],
   "source": [
    "# --- Main Comparison ---\n",
    "if __name__ == '__main__':\n",
    "    gt_indices = compute_ground_truth(vectors, query_vectors, K)\n",
    "\n",
    "    # Base HNSW\n",
    "    idx_time_base, q_time_base, avg_q_base, recall_base = evaluate_hnsw_base(\n",
    "        vectors, query_vectors, gt_indices, K,\n",
    "        M=16, ef_const=200\n",
    "    )\n",
    "\n",
    "    # HNSW with hub boosting\n",
    "    idx_time_freq, q_time_freq, avg_q_freq, recall_freq = evaluate_hnsw_freq(\n",
    "        vectors, query_vectors, gt_indices, K,\n",
    "        hubs=hubs, boost=0.1, M=16, ef_construction=200\n",
    "    )\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\n--- Summary ---\")\n",
    "    print(f\"Method            | Index(s) | Query(s) | AvgQuery(ms) | Recall@{K}\")\n",
    "    print(f\"HNSW (base)       | {idx_time_base:.2f}    | {q_time_base:.2f}   | {avg_q_base*1000:.2f}        | {recall_base:.4f}\")\n",
    "    print(f\"HNSWfreqdistance  | {idx_time_freq:.2f}    | {q_time_freq:.2f}   | {avg_q_freq*1000:.2f}        | {recall_freq:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HNSW with HUB BOOSTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating HNSWfreqdistance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HNSWfreqdistance indexing: 100%|██████████| 400000/400000 [1:24:41<00:00, 78.72it/s]    \n",
      "HNSWfreqdistance querying: 100%|██████████| 40000/40000 [02:20<00:00, 284.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index time: 1544.15s\n",
      "Total query time: 140.83s, Avg/query: 3.52ms\n",
      "Recall@10: 0.4748\n",
      "\n",
      "--- Summary ---\n",
      "Method            | Index(s) | Query(s) | AvgQuery(ms) | Recall@10\n",
      "HNSW (base)       | 1575.04    | 144.92   | 3.62        | 0.4918\n",
      "HNSWfreqdistance  | 1544.15    | 140.83   | 3.52        | 0.4748\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # HNSW with hub boosting\n",
    "    idx_time_freq, q_time_freq, avg_q_freq, recall_freq = evaluate_hnsw_freq(\n",
    "        vectors, query_vectors, gt_indices, K,\n",
    "        hubs=hubs, boost=0.1, M=16, ef_const=200\n",
    "    )\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\n--- Summary ---\")\n",
    "    print(f\"Method            | Index(s) | Query(s) | AvgQuery(ms) | Recall@{K}\")\n",
    "    print(f\"HNSW (base)       | {idx_time_base:.2f}    | {q_time_base:.2f}   | {avg_q_base*1000:.2f}        | {recall_base:.4f}\")\n",
    "    print(f\"HNSWfreqdistance  | {idx_time_freq:.2f}    | {q_time_freq:.2f}   | {avg_q_freq*1000:.2f}        | {recall_freq:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
