{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import hnswlib\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from heapq import heappush, heappop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors of dimension 100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Path to your GloVe file (update this based on your downloaded version)\n",
    "glove_path = '/Users/tanishqchaudhari/Desktop/DataScience Proj  datasets/Dataset/glove.6B.100d.txt'\n",
    "\n",
    "# Load GloVe vectors\n",
    "word_to_vec = {}\n",
    "words = []\n",
    "vectors = []\n",
    "\n",
    "with open(glove_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]  # First token is the word\n",
    "        vector = np.array(values[1:], dtype=np.float32)  # Rest are vector values\n",
    "        word_to_vec[word] = vector\n",
    "        words.append(word)\n",
    "        vectors.append(vector)\n",
    "\n",
    "# Convert to numpy array\n",
    "vectors = np.array(vectors, dtype=np.float32)\n",
    "print(f\"Loaded {len(words)} word vectors of dimension {vectors.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400000, 100)\n"
     ]
    }
   ],
   "source": [
    "print(vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from heapq import heappush, heappop\n",
    "# import random\n",
    "# import math\n",
    "# import time\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# class HNSWQueryTimePCA:\n",
    "#     \"\"\"\n",
    "#     HNSW implementation with conditional PCA application during query time.\n",
    "#     The index is built with standard HNSW, but searches can optionally use PCA\n",
    "#     in the top layers to accelerate distance calculations.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, dim, max_elements, M=16, ef_construction=200,\n",
    "#                  pca_yes=False, pca_top_layers=3, pca_components=50):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             dim (int): Dimensionality of vectors.\n",
    "#             max_elements (int): Maximum expected elements (informational).\n",
    "#             M (int): Maximum connections per node per layer.\n",
    "#             ef_construction (int): Size of candidate list during construction.\n",
    "#             pca_yes (bool): Whether to use PCA during query time.\n",
    "#             pca_top_layers (int): Number of top layers to apply PCA if pca_yes=True.\n",
    "#             pca_components (int): Target dimensionality after PCA reduction.\n",
    "#         \"\"\"\n",
    "#         self.dim = dim\n",
    "#         self.max_elements = max_elements\n",
    "#         self.M = M\n",
    "#         self.ef_construction = ef_construction\n",
    "        \n",
    "#         # PCA parameters\n",
    "#         self.pca_yes = pca_yes\n",
    "#         self.pca_top_layers = pca_top_layers\n",
    "#         self.pca_components = min(pca_components, dim)  # Can't exceed original dimension\n",
    "        \n",
    "#         # HNSW data structures\n",
    "#         self.layers = []  # Store graph layers: layers[l] is a dict {node_idx: [neighbor_indices]}\n",
    "#         self.vectors = []  # Store the actual vectors\n",
    "#         self.entry_point = None  # Entry point for the graph\n",
    "        \n",
    "#         # PCA data - only used if pca_yes=True\n",
    "#         self.pca_models = {}  # {layer_idx: pca_model}\n",
    "#         self.reduced_vectors = {}  # {node_idx: reduced_vector}\n",
    "#         self.pca_applied_layers = set()  # Layers where PCA is active\n",
    "        \n",
    "#         print(f\"Initialized HNSW with {'PCA enabled' if pca_yes else 'standard'} querying\")\n",
    "\n",
    "#     def _get_layer(self):\n",
    "#         \"\"\"Determine the layer for a new node based on exponential decay.\"\"\"\n",
    "#         ml = 1 / math.log(self.M) if self.M > 1 else 1\n",
    "#         return max(0, int(-math.log(random.random()) * ml))\n",
    "\n",
    "#     def _distance(self, idx1, idx2):\n",
    "#         \"\"\"Compute Euclidean distance between vectors using their indices.\"\"\"\n",
    "#         if not (0 <= idx1 < len(self.vectors) and 0 <= idx2 < len(self.vectors)):\n",
    "#             return float('inf')\n",
    "#         return np.linalg.norm(self.vectors[idx1] - self.vectors[idx2])\n",
    "\n",
    "#     def _search_layer_standard(self, query_vec, layer_idx, ep_idx, ef):\n",
    "#         \"\"\"Standard HNSW layer search without PCA.\"\"\"\n",
    "#         layer_graph = self.layers[layer_idx]\n",
    "#         if not layer_graph: return []\n",
    "#         if ep_idx not in layer_graph:\n",
    "#             try: ep_idx = next(iter(layer_graph))\n",
    "#             except StopIteration: return []\n",
    "\n",
    "#         visited = set()\n",
    "#         candidates = []  # Min-heap: (dist, idx)\n",
    "#         results = []     # Max-heap: (-dist, idx)\n",
    "\n",
    "#         initial_dist = np.linalg.norm(query_vec - self.vectors[ep_idx])\n",
    "#         heappush(candidates, (initial_dist, ep_idx))\n",
    "#         heappush(results, (-initial_dist, ep_idx))\n",
    "#         visited.add(ep_idx)\n",
    "\n",
    "#         while candidates:\n",
    "#             dist_candidate, current_idx = heappop(candidates)\n",
    "#             farthest_dist_in_results = -results[0][0] if results else float('inf')\n",
    "\n",
    "#             if dist_candidate > farthest_dist_in_results and len(results) >= ef:\n",
    "#                 break\n",
    "\n",
    "#             for neighbor_idx in layer_graph.get(current_idx, []):\n",
    "#                 if neighbor_idx not in visited and neighbor_idx < len(self.vectors):\n",
    "#                     visited.add(neighbor_idx)\n",
    "#                     dist_neighbor = np.linalg.norm(query_vec - self.vectors[neighbor_idx])\n",
    "#                     farthest_dist_in_results = -results[0][0] if results else float('inf')\n",
    "\n",
    "#                     if dist_neighbor < farthest_dist_in_results or len(results) < ef:\n",
    "#                         heappush(results, (-dist_neighbor, neighbor_idx))\n",
    "#                         if len(results) > ef: heappop(results)\n",
    "#                         heappush(candidates, (dist_neighbor, neighbor_idx))\n",
    "\n",
    "#         final_results = sorted([(-d, idx) for d, idx in results])\n",
    "#         return final_results[:ef]\n",
    "\n",
    "#     def _search_layer_pca(self, query_vec, layer_idx, ep_idx, ef):\n",
    "#         \"\"\"PCA-enhanced layer search for top layers.\"\"\"\n",
    "#         layer_graph = self.layers[layer_idx]\n",
    "#         if not layer_graph: return []\n",
    "#         if ep_idx not in layer_graph:\n",
    "#             try: ep_idx = next(iter(layer_graph))\n",
    "#             except StopIteration: return []\n",
    "\n",
    "#         visited = set()\n",
    "#         candidates = []  # Min-heap: (dist, idx)\n",
    "#         results = []     # Max-heap: (-dist, idx)\n",
    "\n",
    "#         # Determine if we're in a PCA layer and transform query if needed\n",
    "#         is_pca_layer = layer_idx in self.pca_applied_layers\n",
    "#         query_vec_transformed = None\n",
    "#         if is_pca_layer:\n",
    "#             pca_model = self.pca_models.get(layer_idx)\n",
    "#             if pca_model:\n",
    "#                 try:\n",
    "#                     query_vec_transformed = pca_model.transform(query_vec.reshape(1, -1))[0]\n",
    "#                 except Exception as e:\n",
    "#                     is_pca_layer = False  # Fall back to standard if transform fails\n",
    "\n",
    "#         # Calculate initial distance (PCA space if applicable)\n",
    "#         initial_dist = float('inf')\n",
    "#         if is_pca_layer and ep_idx in self.reduced_vectors:\n",
    "#             initial_dist = np.linalg.norm(query_vec_transformed - self.reduced_vectors[ep_idx])\n",
    "#         if initial_dist == float('inf'):  # Fallback to original\n",
    "#             initial_dist = np.linalg.norm(query_vec - self.vectors[ep_idx])\n",
    "\n",
    "#         heappush(candidates, (initial_dist, ep_idx))\n",
    "#         heappush(results, (-initial_dist, ep_idx))\n",
    "#         visited.add(ep_idx)\n",
    "\n",
    "#         while candidates:\n",
    "#             dist_candidate, current_idx = heappop(candidates)\n",
    "#             farthest_dist_in_results = -results[0][0] if results else float('inf')\n",
    "\n",
    "#             if dist_candidate > farthest_dist_in_results and len(results) >= ef:\n",
    "#                 break\n",
    "\n",
    "#             for neighbor_idx in layer_graph.get(current_idx, []):\n",
    "#                 if neighbor_idx not in visited and neighbor_idx < len(self.vectors):\n",
    "#                     visited.add(neighbor_idx)\n",
    "                    \n",
    "#                     # PCA distance if applicable\n",
    "#                     dist_neighbor = float('inf')\n",
    "#                     if is_pca_layer and neighbor_idx in self.reduced_vectors:\n",
    "#                         dist_neighbor = np.linalg.norm(query_vec_transformed - self.reduced_vectors[neighbor_idx])\n",
    "#                     if dist_neighbor == float('inf'):  # Fallback\n",
    "#                         dist_neighbor = np.linalg.norm(query_vec - self.vectors[neighbor_idx])\n",
    "                        \n",
    "#                     farthest_dist_in_results = -results[0][0] if results else float('inf')\n",
    "\n",
    "#                     if dist_neighbor < farthest_dist_in_results or len(results) < ef:\n",
    "#                         heappush(results, (-dist_neighbor, neighbor_idx))\n",
    "#                         if len(results) > ef: heappop(results)\n",
    "#                         heappush(candidates, (dist_neighbor, neighbor_idx))\n",
    "\n",
    "#         final_results = sorted([(-d, idx) for d, idx in results])\n",
    "#         return final_results[:ef]\n",
    "\n",
    "#     def _search_layer(self, query_vec, layer_idx, ep_idx, ef):\n",
    "#         \"\"\"Unified search layer method that delegates to the appropriate implementation.\"\"\"\n",
    "#         if self.pca_yes and layer_idx in self.pca_applied_layers:\n",
    "#             return self._search_layer_pca(query_vec, layer_idx, ep_idx, ef)\n",
    "#         else:\n",
    "#             return self._search_layer_standard(query_vec, layer_idx, ep_idx, ef)\n",
    "\n",
    "#     def insert(self, vector):\n",
    "#         \"\"\"Insert a vector into the HNSW index (standard implementation).\"\"\"\n",
    "#         new_idx = len(self.vectors)\n",
    "#         self.vectors.append(np.array(vector))\n",
    "#         node_max_layer = self._get_layer()\n",
    "#         current_ep_idx = self.entry_point\n",
    "\n",
    "#         while len(self.layers) <= node_max_layer:\n",
    "#             self.layers.append(dict())\n",
    "\n",
    "#         # Find entry points for insertion phase\n",
    "#         for l in range(len(self.layers) - 1, node_max_layer, -1):\n",
    "#             if current_ep_idx is None: break\n",
    "#             if not self.layers[l]: continue\n",
    "#             search_results = self._search_layer_standard(vector, l, current_ep_idx, ef=1)\n",
    "#             if search_results:\n",
    "#                 current_ep_idx = search_results[0][1]\n",
    "\n",
    "#         # Insertion phase\n",
    "#         for l in range(min(node_max_layer, len(self.layers) - 1), -1, -1):\n",
    "#             layer_graph = self.layers[l]\n",
    "#             if current_ep_idx is None and layer_graph:\n",
    "#                 try:\n",
    "#                     current_ep_idx = next(iter(layer_graph))\n",
    "#                 except StopIteration:\n",
    "#                     pass\n",
    "\n",
    "#             if current_ep_idx is None:\n",
    "#                 neighbors = []\n",
    "#             else:\n",
    "#                 neighbors = self._search_layer_standard(vector, l, current_ep_idx, self.ef_construction)\n",
    "#                 if neighbors:\n",
    "#                     current_ep_idx = neighbors[0][1]\n",
    "\n",
    "#             connections = [idx for dist, idx in neighbors[:self.M]]\n",
    "#             layer_graph[new_idx] = connections\n",
    "\n",
    "#             # Add bidirectional links\n",
    "#             for neighbor_idx in connections:\n",
    "#                 if neighbor_idx not in layer_graph: \n",
    "#                     layer_graph[neighbor_idx] = []\n",
    "                \n",
    "#                 neighbor_connections = layer_graph[neighbor_idx]\n",
    "#                 if new_idx not in neighbor_connections:\n",
    "#                     neighbor_connections.append(new_idx)\n",
    "#                     if len(neighbor_connections) > self.M:\n",
    "#                         distances = [(self._distance(neighbor_idx, conn_idx), conn_idx) \n",
    "#                                     for conn_idx in neighbor_connections]\n",
    "#                         distances.sort()\n",
    "#                         layer_graph[neighbor_idx] = [idx for dist, idx in distances[:self.M]]\n",
    "\n",
    "#         # Update global entry point\n",
    "#         if node_max_layer >= len(self.layers) - 1 or self.entry_point is None:\n",
    "#             self.entry_point = new_idx\n",
    "\n",
    "#     def finalize_pca(self):\n",
    "#         \"\"\"\n",
    "#         Prepares PCA models for the top layers after all vectors are inserted.\n",
    "#         Only needed if pca_yes=True.\n",
    "#         \"\"\"\n",
    "#         if not self.pca_yes:\n",
    "#             print(\"PCA is disabled (pca_yes=False). Skipping PCA finalization.\")\n",
    "#             return\n",
    "            \n",
    "#         print(f\"Applying PCA to top {self.pca_top_layers} layers (target dim: {self.pca_components})...\")\n",
    "#         num_actual_layers = len(self.layers)\n",
    "#         if num_actual_layers == 0:\n",
    "#             print(\"No layers found, skipping PCA.\")\n",
    "#             return\n",
    "\n",
    "#         # Determine which layers to apply PCA to\n",
    "#         start_pca_layer = max(0, num_actual_layers - self.pca_top_layers)\n",
    "\n",
    "#         for l_idx in range(start_pca_layer, num_actual_layers):\n",
    "#             layer_graph = self.layers[l_idx]\n",
    "#             node_indices = list(layer_graph.keys())\n",
    "\n",
    "#             if not node_indices:\n",
    "#                 print(f\"Layer {l_idx} is empty, skipping PCA.\")\n",
    "#                 continue\n",
    "\n",
    "#             valid_indices = [idx for idx in node_indices if idx < len(self.vectors)]\n",
    "#             if len(valid_indices) <= self.pca_components:\n",
    "#                 print(f\"Layer {l_idx} has only {len(valid_indices)} nodes, fewer than required {self.pca_components} for PCA.\")\n",
    "#                 continue\n",
    "\n",
    "#             try:\n",
    "#                 layer_vectors = np.array([self.vectors[idx] for idx in valid_indices])\n",
    "#                 print(f\"Fitting PCA for layer {l_idx} on {len(valid_indices)} vectors...\")\n",
    "#                 pca = PCA(n_components=self.pca_components)\n",
    "#                 pca.fit(layer_vectors)\n",
    "#                 reduced_vectors = pca.transform(layer_vectors)\n",
    "                \n",
    "#                 self.pca_models[l_idx] = pca\n",
    "#                 self.pca_applied_layers.add(l_idx)\n",
    "#                 for idx, reduced_vec in zip(valid_indices, reduced_vectors):\n",
    "#                     self.reduced_vectors[idx] = reduced_vec\n",
    "#                 print(f\"PCA applied successfully to layer {l_idx}\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error applying PCA to layer {l_idx}: {e}\")\n",
    "#                 continue\n",
    "\n",
    "#         print(f\"PCA finalization complete. Applied to {len(self.pca_applied_layers)} layers.\")\n",
    "\n",
    "#     def search(self, query_vec, k=10):\n",
    "#         \"\"\"Search for the top-k nearest neighbors using standard or PCA-enhanced search.\"\"\"\n",
    "#         query_vec = np.array(query_vec)\n",
    "#         ef_search = max(self.ef_construction, k)\n",
    "#         current_ep_idx = self.entry_point\n",
    "\n",
    "#         if current_ep_idx is None:\n",
    "#             return []\n",
    "\n",
    "#         # Search down from top layer to find entry point for base layer\n",
    "#         for l in range(len(self.layers) - 1, 0, -1):\n",
    "#             if not self.layers[l]: continue\n",
    "#             search_results = self._search_layer(query_vec, l, current_ep_idx, ef=1)\n",
    "#             if search_results:\n",
    "#                 current_ep_idx = search_results[0][1]\n",
    "\n",
    "#         # Search in base layer\n",
    "#         neighbors = self._search_layer(query_vec, 0, current_ep_idx, ef_search)\n",
    "#         return [idx for dist, idx in neighbors[:k]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compare_standard_vs_pca_hnsw(vectors, num_queries=1000, k=10, M=16, ef_construction=200, \n",
    "#                                ef_search=200, pca_top_layers=3, pca_components=50):\n",
    "#     \"\"\"\n",
    "#     Builds one HNSW index and compares standard vs PCA-enabled query performance.\n",
    "    \n",
    "#     Args:\n",
    "#         vectors: np.ndarray of shape (n, dim) containing the vectors to index\n",
    "#         num_queries: number of queries to run (first num_queries vectors will be used)\n",
    "#         k: number of nearest neighbors to retrieve\n",
    "#         M: HNSW M parameter (max connections per node)\n",
    "#         ef_construction: HNSW ef_construction parameter\n",
    "#         ef_search: HNSW ef_search parameter for querying\n",
    "#         pca_top_layers: number of top layers to apply PCA to\n",
    "#         pca_components: target dimensionality after PCA reduction\n",
    "    \n",
    "#     Returns:\n",
    "#         dict: Results comparison\n",
    "#     \"\"\"\n",
    "#     import time\n",
    "#     from sklearn.neighbors import NearestNeighbors\n",
    "    \n",
    "#     N, dim = vectors.shape\n",
    "#     # Get total number of vectors\n",
    "#     N = vectors.shape[0]  # Note: Use vectors.shape[0] to get count, not just vectors.shape\n",
    "\n",
    "#     # Choose 1000 random indices without replacement\n",
    "#     num_queries = 1000\n",
    "#     random_indices = np.random.choice(N, size=min(num_queries, N), replace=False)\n",
    "\n",
    "#     # Select the corresponding vectors\n",
    "#     query_vectors = vectors[random_indices]\n",
    "    \n",
    "    \n",
    "#     # Calculate ground truth\n",
    "#     print(f\"Calculating ground truth for {len(query_vectors)} queries (k={k})...\")\n",
    "#     start_time = time.perf_counter()\n",
    "#     bf_index = NearestNeighbors(n_neighbors=k, algorithm='brute', metric='euclidean')\n",
    "#     bf_index.fit(vectors)\n",
    "#     _, ground_truth_indices = bf_index.kneighbors(query_vectors)\n",
    "#     gt_time = time.perf_counter() - start_time\n",
    "#     print(f\"Ground truth calculated in {gt_time:.4f} s.\")\n",
    "    \n",
    "#     # Helper function for recall calculation\n",
    "#     def calculate_recall(ground_truth, found_neighbors, k):\n",
    "#         total_found = 0\n",
    "#         for i in range(len(ground_truth)):\n",
    "#             gt_set = set(ground_truth[i][:k])\n",
    "#             found_set = set(found_neighbors[i][:k])\n",
    "#             total_found += len(gt_set.intersection(found_set))\n",
    "#         return total_found / (len(ground_truth) * k)\n",
    "    \n",
    "#     # Build index with PCA disabled initially\n",
    "#     print(\"\\n--- Building HNSW index ---\")\n",
    "#     index = HNSWQueryTimePCA(dim=dim, max_elements=N, M=M, ef_construction=ef_construction,\n",
    "#                              pca_yes=False, pca_top_layers=pca_top_layers, \n",
    "#                              pca_components=pca_components)\n",
    "    \n",
    "#     # Insert all vectors\n",
    "#     start_time = time.perf_counter()\n",
    "#     for i in range(N):\n",
    "#         index.insert(vectors[i])\n",
    "#     build_time = time.perf_counter() - start_time\n",
    "#     print(f\"Index built in {build_time:.4f} s\")\n",
    "    \n",
    "#     # 1. Standard querying (pca_yes=False)\n",
    "#     print(\"\\n--- Standard HNSW Querying ---\")\n",
    "#     index.pca_yes = False\n",
    "#     standard_results = []\n",
    "    \n",
    "#     start_time = time.perf_counter()\n",
    "#     for i in range(len(query_vectors)):\n",
    "#         results = index.search(query_vectors[i], k=k)\n",
    "#         standard_results.append(results)\n",
    "#     standard_query_time = time.perf_counter() - start_time\n",
    "#     standard_avg_time = standard_query_time / len(query_vectors)\n",
    "    \n",
    "#     standard_recall = calculate_recall(ground_truth_indices, standard_results, k)\n",
    "#     print(f\"Standard Query time: {standard_query_time:.4f} s ({standard_avg_time*1000:.4f} ms/query)\")\n",
    "#     print(f\"Standard Recall@{k}: {standard_recall:.4f}\")\n",
    "    \n",
    "#     # 2. Prepare and run PCA-enabled querying\n",
    "#     print(\"\\n--- PCA-Enhanced HNSW Querying ---\")\n",
    "#     index.pca_yes = True\n",
    "#     index.finalize_pca()  # Train PCA models\n",
    "    \n",
    "#     pca_results = []\n",
    "#     start_time = time.perf_counter()\n",
    "#     for i in range(len(query_vectors)):\n",
    "#         results = index.search(query_vectors[i], k=k)\n",
    "#         pca_results.append(results)\n",
    "#     pca_query_time = time.perf_counter() - start_time\n",
    "#     pca_avg_time = pca_query_time / len(query_vectors)\n",
    "    \n",
    "#     pca_recall = calculate_recall(ground_truth_indices, pca_results, k)\n",
    "#     print(f\"PCA Query time: {pca_query_time:.4f} s ({pca_avg_time*1000:.4f} ms/query)\")\n",
    "#     print(f\"PCA Recall@{k}: {pca_recall:.4f}\")\n",
    "    \n",
    "#     # Summary\n",
    "#     print(\"\\n--- Comparison Summary ---\")\n",
    "#     print(f\"Method              | Recall@{k}  | Avg Query Time (ms)\")\n",
    "#     print(f\"--------------------|------------|-------------------\")\n",
    "#     print(f\"Standard HNSW       | {standard_recall:.4f}     | {standard_avg_time*1000:.4f}\")\n",
    "#     print(f\"PCA-Enhanced HNSW   | {pca_recall:.4f}     | {pca_avg_time*1000:.4f}\")\n",
    "    \n",
    "#     return {\n",
    "#         'standard_recall': standard_recall,\n",
    "#         'standard_query_time': standard_query_time,\n",
    "#         'pca_recall': pca_recall,\n",
    "#         'pca_query_time': pca_query_time,\n",
    "#         'build_time': build_time\n",
    "#     }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming 'vectors' is your data (e.g., GloVe vectors)\n",
    "# # vectors: np.ndarray of shape (N, dim)\n",
    "\n",
    "# # Run comparison\n",
    "# results = compare_standard_vs_pca_hnsw(\n",
    "#     vectors=vectors,\n",
    "#     num_queries=1000,  # Number of queries to run\n",
    "#     k=10,              # Number of neighbors to retrieve\n",
    "#     M=16,              # HNSW parameter\n",
    "#     ef_construction=200,  # Index build quality parameter\n",
    "#     ef_search=200,     # Search quality parameter\n",
    "#     pca_top_layers=3,  # Apply PCA to top 3 layers\n",
    "#     pca_components=50  # Reduce to 50 dimensions in those layers\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import math\n",
    "from heapq import heappush, heappop\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import pandas as pd\n",
    "\n",
    "# configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def precompute_all_query_pcas(pca_models, pca_applied_layers, query_vectors):\n",
    "#     \"\"\"\n",
    "#     Precomputes PCA projections for a batch of query vectors.\n",
    "#     Returns dict {layer_idx: array(shape=(n_queries, pca_components))}.\n",
    "#     \"\"\"\n",
    "#     pca_queries = {}\n",
    "#     query_vectors_np = np.asarray(query_vectors)\n",
    "\n",
    "#     if not pca_models or not pca_applied_layers:\n",
    "#         logger.warning(\"No PCA models or applied layers provided; skipping precomputation.\")\n",
    "#         return pca_queries\n",
    "\n",
    "#     for l_idx in pca_applied_layers:\n",
    "#         model = pca_models.get(l_idx)\n",
    "#         if model is None:\n",
    "#             logger.warning(f\"PCA model for layer {l_idx} not found; skipping.\")\n",
    "#             continue\n",
    "#         pca_queries[l_idx] = model.transform(query_vectors_np)\n",
    "#         logger.debug(f\"Layer {l_idx}: precomputed PCA queries shape {pca_queries[l_idx].shape}\")\n",
    "\n",
    "#     return pca_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def precompute_all_query_pcas(pca_models, pca_applied_layers, query_vectors):\n",
    "    \"\"\"\n",
    "    Precomputes PCA projections for a batch of query vectors.\n",
    "    Returns a list where each index corresponds to the layer number, and contains\n",
    "    either the PCA-transformed queries (np.ndarray) or None if not used.\n",
    "    \"\"\"\n",
    "    max_layer = max(pca_applied_layers) + 1 if pca_applied_layers else 0\n",
    "    pca_queries = [None] * max_layer  # indexed list for O(1) access\n",
    "\n",
    "    query_vectors_np = np.asarray(query_vectors)\n",
    "\n",
    "    for l_idx in pca_applied_layers:\n",
    "        model = pca_models[l_idx]  # direct index, avoid .get()\n",
    "        pca_queries[l_idx] = model.transform(query_vectors_np)\n",
    "\n",
    "    return pca_queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class optHNSWPCA:\n",
    "#     \"\"\"\n",
    "#     HNSW with optional PCA-based acceleration.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, dim, max_elements, M=16, ef_construction=200,\n",
    "#                  pca_yes=False, pca_top_layers=3, pca_components=50):\n",
    "#         self.dim = dim\n",
    "#         self.max_elements = max_elements\n",
    "#         self.M = M\n",
    "#         self.ef_construction = ef_construction\n",
    "#         self.pca_yes = pca_yes and pca_components < dim\n",
    "#         self.pca_top_layers = pca_top_layers\n",
    "#         self.pca_components = min(pca_components, dim)\n",
    "\n",
    "#         self.vectors = []\n",
    "#         self.layers = []\n",
    "#         self.entry_point = None\n",
    "\n",
    "#         # will populate after finalize_pca()\n",
    "#         self.pca_models = {}\n",
    "#         self.reduced_vectors = {}\n",
    "#         self.pca_applied_layers = set()\n",
    "\n",
    "#         logger.info(f\"Initialized HNSW (dim={dim}, PCA={'on' if self.pca_yes else 'off'})\")\n",
    "\n",
    "#     def _get_layer(self):\n",
    "#         ml = 1 / math.log(self.M) if self.M > 1 else 1\n",
    "#         level = int(-math.log(random.random()) * ml) if self.M > 1 else 0\n",
    "#         return max(0, level)\n",
    "\n",
    "#     def _distance(self, idx1, idx2):\n",
    "#         return np.linalg.norm(self.vectors[idx1] - self.vectors[idx2])\n",
    "\n",
    "#     def _search_layer_standard(self, query_vec, layer_idx, ep_idx, ef):\n",
    "#         \"\"\"Standard full-dimensional search on one layer.\"\"\"\n",
    "#         graph = self.layers[layer_idx]\n",
    "#         if not graph:\n",
    "#             return []\n",
    "#         if ep_idx not in graph:\n",
    "#             ep_idx = next(iter(graph))\n",
    "\n",
    "#         visited = {ep_idx}\n",
    "#         candidates = [(np.linalg.norm(query_vec - self.vectors[ep_idx]), ep_idx)]\n",
    "#         results = [(-candidates[0][0], ep_idx)]\n",
    "\n",
    "#         while candidates:\n",
    "#             dist, cur = heappop(candidates)\n",
    "#             farthest = -results[0][0]\n",
    "#             if dist > farthest and len(results) >= ef:\n",
    "#                 break\n",
    "\n",
    "#             for nb in graph[cur]:\n",
    "#                 if nb in visited: continue\n",
    "#                 visited.add(nb)\n",
    "#                 d = np.linalg.norm(query_vec - self.vectors[nb])\n",
    "#                 farthest = -results[0][0]\n",
    "#                 if d < farthest or len(results) < ef:\n",
    "#                     heappush(results, (-d, nb))\n",
    "#                     if len(results) > ef:\n",
    "#                         heappop(results)\n",
    "#                     heappush(candidates, (d, nb))\n",
    "\n",
    "#         return sorted([(-d, idx) for d, idx in results])[:ef]\n",
    "\n",
    "#     def _search_layer_pca_precomputed(self, query_idx, pca_queries, layer_idx, ep_idx, ef):\n",
    "#         \"\"\"PCA-based search using precomputed query vectors.\"\"\"\n",
    "#         graph = self.layers[layer_idx]\n",
    "#         if not graph:\n",
    "#             return []\n",
    "#         if ep_idx not in graph:\n",
    "#             ep_idx = next(iter(graph))\n",
    "\n",
    "#         qvec = pca_queries[layer_idx][query_idx]\n",
    "#         visited = {ep_idx}\n",
    "#         rv = self.reduced_vectors\n",
    "#         init = np.linalg.norm(qvec - rv[ep_idx])\n",
    "#         candidates = [(init, ep_idx)]\n",
    "#         results = [(-init, ep_idx)]\n",
    "\n",
    "#         while candidates:\n",
    "#             dist, cur = heappop(candidates)\n",
    "#             farthest = -results[0][0]\n",
    "#             if dist > farthest and len(results) >= ef:\n",
    "#                 break\n",
    "\n",
    "#             for nb in graph[cur]:\n",
    "#                 if nb in visited: continue\n",
    "#                 visited.add(nb)\n",
    "#                 d = np.linalg.norm(qvec - rv.get(nb, qvec * 0))\n",
    "#                 farthest = -results[0][0]\n",
    "#                 if d < farthest or len(results) < ef:\n",
    "#                     heappush(results, (-d, nb))\n",
    "#                     if len(results) > ef:\n",
    "#                         heappop(results)\n",
    "#                     heappush(candidates, (d, nb))\n",
    "\n",
    "#         return sorted([(-d, idx) for d, idx in results])[:ef]\n",
    "\n",
    "#     def insert(self, vector):\n",
    "#         vec = np.asarray(vector)\n",
    "#         idx = len(self.vectors)\n",
    "#         if idx >= self.max_elements:\n",
    "#             raise MemoryError(\"Index full\")\n",
    "#         self.vectors.append(vec)\n",
    "#         level = self._get_layer()\n",
    "#         while len(self.layers) <= level:\n",
    "#             self.layers.append({})\n",
    "\n",
    "#         ep = self.entry_point\n",
    "#         # search down from top\n",
    "#         for l in range(len(self.layers)-1, level, -1):\n",
    "#             if ep is None or not self.layers[l]: break\n",
    "#             res = self._search_layer_standard(vec, l, ep, ef=1)\n",
    "#             if res:\n",
    "#                 ep = res[0][1]\n",
    "\n",
    "#         for l in range(level, -1, -1):\n",
    "#             graph = self.layers[l]\n",
    "#             if ep is None and graph:\n",
    "#                 ep = next(iter(graph))\n",
    "\n",
    "#             neigh = []\n",
    "#             if ep is not None:\n",
    "#                 neigh = self._search_layer_standard(vec, l, ep, self.ef_construction)\n",
    "#                 if neigh:\n",
    "#                     ep = neigh[0][1]\n",
    "#             conns = [i for _, i in neigh[:self.M]]\n",
    "#             graph[idx] = conns\n",
    "#             for nb in conns:\n",
    "#                 graph.setdefault(nb, []).append(idx)\n",
    "#                 if len(graph[nb]) > self.M:\n",
    "#                     # prune\n",
    "#                     dists = [(self._distance(nb, c), c) for c in graph[nb]]\n",
    "#                     dists.sort()\n",
    "#                     graph[nb] = [c for _, c in dists[:self.M]]\n",
    "\n",
    "#         if self.entry_point is None or level > self._get_node_layer(self.entry_point):\n",
    "#             self.entry_point = idx\n",
    "\n",
    "#     def finalize_pca(self):\n",
    "#         if not self.pca_yes:\n",
    "#             logger.info(\"PCA disabled or no reduction needed.\")\n",
    "#             return\n",
    "\n",
    "#         layers = self.layers\n",
    "#         total = len(layers)\n",
    "#         start = max(0, total - self.pca_top_layers)\n",
    "#         self.pca_models.clear()\n",
    "#         self.reduced_vectors.clear()\n",
    "#         self.pca_applied_layers.clear()\n",
    "\n",
    "#         for l in range(start, total):\n",
    "#             graph = layers[l]\n",
    "#             idxs = [i for i in graph if i < len(self.vectors)]\n",
    "#             if len(idxs) <= self.pca_components:\n",
    "#                 continue\n",
    "#             arr = np.vstack([self.vectors[i] for i in idxs])\n",
    "#             pca = PCA(n_components=self.pca_components)\n",
    "#             pca.fit(arr)\n",
    "#             red = pca.transform(arr)\n",
    "#             self.pca_models[l] = pca\n",
    "#             self.pca_applied_layers.add(l)\n",
    "#             for i_node, vec_red in zip(idxs, red):\n",
    "#                 self.reduced_vectors[i_node] = vec_red\n",
    "#         logger.info(f\"PCA applied to layers {sorted(self.pca_applied_layers)}\")\n",
    "\n",
    "#     def search(self, query_vec, k=10):\n",
    "#         ef = max(self.ef_construction, k)\n",
    "#         ep = self.entry_point\n",
    "#         if ep is None:\n",
    "#             return []\n",
    "#         q = np.asarray(query_vec)\n",
    "\n",
    "#         # top-down traversal\n",
    "#         for l in range(len(self.layers)-1, 0, -1):\n",
    "#             if not self.layers[l]: continue\n",
    "#             res = self._search_layer_standard(q, l, ep, ef=1)\n",
    "#             if res:\n",
    "#                 ep = res[0][1]\n",
    "\n",
    "#         # ground search\n",
    "#         res = self._search_layer_standard(q, 0, ep, ef)\n",
    "#         return [i for _, i in res[:k]]\n",
    "\n",
    "#     def search_pca_opt(self, query_idx, query_vec, pca_queries, k=10):\n",
    "#         ef = max(self.ef_construction, k)\n",
    "#         ep = self.entry_point\n",
    "#         if ep is None:\n",
    "#             return []\n",
    "\n",
    "#         # top-down traversal with PCA where available\n",
    "#         for l in range(len(self.layers)-1, 0, -1):\n",
    "#             if not self.layers[l]: continue\n",
    "#             if self.pca_yes and l in self.pca_applied_layers and l in pca_queries:\n",
    "#                 res = self._search_layer_pca_precomputed(query_idx, pca_queries, l, ep, ef=1)\n",
    "#             else:\n",
    "#                 # fallback to full-dim search\n",
    "#                 res = self._search_layer_standard(query_vec, l, ep, ef=1)\n",
    "#             if res:\n",
    "#                 ep = res[0][1]\n",
    "\n",
    "#         # base layer\n",
    "#         if 0 in self.pca_applied_layers and 0 in pca_queries and self.pca_yes:\n",
    "#             res = self._search_layer_pca_precomputed(query_idx, pca_queries, 0, ep, ef)\n",
    "#         else:\n",
    "#             res = self._search_layer_standard(query_vec, 0, ep, ef)\n",
    "\n",
    "#         return [i for _, i in res[:k]]\n",
    "\n",
    "#     def _get_node_layer(self, node_idx):\n",
    "#         return max((l for l, g in enumerate(self.layers) if node_idx in g), default=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_all_query_pcas(pca_models, pca_applied_layers, query_vectors):\n",
    "    \"\"\"\n",
    "    Precomputes PCA projections for a batch of query vectors.\n",
    "    Returns a list where each index = layer number, and value is\n",
    "    either the (n_queries, pca_components) array or None.\n",
    "    \"\"\"\n",
    "    if not pca_applied_layers:\n",
    "        return []\n",
    "\n",
    "    max_layer = max(pca_applied_layers) + 1\n",
    "    pca_queries = [None] * max_layer\n",
    "    Q = np.asarray(query_vectors)\n",
    "\n",
    "    for l in pca_applied_layers:\n",
    "        model = pca_models[l]\n",
    "        pca_queries[l] = model.transform(Q)\n",
    "\n",
    "    return pca_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import logging\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from heapq import heappush, heappop\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def precompute_all_query_pcas(pca_models, pca_applied_layers, query_vectors):\n",
    "    \"\"\"\n",
    "    Precomputes PCA projections for a batch of query vectors.\n",
    "    Returns a list where each index = layer number, and value is\n",
    "    either the (n_queries, pca_components) array or None.\n",
    "    \"\"\"\n",
    "    if not pca_applied_layers:\n",
    "        return []\n",
    "\n",
    "    max_layer = max(pca_applied_layers) + 1\n",
    "    pca_queries = [None] * max_layer\n",
    "    Q = np.asarray(query_vectors)\n",
    "\n",
    "    for l in pca_applied_layers:\n",
    "        model = pca_models[l]\n",
    "        pca_queries[l] = model.transform(Q)\n",
    "\n",
    "    return pca_queries\n",
    "\n",
    "class optHNSWPCA:\n",
    "    \"\"\"\n",
    "    HNSW with optional PCA-based acceleration.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, max_elements, M=16, ef_construction=200,\n",
    "                 pca_yes=False, pca_top_layers=3, pca_components=50):\n",
    "        self.dim = dim\n",
    "        self.max_elements = max_elements\n",
    "        self.M = M\n",
    "        self.ef_construction = ef_construction\n",
    "        self.pca_yes = pca_yes and pca_components < dim\n",
    "        self.pca_top_layers = pca_top_layers\n",
    "        self.pca_components = min(pca_components, dim)\n",
    "\n",
    "        self.vectors = []\n",
    "        self.layers = []\n",
    "        self.entry_point = None\n",
    "        self.entry_point_level = -1\n",
    "\n",
    "        # PCA internals\n",
    "        self.pca_models = {}\n",
    "        self.pca_applied_layers = set()\n",
    "\n",
    "        # optimized lookup structures\n",
    "        self._rv_arr = None      # shape (max_elements, pca_components)\n",
    "        self._rv_mask = None     # boolean mask per node index\n",
    "        self._use_pca_layer = None  # boolean mask per layer index\n",
    "\n",
    "        logger.info(f\"Initialized HNSW (dim={dim}, PCA={'on' if self.pca_yes else 'off'})\")\n",
    "\n",
    "    def _get_layer(self):\n",
    "        ml = 1 / math.log(self.M) if self.M > 1 else 1\n",
    "        return max(0, int(-math.log(random.random()) * ml))\n",
    "\n",
    "    def _distance(self, idx1, idx2):\n",
    "        return np.linalg.norm(self.vectors[idx1] - self.vectors[idx2])\n",
    "\n",
    "    def _search_layer_standard(self, query_vec, layer_idx, ep_idx, ef):\n",
    "        graph = self.layers[layer_idx]\n",
    "        if not graph:\n",
    "            return []\n",
    "        if ep_idx not in graph:\n",
    "            ep_idx = next(iter(graph))\n",
    "\n",
    "        visited = {ep_idx}\n",
    "        dist0 = np.linalg.norm(query_vec - self.vectors[ep_idx])\n",
    "        candidates = [(dist0, ep_idx)]\n",
    "        results = [(-dist0, ep_idx)]\n",
    "\n",
    "        while candidates:\n",
    "            dist, cur = heappop(candidates)\n",
    "            farthest = -results[0][0]\n",
    "            if dist > farthest and len(results) >= ef:\n",
    "                break\n",
    "            for nb in graph[cur]:\n",
    "                if nb in visited:\n",
    "                    continue\n",
    "                visited.add(nb)\n",
    "                d = np.linalg.norm(query_vec - self.vectors[nb])\n",
    "                if d < farthest or len(results) < ef:\n",
    "                    heappush(results, (-d, nb))\n",
    "                    if len(results) > ef:\n",
    "                        heappop(results)\n",
    "                    heappush(candidates, (d, nb))\n",
    "\n",
    "        return sorted([(-d, idx) for d, idx in results])[:ef]\n",
    "\n",
    "    def insert(self, vector):\n",
    "        vec = np.asarray(vector)\n",
    "        idx = len(self.vectors)\n",
    "        if idx >= self.max_elements:\n",
    "            raise MemoryError(\"Index full\")\n",
    "        self.vectors.append(vec)\n",
    "        level = self._get_layer()\n",
    "        while len(self.layers) <= level:\n",
    "            self.layers.append({})\n",
    "\n",
    "        ep = self.entry_point\n",
    "        # search down from top layers to 'level'\n",
    "        for l in range(len(self.layers) - 1, level, -1):\n",
    "            if ep is None or not self.layers[l]:\n",
    "                break\n",
    "            res = self._search_layer_standard(vec, l, ep, ef=1)\n",
    "            if res:\n",
    "                ep = res[0][1]\n",
    "\n",
    "        # connect this node on each layer up to 'level'\n",
    "        for l in range(level, -1, -1):\n",
    "            graph = self.layers[l]\n",
    "            if ep is None and graph:\n",
    "                ep = next(iter(graph))\n",
    "            neigh = []\n",
    "            if ep is not None:\n",
    "                neigh = self._search_layer_standard(vec, l, ep, self.ef_construction)\n",
    "                if neigh:\n",
    "                    ep = neigh[0][1]\n",
    "\n",
    "            conns = [nid for _, nid in neigh[:self.M]]\n",
    "            graph[idx] = conns\n",
    "            for nb in conns:\n",
    "                graph.setdefault(nb, []).append(idx)\n",
    "                if len(graph[nb]) > self.M:\n",
    "                    dists = [(self._distance(nb, c), c) for c in graph[nb]]\n",
    "                    dists.sort()\n",
    "                    graph[nb] = [c for _, c in dists[:self.M]]\n",
    "\n",
    "        # update entry point based on known 'level'\n",
    "        if self.entry_point is None or level > self.entry_point_level:\n",
    "            self.entry_point = idx\n",
    "            self.entry_point_level = level\n",
    "\n",
    "    def finalize_pca(self):\n",
    "        if not self.pca_yes:\n",
    "            logger.info(\"PCA disabled or no reduction needed.\")\n",
    "            return\n",
    "\n",
    "        total_layers = len(self.layers)\n",
    "        start = max(0, total_layers - self.pca_top_layers)\n",
    "        self.pca_models.clear()\n",
    "        self.pca_applied_layers.clear()\n",
    "\n",
    "        # fit PCA on each top layer\n",
    "        for l in range(start, total_layers):\n",
    "            idxs = [i for i in self.layers[l] if i < len(self.vectors)]\n",
    "            if len(idxs) <= self.pca_components:\n",
    "                continue\n",
    "            arr = np.vstack([self.vectors[i] for i in idxs])\n",
    "            pca = PCA(n_components=self.pca_components)\n",
    "            pca.fit(arr)\n",
    "            self.pca_models[l] = pca\n",
    "            self.pca_applied_layers.add(l)\n",
    "\n",
    "        # allocate optimized structures\n",
    "        self._rv_arr = np.zeros((self.max_elements, self.pca_components), dtype=float)\n",
    "        self._rv_mask = np.zeros(self.max_elements, dtype=bool)\n",
    "        self._use_pca_layer = np.zeros(len(self.layers), dtype=bool)\n",
    "\n",
    "        # fill reduced vectors & flags\n",
    "        for l in self.pca_applied_layers:\n",
    "            idxs = [i for i in self.layers[l] if i < len(self.vectors)]\n",
    "            arr = np.vstack([self.vectors[i] for i in idxs])\n",
    "            red = self.pca_models[l].transform(arr)\n",
    "            for node, vec_red in zip(idxs, red):\n",
    "                self._rv_arr[node] = vec_red\n",
    "                self._rv_mask[node] = True\n",
    "            self._use_pca_layer[l] = True\n",
    "\n",
    "        logger.info(f\"PCA applied to layers {sorted(self.pca_applied_layers)}\")\n",
    "\n",
    "    def _search_layer_pca_precomputed(self, query_idx, query_vec, pca_queries, layer_idx, ep_idx, ef):\n",
    "        graph = self.layers[layer_idx]\n",
    "        if not graph:\n",
    "            return []\n",
    "        q_red = pca_queries[layer_idx][query_idx]\n",
    "\n",
    "        visited = {ep_idx}\n",
    "        init = np.linalg.norm(q_red - self._rv_arr[ep_idx])\n",
    "        candidates = [(init, ep_idx)]\n",
    "        results = [(-init, ep_idx)]\n",
    "\n",
    "        while candidates:\n",
    "            dist, cur = heappop(candidates)\n",
    "            farthest = -results[0][0]\n",
    "            if dist > farthest and len(results) >= ef:\n",
    "                break\n",
    "            for nb in graph[cur]:\n",
    "                if nb in visited:\n",
    "                    continue\n",
    "                visited.add(nb)\n",
    "                if self._rv_mask[nb]:\n",
    "                    d = np.linalg.norm(q_red - self._rv_arr[nb])\n",
    "                else:\n",
    "                    d = np.linalg.norm(query_vec - self.vectors[nb])\n",
    "                if d < farthest or len(results) < ef:\n",
    "                    heappush(results, (-d, nb))\n",
    "                    if len(results) > ef:\n",
    "                        heappop(results)\n",
    "                    heappush(candidates, (d, nb))\n",
    "\n",
    "        return sorted([(-d, idx) for d, idx in results])[:ef]\n",
    "\n",
    "    def search(self, query_vec, k=10):\n",
    "        ef = max(self.ef_construction, k)\n",
    "        ep = self.entry_point\n",
    "        if ep is None:\n",
    "            return []\n",
    "        q = np.asarray(query_vec)\n",
    "        # top-down traversal\n",
    "        for l in range(len(self.layers) - 1, 0, -1):\n",
    "            if not self.layers[l]:\n",
    "                continue\n",
    "            res = self._search_layer_standard(q, l, ep, ef=1)\n",
    "            if res:\n",
    "                ep = res[0][1]\n",
    "        # base layer search\n",
    "        res = self._search_layer_standard(q, 0, ep, ef)\n",
    "        return [i for _, i in res[:k]]\n",
    "\n",
    "    def search_pca_opt(self, query_idx, query_vec, pca_queries, k=10):\n",
    "        ef = max(self.ef_construction, k)\n",
    "        ep = self.entry_point\n",
    "        if ep is None:\n",
    "            return []\n",
    "        q = np.asarray(query_vec)\n",
    "\n",
    "        # top-down traversal\n",
    "        for l in range(len(self.layers) - 1, 0, -1):\n",
    "            if not self.layers[l]:\n",
    "                continue\n",
    "            if self._use_pca_layer[l] and l < len(pca_queries) and pca_queries[l] is not None:\n",
    "                res = self._search_layer_pca_precomputed(query_idx, q, pca_queries, l, ep, ef=1)\n",
    "            else:\n",
    "                res = self._search_layer_standard(q, l, ep, ef=1)\n",
    "            if res:\n",
    "                ep = res[0][1]\n",
    "\n",
    "        # base layer search\n",
    "        if self._use_pca_layer[0] and pca_queries and pca_queries[0] is not None:\n",
    "            res = self._search_layer_pca_precomputed(query_idx, q, pca_queries, 0, ep, ef)\n",
    "        else:\n",
    "            res = self._search_layer_standard(q, 0, ep, ef)\n",
    "\n",
    "        return [i for (_, i) in res[:k]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data…\n",
      "Computing ground truth…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Initialized HNSW (dim=100, PCA=off)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building standard HNSW index (no PCA)…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing standard HNSW: 100%|██████████| 400000/400000 [1:22:55<00:00, 80.40it/s]    \n",
      "INFO:__main__:Initialized HNSW (dim=100, PCA=on)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard build time: 1437.76s\n",
      "Building PCA-enabled HNSW index…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing PCA-enabled HNSW: 100%|██████████| 400000/400000 [24:58<00:00, 266.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA-enabled build time: 1498.69s\n",
      "Finalizing PCA models on index…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:PCA applied to layers [0, 1, 2, 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA fit time: 1.16s\n",
      "Evaluating standard search…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Standard query: 100%|██████████| 5000/5000 [00:17<00:00, 293.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating PCA-optimized search…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PCA-opt query: 100%|██████████| 5000/5000 [00:21<00:00, 233.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard: recall=0.5113, total_query_time=17.06s, avg_query_time=3.37ms\n",
      "PCA-opt: recall=0.1732, total_query_time=21.41s, avg_query_time=4.24ms\n",
      "\n",
      "--- Comparison Summary ---\n",
      "Method         | Recall@100 | TotalQuery(s) | AvgQuery(ms)\n",
      "Standard       | 0.5113    | 17.06       | 3.37\n",
      "PCA-optimized  | 0.1732    | 21.41       | 4.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ensure optHNSWPCA and precompute_all_query_pcas are in scope\n",
    "# from your_module import optHNSWPCA, precompute_all_query_pcas\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- parameters ---\n",
    "    num_vectors = 400_000\n",
    "    num_queries = 40_000\n",
    "    dim = 100\n",
    "    k = 100\n",
    "    M = 16\n",
    "    ef_construction = 200\n",
    "    pca_top_layers = 3\n",
    "    pca_components = 20\n",
    "\n",
    "    # load or generate data\n",
    "    print(\"Generating data…\")\n",
    "    data = vectors\n",
    "    queries = data[np.random.choice(num_vectors, num_queries, replace=False)]\n",
    "\n",
    "    # compute ground truth once\n",
    "    print(\"Computing ground truth…\")\n",
    "    nn = NearestNeighbors(n_neighbors=k, algorithm='brute')\n",
    "    nn.fit(data)\n",
    "    gt_d, gt_i = nn.kneighbors(queries)\n",
    "\n",
    "    # --- Build standard (non-PCA) index ---\n",
    "    print(\"Building standard HNSW index (no PCA)…\")\n",
    "    idx_std = optHNSWPCA(\n",
    "        dim, num_vectors + 10, M, ef_construction,\n",
    "        pca_yes=False, pca_top_layers=pca_top_layers,\n",
    "        pca_components=pca_components\n",
    "    )\n",
    "    t0 = time.perf_counter()\n",
    "    for v in tqdm(data, desc=\"Indexing standard HNSW\"):\n",
    "        idx_std.insert(v)\n",
    "    build_std = time.perf_counter() - t0\n",
    "    print(f\"Standard build time: {build_std:.2f}s\")\n",
    "\n",
    "    # --- Build PCA-enabled index ---\n",
    "    print(\"Building PCA-enabled HNSW index…\")\n",
    "    idx_pca = optHNSWPCA(\n",
    "        dim, num_vectors + 10, M, ef_construction,\n",
    "        pca_yes=True, pca_top_layers=pca_top_layers,\n",
    "        pca_components=pca_components\n",
    "    )\n",
    "    t0 = time.perf_counter()\n",
    "    for v in tqdm(data, desc=\"Indexing PCA-enabled HNSW\"):\n",
    "        idx_pca.insert(v)\n",
    "    build_pca = time.perf_counter() - t0\n",
    "    print(f\"PCA-enabled build time: {build_pca:.2f}s\")\n",
    "\n",
    "    # finalize PCA on PCA-enabled index\n",
    "    print(\"Finalizing PCA models on index…\")\n",
    "    t0 = time.perf_counter()\n",
    "    idx_pca.finalize_pca()\n",
    "    fit_pca = time.perf_counter() - t0\n",
    "    print(f\"PCA fit time: {fit_pca:.2f}s\")\n",
    "\n",
    "    # precompute PCA projections for queries\n",
    "    pca_q = precompute_all_query_pcas(idx_pca.pca_models, idx_pca.pca_applied_layers, queries)\n",
    "\n",
    "    # helper for recall\n",
    "    def recall_at_k(truth, pred): return len(set(truth) & set(pred)) / k\n",
    "\n",
    "    # evaluate standard search\n",
    "    print(\"Evaluating standard search…\")\n",
    "    total_r_std, times_std = 0, []\n",
    "    tstart = time.perf_counter()\n",
    "    for i, q in enumerate(tqdm(queries, desc=\"Standard query\")):\n",
    "        ts = time.perf_counter()\n",
    "        out = idx_std.search(q, k)\n",
    "        te = time.perf_counter()\n",
    "        times_std.append(te - ts)\n",
    "        total_r_std += recall_at_k(gt_i[i], out)\n",
    "    tot_time_std = time.perf_counter() - tstart\n",
    "    avg_time_std = np.mean(times_std) * 1000\n",
    "    recall_std = total_r_std / num_queries\n",
    "\n",
    "    # evaluate PCA-optimized search\n",
    "    print(\"Evaluating PCA-optimized search…\")\n",
    "    total_r_pca, times_pca = 0, []\n",
    "    tstart = time.perf_counter()\n",
    "    for i, q in enumerate(tqdm(queries, desc=\"PCA-opt query\")):\n",
    "        ts = time.perf_counter()\n",
    "        out = idx_pca.search_pca_opt(i, q, pca_q, k)\n",
    "        te = time.perf_counter()\n",
    "        times_pca.append(te - ts)\n",
    "        total_r_pca += recall_at_k(gt_i[i], out)\n",
    "    tot_time_pca = time.perf_counter() - tstart\n",
    "    avg_time_pca = np.mean(times_pca) * 1000\n",
    "    recall_pca = total_r_pca / num_queries\n",
    "\n",
    "    # output results\n",
    "    print(f\"Standard: recall={recall_std:.4f}, total_query_time={tot_time_std:.2f}s, avg_query_time={avg_time_std:.2f}ms\")\n",
    "    print(f\"PCA-opt: recall={recall_pca:.4f}, total_query_time={tot_time_pca:.2f}s, avg_query_time={avg_time_pca:.2f}ms\")\n",
    "\n",
    "    print(\"\\n--- Comparison Summary ---\")\n",
    "    print(f\"Method         | Recall@{k} | TotalQuery(s) | AvgQuery(ms)\")\n",
    "    print(f\"Standard       | {recall_std:.4f}    | {tot_time_std:.2f}       | {avg_time_std:.2f}\")\n",
    "    print(f\"PCA-optimized  | {recall_pca:.4f}    | {tot_time_pca:.2f}       | {avg_time_pca:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating standard search…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Standard query: 100%|██████████| 5000/5000 [00:17<00:00, 291.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating PCA-optimized search…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PCA-opt query: 100%|██████████| 5000/5000 [00:21<00:00, 233.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard: recall=0.5113, total_query_time=17.14s, avg_query_time=3.39ms\n",
      "PCA-opt: recall=0.1732, total_query_time=21.41s, avg_query_time=4.24ms\n",
      "\n",
      "--- Comparison Summary ---\n",
      "Method         | Recall@100 | TotalQuery(s) | AvgQuery(ms)\n",
      "Standard       | 0.5113    | 17.14       | 3.39\n",
      "PCA-optimized  | 0.1732    | 21.41       | 4.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "\n",
    "    # evaluate standard search\n",
    "    print(\"Evaluating standard search…\")\n",
    "    total_r_std, times_std = 0, []\n",
    "    tstart = time.perf_counter()\n",
    "    for i, q in enumerate(tqdm(queries, desc=\"Standard query\")):\n",
    "        ts = time.perf_counter()\n",
    "        out = idx_std.search(q, k)\n",
    "        te = time.perf_counter()\n",
    "        times_std.append(te - ts)\n",
    "        total_r_std += recall_at_k(gt_i[i], out)\n",
    "    tot_time_std = time.perf_counter() - tstart\n",
    "    avg_time_std = np.mean(times_std) * 1000\n",
    "    recall_std = total_r_std / num_queries\n",
    "\n",
    "    # evaluate PCA-optimized search\n",
    "    print(\"Evaluating PCA-optimized search…\")\n",
    "    total_r_pca, times_pca = 0, []\n",
    "    tstart = time.perf_counter()\n",
    "    for i, q in enumerate(tqdm(queries, desc=\"PCA-opt query\")):\n",
    "        ts = time.perf_counter()\n",
    "        out = idx_pca.search_pca_opt(i, q, pca_q, k)\n",
    "        te = time.perf_counter()\n",
    "        times_pca.append(te - ts)\n",
    "        total_r_pca += recall_at_k(gt_i[i], out)\n",
    "    tot_time_pca = time.perf_counter() - tstart\n",
    "    avg_time_pca = np.mean(times_pca) * 1000\n",
    "    recall_pca = total_r_pca / num_queries\n",
    "\n",
    "    # output results\n",
    "    print(f\"Standard: recall={recall_std:.4f}, total_query_time={tot_time_std:.2f}s, avg_query_time={avg_time_std:.2f}ms\")\n",
    "    print(f\"PCA-opt: recall={recall_pca:.4f}, total_query_time={tot_time_pca:.2f}s, avg_query_time={avg_time_pca:.2f}ms\")\n",
    "\n",
    "    print(\"\\n--- Comparison Summary ---\")\n",
    "    print(f\"Method         | Recall@{k} | TotalQuery(s) | AvgQuery(ms)\")\n",
    "    print(f\"Standard       | {recall_std:.4f}    | {tot_time_std:.2f}       | {avg_time_std:.2f}\")\n",
    "    print(f\"PCA-optimized  | {recall_pca:.4f}    | {tot_time_pca:.2f}       | {avg_time_pca:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Height of HNSW graph: 5\n"
     ]
    }
   ],
   "source": [
    "print(\"Height of HNSW graph:\", idx_pca.entry_point_level)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
