{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29be9481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import math\n",
    "from heapq import heappush, heappop\n",
    "from sklearn.decomposition import PCA # Kept for potential future use, but not active in this comparison\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import pandas as pd\n",
    "from tqdm import tqdm # For progress bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0939a52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "306d936b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors of dimension 100\n"
     ]
    }
   ],
   "source": [
    "glove_path = 'C:\\Sem 4\\Data_Science\\project\\glove.6B\\glove.6B.100d.txt' \n",
    "\n",
    "# Load GloVe vectors\n",
    "word_to_vec = {}\n",
    "words = []\n",
    "vectors = []\n",
    "\n",
    "try:\n",
    "    with open(glove_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]  # First token is the word\n",
    "            try:\n",
    "                vector = np.array(values[1:], dtype=np.float32)  # Rest are vector values\n",
    "                if vector.shape[0] == 100: # Ensure correct dimension\n",
    "                     word_to_vec[word] = vector\n",
    "                     words.append(word)\n",
    "                     vectors.append(vector)\n",
    "                else:\n",
    "                     logger.warning(f\"Skipping word '{word}': incorrect vector dimension {vector.shape[0]} (expected 100)\")\n",
    "            except ValueError:\n",
    "                logger.warning(f\"Skipping word '{word}': could not parse vector values\")\n",
    "\n",
    "    # Convert to numpy array\n",
    "    vectors = np.array(vectors, dtype=np.float32)\n",
    "    if vectors.shape[0] > 0:\n",
    "        print(f\"Loaded {len(words)} word vectors of dimension {vectors.shape[1]}\")\n",
    "    else:\n",
    "        print(\"Error: No vectors loaded. Check GloVe path and file format.\")\n",
    "        # Handle error appropriately, maybe exit or raise exception\n",
    "        raise FileNotFoundError(f\"Could not load sufficient vectors from {glove_path}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: GloVe file not found at {glove_path}\")\n",
    "    print(\"Please download glove.6B.100d.txt and update the 'glove_path' variable.\")\n",
    "    # Simulate dummy data if GloVe is not found, for testing the structure\n",
    "    print(\"Using dummy data for demonstration purposes.\")\n",
    "    dim_sim = 100\n",
    "    num_vectors_sim = 10000 # Use a smaller number for dummy data\n",
    "    vectors = np.random.rand(num_vectors_sim, dim_sim).astype(np.float32)\n",
    "    words = [f\"word_{i}\" for i in range(num_vectors_sim)]\n",
    "    print(f\"Generated {num_vectors_sim} dummy vectors of dimension {dim_sim}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad4541a",
   "metadata": {},
   "source": [
    "# HNSW class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10ef37c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HNSW_SCC:\n",
    "    \"\"\"\n",
    "    HNSW implementation with optional SCC-Aware neighbor selection heuristic\n",
    "    applied during construction in Layer 0.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, max_elements, M=16, ef_construction=200,\n",
    "                 scc_aware_yes=False, scc_alpha=0.01): # SCC-Aware params\n",
    "        self.dim = dim\n",
    "        self.max_elements = max_elements\n",
    "        self.M = M\n",
    "        self.ef_construction = ef_construction\n",
    "\n",
    "        # --- Modification Flags ---\n",
    "        self.scc_aware_yes = scc_aware_yes # Flag for SCC-Aware heuristic\n",
    "\n",
    "        # --- SCC Parameters ---\n",
    "        self.scc_alpha = scc_alpha # Weight for triangle bonus in pruning\n",
    "\n",
    "        # --- Core HNSW Data Structures ---\n",
    "        self.vectors = []\n",
    "        self.layers = []\n",
    "        self.entry_point = None\n",
    "        self.element_count = 0\n",
    "\n",
    "        mod_info = []\n",
    "        if self.scc_aware_yes:\n",
    "            mod_info.append(f\"SCC-Aware (alpha={scc_alpha})\")\n",
    "        else:\n",
    "            mod_info.append(\"Standard\")\n",
    "\n",
    "        logger.info(f\"Initialized HNSW (dim={dim}, M={M}, efC={ef_construction}, Modifications: {', '.join(mod_info)})\")\n",
    "\n",
    "    def _get_layer(self):\n",
    "        # Calculates the layer for a new node based on HNSW probability distribution\n",
    "        ml = 1 / math.log(self.M) if self.M > 1 else 1\n",
    "        level = int(-math.log(random.random()) * ml) if self.M > 1 else 0\n",
    "        return max(0, level)\n",
    "\n",
    "    def _distance(self, idx1, idx2):\n",
    "        # Calculates Euclidean distance between two vectors using their indices\n",
    "        if idx1 < len(self.vectors) and idx2 < len(self.vectors):\n",
    "            return np.linalg.norm(self.vectors[idx1] - self.vectors[idx2])\n",
    "        else:\n",
    "            logger.error(f\"Invalid index encountered in _distance: idx1={idx1}, idx2={idx2}, vector count={len(self.vectors)}\")\n",
    "            return float('inf')\n",
    "\n",
    "\n",
    "    def _search_layer_standard(self, query_vec, layer_idx, ep_idx, ef):\n",
    "        \"\"\"Standard full-dimensional search on one layer.\"\"\"\n",
    "        # Standard HNSW greedy search within a single layer\n",
    "        graph = self.layers[layer_idx]\n",
    "        if not graph: # Empty layer\n",
    "            return []\n",
    "        # Ensure entry point is valid and exists in the graph for this layer\n",
    "        if ep_idx is None or ep_idx >= len(self.vectors) or ep_idx not in graph:\n",
    "             if graph: # Pick a random node from the layer as entry point if invalid\n",
    "                ep_idx = random.choice(list(graph.keys()))\n",
    "             else: return [] # Should not happen if graph is not empty\n",
    "\n",
    "        q_vec_np = np.asarray(query_vec)\n",
    "        ep_vec = self.vectors[ep_idx]\n",
    "        init_dist = np.linalg.norm(q_vec_np - ep_vec)\n",
    "\n",
    "        visited = {ep_idx}\n",
    "        candidates = [(init_dist, ep_idx)] # Min-heap (distance, node_idx)\n",
    "        results = [(-init_dist, ep_idx)] # Max-heap (-distance, node_idx)\n",
    "\n",
    "        while candidates:\n",
    "            dist, cur_idx = heappop(candidates)\n",
    "            farthest_dist_neg, _ = results[0]\n",
    "\n",
    "            if dist > -farthest_dist_neg and len(results) >= ef: break\n",
    "\n",
    "            for nb_idx in graph.get(cur_idx, []): # Use .get for safety\n",
    "                if nb_idx not in visited:\n",
    "                    visited.add(nb_idx)\n",
    "                    # Check if neighbor index is valid before accessing vector\n",
    "                    if nb_idx >= len(self.vectors):\n",
    "                         logger.warning(f\"Neighbor index {nb_idx} out of bounds (max: {len(self.vectors)-1}). Skipping.\")\n",
    "                         continue\n",
    "                    nb_vec = self.vectors[nb_idx]\n",
    "                    d = np.linalg.norm(q_vec_np - nb_vec)\n",
    "\n",
    "                    farthest_dist_neg, _ = results[0]\n",
    "                    if d < -farthest_dist_neg or len(results) < ef:\n",
    "                        heappush(results, (-d, nb_idx))\n",
    "                        if len(results) > ef: heappop(results)\n",
    "                        heappush(candidates, (d, nb_idx))\n",
    "\n",
    "        return sorted([(-d, idx) for d, idx in results])[:ef]\n",
    "\n",
    "    def _apply_scc_aware_pruning(self, nb_idx, layer_idx):\n",
    "        \"\"\"\n",
    "        Applies SCC-aware pruning heuristic to the neighbors of nb_idx in layer_idx.\n",
    "        Only applied if self.scc_aware_yes is True and layer_idx is 0.\n",
    "        \"\"\"\n",
    "        graph = self.layers[layer_idx]\n",
    "        current_neighbors = list(graph.get(nb_idx, []))\n",
    "        num_neighbors = len(current_neighbors)\n",
    "\n",
    "        if num_neighbors <= self.M: return # No pruning needed\n",
    "\n",
    "        adjusted_scores = []\n",
    "        neighbor_set = set(current_neighbors) # Faster lookups\n",
    "\n",
    "        for c_idx in current_neighbors:\n",
    "             # Ensure candidate index is valid\n",
    "             if c_idx >= len(self.vectors): continue\n",
    "             distance = self._distance(nb_idx, c_idx)\n",
    "             triangle_count = 0\n",
    "             # Check connections between c_idx and other valid neighbors of nb_idx\n",
    "             c_neighbors_in_layer = set(graph.get(c_idx, []))\n",
    "             for other_c_idx in current_neighbors:\n",
    "                 # Ensure other candidate index is valid and different\n",
    "                  if c_idx != other_c_idx and other_c_idx < len(self.vectors) and other_c_idx in c_neighbors_in_layer:\n",
    "                       triangle_count += 1\n",
    "\n",
    "             # Score: Lower is better. Subtract bonus for triangles.\n",
    "             adjusted_score = distance - (self.scc_alpha * triangle_count)\n",
    "             adjusted_scores.append((adjusted_score, c_idx))\n",
    "\n",
    "        # Sort by adjusted score (ascending)\n",
    "        adjusted_scores.sort()\n",
    "\n",
    "        # Keep the top M neighbors based on the adjusted score\n",
    "        graph[nb_idx] = [c for score, c in adjusted_scores[:self.M]]\n",
    "\n",
    "\n",
    "    def insert(self, vector):\n",
    "        \"\"\"Inserts a vector into the HNSW index.\"\"\"\n",
    "        vec = np.asarray(vector)\n",
    "        idx = self.element_count\n",
    "        if idx >= self.max_elements:\n",
    "            raise MemoryError(f\"Index full (max_elements={self.max_elements} reached)\")\n",
    "\n",
    "        # Append vector before incrementing element_count if index is based on it\n",
    "        if idx == len(self.vectors): self.vectors.append(vec)\n",
    "        elif idx < len(self.vectors): self.vectors[idx] = vec # Overwrite pre-allocated? Unlikely for simple append.\n",
    "        else: raise IndexError(\"Inconsistent vector index during insertion\")\n",
    "\n",
    "        self.element_count += 1\n",
    "        level = self._get_layer()\n",
    "\n",
    "        while len(self.layers) <= level: self.layers.append({})\n",
    "\n",
    "        ep = self.entry_point\n",
    "        current_top_layer = len(self.layers) - 1\n",
    "\n",
    "        # Phase 1: Find entry points in upper layers\n",
    "        for l in range(current_top_layer, level, -1):\n",
    "             if ep is None or not self.layers[l]: continue\n",
    "             res = self._search_layer_standard(vec, l, ep, ef=1)\n",
    "             if res: ep = res[0][1]\n",
    "\n",
    "        # Phase 2: Insert node layer by layer from `level` down to 0\n",
    "        for l in range(min(level, current_top_layer), -1, -1):\n",
    "            graph = self.layers[l]\n",
    "            neighbors = []\n",
    "            layer_ep = ep\n",
    "            if layer_ep is None or layer_ep >= len(self.vectors) or layer_ep not in graph:\n",
    "                 if graph: layer_ep = random.choice(list(graph.keys()))\n",
    "                 else: layer_ep = None # Layer is empty\n",
    "\n",
    "            if layer_ep is not None:\n",
    "                 neighbors = self._search_layer_standard(vec, l, layer_ep, self.ef_construction)\n",
    "                 if neighbors: ep = neighbors[0][1] # Update overall entry point\n",
    "\n",
    "            selected_connections = [neighbor_idx for _, neighbor_idx in neighbors[:self.M]]\n",
    "            graph[idx] = selected_connections # Set new node's neighbors\n",
    "\n",
    "            for nb_idx in selected_connections:\n",
    "                 # Ensure neighbor exists and is valid before adding back-link\n",
    "                 if nb_idx >= len(self.vectors): continue # Skip invalid neighbor index\n",
    "                 graph.setdefault(nb_idx, []).append(idx) # Add back-link\n",
    "\n",
    "                 # --- Pruning Step (Potentially Modified) ---\n",
    "                 if len(graph[nb_idx]) > self.M:\n",
    "                      if self.scc_aware_yes and l == 0:\n",
    "                           # Apply SCC-Aware pruning only on Layer 0\n",
    "                           self._apply_scc_aware_pruning(nb_idx, l)\n",
    "                      else:\n",
    "                           # Standard HNSW pruning (distance-based)\n",
    "                           dists = [(self._distance(nb_idx, c), c) for c in graph[nb_idx] if c < len(self.vectors)] # Ensure valid indices\n",
    "                           dists.sort()\n",
    "                           graph[nb_idx] = [c for _, c in dists[:self.M]]\n",
    "\n",
    "        # Update the global entry point\n",
    "        if self.entry_point is None or level > self._get_node_layer(self.entry_point):\n",
    "            self.entry_point = idx\n",
    "\n",
    "    def search(self, query_vec, k=10, ef_search=None):\n",
    "        \"\"\"Performs KNN search using the standard HNSW algorithm.\"\"\"\n",
    "        ef = max(self.ef_construction if ef_search is None else ef_search, k)\n",
    "        ep = self.entry_point\n",
    "        if ep is None or ep >= len(self.vectors): return [] # No elements or invalid entry point\n",
    "\n",
    "        q = np.asarray(query_vec)\n",
    "        current_top_layer = len(self.layers) - 1\n",
    "\n",
    "        # Phase 1: Search upper layers to find entry point for Layer 0\n",
    "        for l in range(current_top_layer, 0, -1):\n",
    "             if not self.layers[l]: continue\n",
    "             res = self._search_layer_standard(q, l, ep, ef=1)\n",
    "             if res: ep = res[0][1]\n",
    "             # Ensure ep remains valid after update\n",
    "             if ep >= len(self.vectors):\n",
    "                 logger.error(f\"Entry point became invalid ({ep}) during top-down search at layer {l}.\")\n",
    "                 # Fallback: maybe try finding a random node in layer 0 if possible?\n",
    "                 # Or simply return empty list as search cannot reliably continue.\n",
    "                 return []\n",
    "\n",
    "\n",
    "        # Phase 2: Perform search on Layer 0\n",
    "        res = self._search_layer_standard(q, 0, ep, ef)\n",
    "        return [idx for _, idx in res[:k]] # Return top K results\n",
    "\n",
    "    def _get_node_layer(self, node_idx):\n",
    "        \"\"\"Finds the highest layer a node exists in.\"\"\"\n",
    "        if node_idx is None or node_idx >= len(self.vectors): return -1 # Handle invalid index\n",
    "        for l in range(len(self.layers) - 1, -1, -1):\n",
    "            if node_idx in self.layers[l]: return l\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfd91cd",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d96381ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a subset of 50000 vectors and 1000 queries.\n",
      "Computing ground truth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Initialized HNSW (dim=100, M=16, efC=100, Modifications: Standard)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth computed in 0.36s\n",
      "\n",
      "Building standard HNSW index (scc_aware_yes=False)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing standard HNSW: 100%|██████████| 50000/50000 [05:51<00:00, 142.33it/s]\n",
      "INFO:__main__:Initialized HNSW (dim=100, M=16, efC=100, Modifications: SCC-Aware (alpha=0.01))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard build time: 351.32s\n",
      "\n",
      "Building SCC-Aware HNSW index (scc_aware_yes=True)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing SCC-Aware HNSW: 100%|██████████| 50000/50000 [07:55<00:00, 105.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCC-Aware build time: 475.88s\n",
      "\n",
      "Evaluating standard search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Standard query: 100%|██████████| 1000/1000 [00:05<00:00, 193.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating SCC-Aware search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SCC-Aware query: 100%|██████████| 1000/1000 [00:05<00:00, 188.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Comparison Summary ---\n",
      "Parameters: k=10, M=16, efConstruction=100, efSearch=100, SCC_Alpha=0.01\n",
      "Dataset size: 50000 vectors, 1000 queries\n",
      "----------------------------------------------------------------------\n",
      "Method           | Build Time (s)  | Recall@10  | Total Query (s) | Avg Query (ms) \n",
      "----------------------------------------------------------------------\n",
      "Standard HNSW    | 351.32          | 0.5204     | 5.16            | 5.07           \n",
      "SCC-Aware HNSW   | 475.88          | 0.5224     | 5.30            | 5.21           \n",
      "----------------------------------------------------------------------\n",
      "Avg Query Time Speedup (SCC vs Std): 0.97x\n",
      "Recall Difference (SCC - Std): +0.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # --- Parameters ---\n",
    "    num_vectors = min(50_000, len(vectors)) # Use a smaller subset or max available\n",
    "    num_queries = min(1_000, num_vectors // 10)\n",
    "    if num_vectors <= 0:\n",
    "         print(\"Cannot proceed without data. Exiting.\")\n",
    "         exit()\n",
    "\n",
    "    # Randomly sample data and queries if using a subset\n",
    "    if num_vectors < len(vectors):\n",
    "        data_indices = np.random.choice(len(vectors), num_vectors, replace=False)\n",
    "        data = vectors[data_indices]\n",
    "        query_indices = np.random.choice(num_vectors, num_queries, replace=False)\n",
    "        queries = data[query_indices]\n",
    "        print(f\"Using a subset of {num_vectors} vectors and {num_queries} queries.\")\n",
    "    else:\n",
    "        data = vectors\n",
    "        query_indices = np.random.choice(num_vectors, num_queries, replace=False)\n",
    "        queries = data[query_indices]\n",
    "        print(f\"Using the full dataset of {num_vectors} vectors and {num_queries} queries.\")\n",
    "\n",
    "\n",
    "    dim = data.shape[1]\n",
    "    k = 10 # Number of neighbors to retrieve\n",
    "    M = 16 # Max connections per node per layer\n",
    "    ef_construction = 100 # Candidate list size during construction\n",
    "    ef_search = 100 # Candidate list size during search\n",
    "\n",
    "    # SCC-Aware parameter\n",
    "    scc_heuristic_alpha = 0.01 # <-- ** Role Explained Below **\n",
    "\n",
    "    # --- Compute Ground Truth ---\n",
    "    print(\"Computing ground truth...\")\n",
    "    t_gt_start = time.perf_counter()\n",
    "    nn_brute = NearestNeighbors(n_neighbors=k, algorithm='brute', metric='euclidean')\n",
    "    nn_brute.fit(data)\n",
    "    gt_d, gt_i = nn_brute.kneighbors(queries)\n",
    "    t_gt_end = time.perf_counter()\n",
    "    print(f\"Ground truth computed in {t_gt_end - t_gt_start:.2f}s\")\n",
    "\n",
    "    # --- Build Standard Index ---\n",
    "    print(\"\\nBuilding standard HNSW index (scc_aware_yes=False)...\")\n",
    "    idx_std = HNSW_SCC(\n",
    "        dim, num_vectors + 10, M, ef_construction,\n",
    "        scc_aware_yes=False  # Standard HNSW\n",
    "    )\n",
    "    t0 = time.perf_counter()\n",
    "    for v in tqdm(data, desc=\"Indexing standard HNSW\"):\n",
    "        idx_std.insert(v)\n",
    "    build_std_time = time.perf_counter() - t0\n",
    "    print(f\"Standard build time: {build_std_time:.2f}s\")\n",
    "\n",
    "    # --- Build SCC-Aware Index ---\n",
    "    print(\"\\nBuilding SCC-Aware HNSW index (scc_aware_yes=True)...\")\n",
    "    idx_scc = HNSW_SCC(\n",
    "        dim, num_vectors + 10, M, ef_construction,\n",
    "        scc_aware_yes=True,  # Enable SCC-Aware heuristic\n",
    "        scc_alpha=scc_heuristic_alpha\n",
    "    )\n",
    "    t0 = time.perf_counter()\n",
    "    for v in tqdm(data, desc=\"Indexing SCC-Aware HNSW\"):\n",
    "        idx_scc.insert(v)\n",
    "    build_scc_time = time.perf_counter() - t0\n",
    "    print(f\"SCC-Aware build time: {build_scc_time:.2f}s\")\n",
    "\n",
    "\n",
    "    # --- Evaluate Standard Search ---\n",
    "    print(\"\\nEvaluating standard search...\")\n",
    "    total_recall_std = 0\n",
    "    times_std = []\n",
    "    results_std = []\n",
    "    t_eval_std_start = time.perf_counter()\n",
    "    for i, q in enumerate(tqdm(queries, desc=\"Standard query\")):\n",
    "        ts = time.perf_counter()\n",
    "        out_indices = idx_std.search(q, k, ef_search=ef_search)\n",
    "        te = time.perf_counter()\n",
    "        times_std.append(te - ts)\n",
    "        if k > 0: # Avoid division by zero if k=0\n",
    "             recall = len(set(gt_i[i]) & set(out_indices)) / k\n",
    "             total_recall_std += recall\n",
    "        results_std.append(out_indices)\n",
    "    t_eval_std_end = time.perf_counter()\n",
    "\n",
    "    avg_time_std_ms = np.mean(times_std) * 1000 if times_std else 0\n",
    "    recall_std = total_recall_std / num_queries if num_queries > 0 else 0\n",
    "    total_query_time_std = t_eval_std_end - t_eval_std_start\n",
    "\n",
    "    # --- Evaluate SCC-Aware Search ---\n",
    "    print(\"\\nEvaluating SCC-Aware search...\")\n",
    "    total_recall_scc = 0\n",
    "    times_scc = []\n",
    "    results_scc = []\n",
    "    t_eval_scc_start = time.perf_counter()\n",
    "    for i, q in enumerate(tqdm(queries, desc=\"SCC-Aware query\")):\n",
    "        ts = time.perf_counter()\n",
    "        out_indices = idx_scc.search(q, k, ef_search=ef_search)\n",
    "        te = time.perf_counter()\n",
    "        times_scc.append(te - ts)\n",
    "        if k > 0: # Avoid division by zero if k=0\n",
    "            recall = len(set(gt_i[i]) & set(out_indices)) / k\n",
    "            total_recall_scc += recall\n",
    "        results_scc.append(out_indices)\n",
    "    t_eval_scc_end = time.perf_counter()\n",
    "\n",
    "    avg_time_scc_ms = np.mean(times_scc) * 1000 if times_scc else 0\n",
    "    recall_scc = total_recall_scc / num_queries if num_queries > 0 else 0\n",
    "    total_query_time_scc = t_eval_scc_end - t_eval_scc_start\n",
    "\n",
    "    # --- Output Results ---\n",
    "    print(\"\\n--- Comparison Summary ---\")\n",
    "    print(f\"Parameters: k={k}, M={M}, efConstruction={ef_construction}, efSearch={ef_search}, SCC_Alpha={scc_heuristic_alpha if idx_scc.scc_aware_yes else 'N/A'}\")\n",
    "    print(f\"Dataset size: {num_vectors} vectors, {num_queries} queries\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'Method':<16} | {'Build Time (s)':<15} | {'Recall@' + str(k):<10} | {'Total Query (s)':<15} | {'Avg Query (ms)':<15}\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'Standard HNSW':<16} | {build_std_time:<15.2f} | {recall_std:<10.4f} | {total_query_time_std:<15.2f} | {avg_time_std_ms:<15.2f}\")\n",
    "    print(f\"{'SCC-Aware HNSW':<16} | {build_scc_time:<15.2f} | {recall_scc:<10.4f} | {total_query_time_scc:<15.2f} | {avg_time_scc_ms:<15.2f}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    # Optional: Calculate speedup/recall difference\n",
    "    if avg_time_std_ms > 0 and avg_time_scc_ms > 0:\n",
    "         speedup = avg_time_std_ms / avg_time_scc_ms\n",
    "         print(f\"Avg Query Time Speedup (SCC vs Std): {speedup:.2f}x\")\n",
    "    recall_diff = recall_scc - recall_std\n",
    "    print(f\"Recall Difference (SCC - Std): {recall_diff:+.4f}\")\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09711933",
   "metadata": {},
   "outputs": [],
   "source": [
    "scc_heuristic_alpha = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ca4e89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Initialized HNSW (dim=100, M=16, efC=100, Modifications: SCC-Aware (alpha=0.1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building SCC-Aware HNSW index (scc_aware_yes=True)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing SCC-Aware HNSW: 100%|██████████| 50000/50000 [08:25<00:00, 98.83it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCC-Aware build time: 505.92s\n",
      "\n",
      "Evaluating standard search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Standard query: 100%|██████████| 1000/1000 [00:05<00:00, 177.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating SCC-Aware search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SCC-Aware query: 100%|██████████| 1000/1000 [00:06<00:00, 157.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Comparison Summary ---\n",
      "Parameters: k=10, M=16, efConstruction=100, efSearch=100, SCC_Alpha=0.1\n",
      "Dataset size: 50000 vectors, 1000 queries\n",
      "----------------------------------------------------------------------\n",
      "Method           | Build Time (s)  | Recall@10  | Total Query (s) | Avg Query (ms) \n",
      "----------------------------------------------------------------------\n",
      "Standard HNSW    | 351.32          | 0.5204     | 5.63            | 5.53           \n",
      "SCC-Aware HNSW   | 505.92          | 0.5889     | 6.36            | 6.25           \n",
      "----------------------------------------------------------------------\n",
      "Avg Query Time Speedup (SCC vs Std): 0.88x\n",
      "Recall Difference (SCC - Std): +0.0685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":    \n",
    "    print(\"\\nBuilding SCC-Aware HNSW index (scc_aware_yes=True)...\")\n",
    "    idx_scc = HNSW_SCC(\n",
    "        dim, num_vectors + 10, M, ef_construction,\n",
    "        scc_aware_yes=True,  # Enable SCC-Aware heuristic\n",
    "        scc_alpha=scc_heuristic_alpha\n",
    "    )\n",
    "    t0 = time.perf_counter()\n",
    "    for v in tqdm(data, desc=\"Indexing SCC-Aware HNSW\"):\n",
    "        idx_scc.insert(v)\n",
    "    build_scc_time = time.perf_counter() - t0\n",
    "    print(f\"SCC-Aware build time: {build_scc_time:.2f}s\")\n",
    "\n",
    "\n",
    "    # --- Evaluate Standard Search ---\n",
    "    print(\"\\nEvaluating standard search...\")\n",
    "    total_recall_std = 0\n",
    "    times_std = []\n",
    "    results_std = []\n",
    "    t_eval_std_start = time.perf_counter()\n",
    "    for i, q in enumerate(tqdm(queries, desc=\"Standard query\")):\n",
    "        ts = time.perf_counter()\n",
    "        out_indices = idx_std.search(q, k, ef_search=ef_search)\n",
    "        te = time.perf_counter()\n",
    "        times_std.append(te - ts)\n",
    "        if k > 0: # Avoid division by zero if k=0\n",
    "             recall = len(set(gt_i[i]) & set(out_indices)) / k\n",
    "             total_recall_std += recall\n",
    "        results_std.append(out_indices)\n",
    "    t_eval_std_end = time.perf_counter()\n",
    "\n",
    "    avg_time_std_ms = np.mean(times_std) * 1000 if times_std else 0\n",
    "    recall_std = total_recall_std / num_queries if num_queries > 0 else 0\n",
    "    total_query_time_std = t_eval_std_end - t_eval_std_start\n",
    "\n",
    "    # --- Evaluate SCC-Aware Search ---\n",
    "    print(\"\\nEvaluating SCC-Aware search...\")\n",
    "    total_recall_scc = 0\n",
    "    times_scc = []\n",
    "    results_scc = []\n",
    "    t_eval_scc_start = time.perf_counter()\n",
    "    for i, q in enumerate(tqdm(queries, desc=\"SCC-Aware query\")):\n",
    "        ts = time.perf_counter()\n",
    "        out_indices = idx_scc.search(q, k, ef_search=ef_search)\n",
    "        te = time.perf_counter()\n",
    "        times_scc.append(te - ts)\n",
    "        if k > 0: # Avoid division by zero if k=0\n",
    "            recall = len(set(gt_i[i]) & set(out_indices)) / k\n",
    "            total_recall_scc += recall\n",
    "        results_scc.append(out_indices)\n",
    "    t_eval_scc_end = time.perf_counter()\n",
    "\n",
    "    avg_time_scc_ms = np.mean(times_scc) * 1000 if times_scc else 0\n",
    "    recall_scc = total_recall_scc / num_queries if num_queries > 0 else 0\n",
    "    total_query_time_scc = t_eval_scc_end - t_eval_scc_start\n",
    "\n",
    "    # --- Output Results ---\n",
    "    print(\"\\n--- Comparison Summary ---\")\n",
    "    print(f\"Parameters: k={k}, M={M}, efConstruction={ef_construction}, efSearch={ef_search}, SCC_Alpha={scc_heuristic_alpha if idx_scc.scc_aware_yes else 'N/A'}\")\n",
    "    print(f\"Dataset size: {num_vectors} vectors, {num_queries} queries\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'Method':<16} | {'Build Time (s)':<15} | {'Recall@' + str(k):<10} | {'Total Query (s)':<15} | {'Avg Query (ms)':<15}\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'Standard HNSW':<16} | {build_std_time:<15.2f} | {recall_std:<10.4f} | {total_query_time_std:<15.2f} | {avg_time_std_ms:<15.2f}\")\n",
    "    print(f\"{'SCC-Aware HNSW':<16} | {build_scc_time:<15.2f} | {recall_scc:<10.4f} | {total_query_time_scc:<15.2f} | {avg_time_scc_ms:<15.2f}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    # Optional: Calculate speedup/recall difference\n",
    "    if avg_time_std_ms > 0 and avg_time_scc_ms > 0:\n",
    "         speedup = avg_time_std_ms / avg_time_scc_ms\n",
    "         print(f\"Avg Query Time Speedup (SCC vs Std): {speedup:.2f}x\")\n",
    "    recall_diff = recall_scc - recall_std\n",
    "    print(f\"Recall Difference (SCC - Std): {recall_diff:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84b6966b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scc_heuristic_alpha = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f082c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Initialized HNSW (dim=100, M=16, efC=100, Modifications: SCC-Aware (alpha=0.5))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building SCC-Aware HNSW index (scc_aware_yes=True)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing SCC-Aware HNSW: 100%|██████████| 50000/50000 [09:19<00:00, 89.42it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCC-Aware build time: 559.15s\n",
      "\n",
      "Evaluating standard search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Standard query: 100%|██████████| 1000/1000 [00:05<00:00, 181.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating SCC-Aware search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SCC-Aware query: 100%|██████████| 1000/1000 [00:07<00:00, 128.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Comparison Summary ---\n",
      "Parameters: k=10, M=16, efConstruction=100, efSearch=100, SCC_Alpha=0.5\n",
      "Dataset size: 50000 vectors, 1000 queries\n",
      "----------------------------------------------------------------------\n",
      "Method           | Build Time (s)  | Recall@10  | Total Query (s) | Avg Query (ms) \n",
      "----------------------------------------------------------------------\n",
      "Standard HNSW    | 351.32          | 0.5204     | 5.52            | 5.42           \n",
      "SCC-Aware HNSW   | 559.15          | 0.6832     | 7.78            | 7.66           \n",
      "----------------------------------------------------------------------\n",
      "Avg Query Time Speedup (SCC vs Std): 0.71x\n",
      "Recall Difference (SCC - Std): +0.1628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":    \n",
    "    print(\"\\nBuilding SCC-Aware HNSW index (scc_aware_yes=True)...\")\n",
    "    idx_scc = HNSW_SCC(\n",
    "        dim, num_vectors + 10, M, ef_construction,\n",
    "        scc_aware_yes=True,  # Enable SCC-Aware heuristic\n",
    "        scc_alpha=scc_heuristic_alpha\n",
    "    )\n",
    "    t0 = time.perf_counter()\n",
    "    for v in tqdm(data, desc=\"Indexing SCC-Aware HNSW\"):\n",
    "        idx_scc.insert(v)\n",
    "    build_scc_time = time.perf_counter() - t0\n",
    "    print(f\"SCC-Aware build time: {build_scc_time:.2f}s\")\n",
    "\n",
    "\n",
    "    # --- Evaluate Standard Search ---\n",
    "    print(\"\\nEvaluating standard search...\")\n",
    "    total_recall_std = 0\n",
    "    times_std = []\n",
    "    results_std = []\n",
    "    t_eval_std_start = time.perf_counter()\n",
    "    for i, q in enumerate(tqdm(queries, desc=\"Standard query\")):\n",
    "        ts = time.perf_counter()\n",
    "        out_indices = idx_std.search(q, k, ef_search=ef_search)\n",
    "        te = time.perf_counter()\n",
    "        times_std.append(te - ts)\n",
    "        if k > 0: # Avoid division by zero if k=0\n",
    "             recall = len(set(gt_i[i]) & set(out_indices)) / k\n",
    "             total_recall_std += recall\n",
    "        results_std.append(out_indices)\n",
    "    t_eval_std_end = time.perf_counter()\n",
    "\n",
    "    avg_time_std_ms = np.mean(times_std) * 1000 if times_std else 0\n",
    "    recall_std = total_recall_std / num_queries if num_queries > 0 else 0\n",
    "    total_query_time_std = t_eval_std_end - t_eval_std_start\n",
    "\n",
    "    # --- Evaluate SCC-Aware Search ---\n",
    "    print(\"\\nEvaluating SCC-Aware search...\")\n",
    "    total_recall_scc = 0\n",
    "    times_scc = []\n",
    "    results_scc = []\n",
    "    t_eval_scc_start = time.perf_counter()\n",
    "    for i, q in enumerate(tqdm(queries, desc=\"SCC-Aware query\")):\n",
    "        ts = time.perf_counter()\n",
    "        out_indices = idx_scc.search(q, k, ef_search=ef_search)\n",
    "        te = time.perf_counter()\n",
    "        times_scc.append(te - ts)\n",
    "        if k > 0: # Avoid division by zero if k=0\n",
    "            recall = len(set(gt_i[i]) & set(out_indices)) / k\n",
    "            total_recall_scc += recall\n",
    "        results_scc.append(out_indices)\n",
    "    t_eval_scc_end = time.perf_counter()\n",
    "\n",
    "    avg_time_scc_ms = np.mean(times_scc) * 1000 if times_scc else 0\n",
    "    recall_scc = total_recall_scc / num_queries if num_queries > 0 else 0\n",
    "    total_query_time_scc = t_eval_scc_end - t_eval_scc_start\n",
    "\n",
    "    # --- Output Results ---\n",
    "    print(\"\\n--- Comparison Summary ---\")\n",
    "    print(f\"Parameters: k={k}, M={M}, efConstruction={ef_construction}, efSearch={ef_search}, SCC_Alpha={scc_heuristic_alpha if idx_scc.scc_aware_yes else 'N/A'}\")\n",
    "    print(f\"Dataset size: {num_vectors} vectors, {num_queries} queries\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'Method':<16} | {'Build Time (s)':<15} | {'Recall@' + str(k):<10} | {'Total Query (s)':<15} | {'Avg Query (ms)':<15}\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'Standard HNSW':<16} | {build_std_time:<15.2f} | {recall_std:<10.4f} | {total_query_time_std:<15.2f} | {avg_time_std_ms:<15.2f}\")\n",
    "    print(f\"{'SCC-Aware HNSW':<16} | {build_scc_time:<15.2f} | {recall_scc:<10.4f} | {total_query_time_scc:<15.2f} | {avg_time_scc_ms:<15.2f}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    # Optional: Calculate speedup/recall difference\n",
    "    if avg_time_std_ms > 0 and avg_time_scc_ms > 0:\n",
    "         speedup = avg_time_std_ms / avg_time_scc_ms\n",
    "         print(f\"Avg Query Time Speedup (SCC vs Std): {speedup:.2f}x\")\n",
    "    recall_diff = recall_scc - recall_std\n",
    "    print(f\"Recall Difference (SCC - Std): {recall_diff:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a46580a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a subset of 50000 vectors and 1000 queries.\n",
      "Computing ground truth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Initialized HNSW (dim=100, M=16, efC=200, Modifications: Standard)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth computed in 0.23s\n",
      "\n",
      "Building standard HNSW index (scc_aware_yes=False)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing standard HNSW: 100%|██████████| 50000/50000 [08:22<00:00, 99.40it/s] \n",
      "INFO:__main__:Initialized HNSW (dim=100, M=16, efC=200, Modifications: SCC-Aware (alpha=0.5))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard build time: 503.00s\n",
      "\n",
      "Building SCC-Aware HNSW index (scc_aware_yes=True)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing SCC-Aware HNSW: 100%|██████████| 50000/50000 [14:00<00:00, 59.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCC-Aware build time: 840.12s\n",
      "\n",
      "Evaluating standard search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Standard query: 100%|██████████| 1000/1000 [00:05<00:00, 186.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating SCC-Aware search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SCC-Aware query: 100%|██████████| 1000/1000 [00:07<00:00, 137.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Comparison Summary ---\n",
      "Parameters: k=10, M=16, efConstruction=200, efSearch=100, SCC_Alpha=0.5\n",
      "Dataset size: 50000 vectors, 1000 queries\n",
      "----------------------------------------------------------------------\n",
      "Method           | Build Time (s)  | Recall@10  | Total Query (s) | Avg Query (ms) \n",
      "----------------------------------------------------------------------\n",
      "Standard HNSW    | 503.00          | 0.5763     | 5.36            | 5.26           \n",
      "SCC-Aware HNSW   | 840.12          | 0.7361     | 7.25            | 7.14           \n",
      "----------------------------------------------------------------------\n",
      "Avg Query Time Speedup (SCC vs Std): 0.74x\n",
      "Recall Difference (SCC - Std): +0.1598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # --- Parameters ---\n",
    "    num_vectors = min(50_000, len(vectors)) # Use a smaller subset or max available\n",
    "    num_queries = min(1_000, num_vectors // 10)\n",
    "    if num_vectors <= 0:\n",
    "         print(\"Cannot proceed without data. Exiting.\")\n",
    "         exit()\n",
    "\n",
    "    # Randomly sample data and queries if using a subset\n",
    "    if num_vectors < len(vectors):\n",
    "        data_indices = np.random.choice(len(vectors), num_vectors, replace=False)\n",
    "        data = vectors[data_indices]\n",
    "        query_indices = np.random.choice(num_vectors, num_queries, replace=False)\n",
    "        queries = data[query_indices]\n",
    "        print(f\"Using a subset of {num_vectors} vectors and {num_queries} queries.\")\n",
    "    else:\n",
    "        data = vectors\n",
    "        query_indices = np.random.choice(num_vectors, num_queries, replace=False)\n",
    "        queries = data[query_indices]\n",
    "        print(f\"Using the full dataset of {num_vectors} vectors and {num_queries} queries.\")\n",
    "\n",
    "\n",
    "    dim = data.shape[1]\n",
    "    k = 10 # Number of neighbors to retrieve\n",
    "    M = 16 # Max connections per node per layer\n",
    "    ef_construction = 200 # Candidate list size during construction\n",
    "    ef_search = 100 # Candidate list size during search\n",
    "\n",
    "    # SCC-Aware parameter\n",
    "    scc_heuristic_alpha = 0.5 # <-- ** Role Explained Below **\n",
    "\n",
    "    # --- Compute Ground Truth ---\n",
    "    print(\"Computing ground truth...\")\n",
    "    t_gt_start = time.perf_counter()\n",
    "    nn_brute = NearestNeighbors(n_neighbors=k, algorithm='brute', metric='euclidean')\n",
    "    nn_brute.fit(data)\n",
    "    gt_d, gt_i = nn_brute.kneighbors(queries)\n",
    "    t_gt_end = time.perf_counter()\n",
    "    print(f\"Ground truth computed in {t_gt_end - t_gt_start:.2f}s\")\n",
    "\n",
    "    # --- Build Standard Index ---\n",
    "    print(\"\\nBuilding standard HNSW index (scc_aware_yes=False)...\")\n",
    "    idx_std = HNSW_SCC(\n",
    "        dim, num_vectors + 10, M, ef_construction,\n",
    "        scc_aware_yes=False  # Standard HNSW\n",
    "    )\n",
    "    t0 = time.perf_counter()\n",
    "    for v in tqdm(data, desc=\"Indexing standard HNSW\"):\n",
    "        idx_std.insert(v)\n",
    "    build_std_time = time.perf_counter() - t0\n",
    "    print(f\"Standard build time: {build_std_time:.2f}s\")\n",
    "\n",
    "    # --- Build SCC-Aware Index ---\n",
    "    print(\"\\nBuilding SCC-Aware HNSW index (scc_aware_yes=True)...\")\n",
    "    idx_scc = HNSW_SCC(\n",
    "        dim, num_vectors + 10, M, ef_construction,\n",
    "        scc_aware_yes=True,  # Enable SCC-Aware heuristic\n",
    "        scc_alpha=scc_heuristic_alpha\n",
    "    )\n",
    "    t0 = time.perf_counter()\n",
    "    for v in tqdm(data, desc=\"Indexing SCC-Aware HNSW\"):\n",
    "        idx_scc.insert(v)\n",
    "    build_scc_time = time.perf_counter() - t0\n",
    "    print(f\"SCC-Aware build time: {build_scc_time:.2f}s\")\n",
    "\n",
    "\n",
    "    # --- Evaluate Standard Search ---\n",
    "    print(\"\\nEvaluating standard search...\")\n",
    "    total_recall_std = 0\n",
    "    times_std = []\n",
    "    results_std = []\n",
    "    t_eval_std_start = time.perf_counter()\n",
    "    for i, q in enumerate(tqdm(queries, desc=\"Standard query\")):\n",
    "        ts = time.perf_counter()\n",
    "        out_indices = idx_std.search(q, k, ef_search=ef_search)\n",
    "        te = time.perf_counter()\n",
    "        times_std.append(te - ts)\n",
    "        if k > 0: # Avoid division by zero if k=0\n",
    "             recall = len(set(gt_i[i]) & set(out_indices)) / k\n",
    "             total_recall_std += recall\n",
    "        results_std.append(out_indices)\n",
    "    t_eval_std_end = time.perf_counter()\n",
    "\n",
    "    avg_time_std_ms = np.mean(times_std) * 1000 if times_std else 0\n",
    "    recall_std = total_recall_std / num_queries if num_queries > 0 else 0\n",
    "    total_query_time_std = t_eval_std_end - t_eval_std_start\n",
    "\n",
    "    # --- Evaluate SCC-Aware Search ---\n",
    "    print(\"\\nEvaluating SCC-Aware search...\")\n",
    "    total_recall_scc = 0\n",
    "    times_scc = []\n",
    "    results_scc = []\n",
    "    t_eval_scc_start = time.perf_counter()\n",
    "    for i, q in enumerate(tqdm(queries, desc=\"SCC-Aware query\")):\n",
    "        ts = time.perf_counter()\n",
    "        out_indices = idx_scc.search(q, k, ef_search=ef_search)\n",
    "        te = time.perf_counter()\n",
    "        times_scc.append(te - ts)\n",
    "        if k > 0: # Avoid division by zero if k=0\n",
    "            recall = len(set(gt_i[i]) & set(out_indices)) / k\n",
    "            total_recall_scc += recall\n",
    "        results_scc.append(out_indices)\n",
    "    t_eval_scc_end = time.perf_counter()\n",
    "\n",
    "    avg_time_scc_ms = np.mean(times_scc) * 1000 if times_scc else 0\n",
    "    recall_scc = total_recall_scc / num_queries if num_queries > 0 else 0\n",
    "    total_query_time_scc = t_eval_scc_end - t_eval_scc_start\n",
    "\n",
    "    # --- Output Results ---\n",
    "    print(\"\\n--- Comparison Summary ---\")\n",
    "    print(f\"Parameters: k={k}, M={M}, efConstruction={ef_construction}, efSearch={ef_search}, SCC_Alpha={scc_heuristic_alpha if idx_scc.scc_aware_yes else 'N/A'}\")\n",
    "    print(f\"Dataset size: {num_vectors} vectors, {num_queries} queries\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'Method':<16} | {'Build Time (s)':<15} | {'Recall@' + str(k):<10} | {'Total Query (s)':<15} | {'Avg Query (ms)':<15}\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'Standard HNSW':<16} | {build_std_time:<15.2f} | {recall_std:<10.4f} | {total_query_time_std:<15.2f} | {avg_time_std_ms:<15.2f}\")\n",
    "    print(f\"{'SCC-Aware HNSW':<16} | {build_scc_time:<15.2f} | {recall_scc:<10.4f} | {total_query_time_scc:<15.2f} | {avg_time_scc_ms:<15.2f}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    # Optional: Calculate speedup/recall difference\n",
    "    if avg_time_std_ms > 0 and avg_time_scc_ms > 0:\n",
    "         speedup = avg_time_std_ms / avg_time_scc_ms\n",
    "         print(f\"Avg Query Time Speedup (SCC vs Std): {speedup:.2f}x\")\n",
    "    recall_diff = recall_scc - recall_std\n",
    "    print(f\"Recall Difference (SCC - Std): {recall_diff:+.4f}\")\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06ac909e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a subset of 50000 vectors and 1000 queries.\n",
      "Computing ground truth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Initialized HNSW (dim=100, M=16, efC=200, Modifications: Standard)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth computed in 0.35s\n",
      "\n",
      "Building standard HNSW index (scc_aware_yes=False)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing standard HNSW: 100%|██████████| 50000/50000 [08:34<00:00, 97.22it/s] \n",
      "INFO:__main__:Initialized HNSW (dim=100, M=16, efC=200, Modifications: SCC-Aware (alpha=0.5))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard build time: 514.30s\n",
      "\n",
      "Building SCC-Aware HNSW index (scc_aware_yes=True)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing SCC-Aware HNSW: 100%|██████████| 50000/50000 [10:10<00:00, 81.88it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCC-Aware build time: 610.68s\n",
      "\n",
      "Evaluating standard search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Standard query: 100%|██████████| 1000/1000 [00:05<00:00, 193.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating SCC-Aware search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SCC-Aware query: 100%|██████████| 1000/1000 [00:07<00:00, 131.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Comparison Summary ---\n",
      "Parameters: k=10, M=16, efConstruction=200, efSearch=200, SCC_Alpha=0.5\n",
      "Dataset size: 50000 vectors, 1000 queries\n",
      "----------------------------------------------------------------------\n",
      "Method           | Build Time (s)  | Recall@10  | Total Query (s) | Avg Query (ms) \n",
      "----------------------------------------------------------------------\n",
      "Standard HNSW    | 514.30          | 0.6101     | 5.17            | 5.08           \n",
      "SCC-Aware HNSW   | 610.68          | 0.8222     | 7.62            | 7.53           \n",
      "----------------------------------------------------------------------\n",
      "Avg Query Time Speedup (SCC vs Std): 0.68x\n",
      "Recall Difference (SCC - Std): +0.2121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # --- Parameters ---\n",
    "    num_vectors = min(50_000, len(vectors)) # Use a smaller subset or max available\n",
    "    num_queries = min(1_000, num_vectors // 10)\n",
    "    if num_vectors <= 0:\n",
    "         print(\"Cannot proceed without data. Exiting.\")\n",
    "         exit()\n",
    "\n",
    "    # Randomly sample data and queries if using a subset\n",
    "    if num_vectors < len(vectors):\n",
    "        data_indices = np.random.choice(len(vectors), num_vectors, replace=False)\n",
    "        data = vectors[data_indices]\n",
    "        query_indices = np.random.choice(num_vectors, num_queries, replace=False)\n",
    "        queries = data[query_indices]\n",
    "        print(f\"Using a subset of {num_vectors} vectors and {num_queries} queries.\")\n",
    "    else:\n",
    "        data = vectors\n",
    "        query_indices = np.random.choice(num_vectors, num_queries, replace=False)\n",
    "        queries = data[query_indices]\n",
    "        print(f\"Using the full dataset of {num_vectors} vectors and {num_queries} queries.\")\n",
    "\n",
    "\n",
    "    dim = data.shape[1]\n",
    "    k = 10 # Number of neighbors to retrieve\n",
    "    M = 16 # Max connections per node per layer\n",
    "    ef_construction = 200 # Candidate list size during construction\n",
    "    ef_search = 200 # Candidate list size during search\n",
    "\n",
    "    # SCC-Aware parameter\n",
    "    scc_heuristic_alpha = 0.5 # <-- ** Role Explained Below **\n",
    "\n",
    "    # --- Compute Ground Truth ---\n",
    "    print(\"Computing ground truth...\")\n",
    "    t_gt_start = time.perf_counter()\n",
    "    nn_brute = NearestNeighbors(n_neighbors=k, algorithm='brute', metric='euclidean')\n",
    "    nn_brute.fit(data)\n",
    "    gt_d, gt_i = nn_brute.kneighbors(queries)\n",
    "    t_gt_end = time.perf_counter()\n",
    "    print(f\"Ground truth computed in {t_gt_end - t_gt_start:.2f}s\")\n",
    "\n",
    "    # --- Build Standard Index ---\n",
    "    print(\"\\nBuilding standard HNSW index (scc_aware_yes=False)...\")\n",
    "    idx_std = HNSW_SCC(\n",
    "        dim, num_vectors + 10, M, ef_construction,\n",
    "        scc_aware_yes=False  # Standard HNSW\n",
    "    )\n",
    "    t0 = time.perf_counter()\n",
    "    for v in tqdm(data, desc=\"Indexing standard HNSW\"):\n",
    "        idx_std.insert(v)\n",
    "    build_std_time = time.perf_counter() - t0\n",
    "    print(f\"Standard build time: {build_std_time:.2f}s\")\n",
    "\n",
    "    # --- Build SCC-Aware Index ---\n",
    "    print(\"\\nBuilding SCC-Aware HNSW index (scc_aware_yes=True)...\")\n",
    "    idx_scc = HNSW_SCC(\n",
    "        dim, num_vectors + 10, M, ef_construction,\n",
    "        scc_aware_yes=True,  # Enable SCC-Aware heuristic\n",
    "        scc_alpha=scc_heuristic_alpha\n",
    "    )\n",
    "    t0 = time.perf_counter()\n",
    "    for v in tqdm(data, desc=\"Indexing SCC-Aware HNSW\"):\n",
    "        idx_scc.insert(v)\n",
    "    build_scc_time = time.perf_counter() - t0\n",
    "    print(f\"SCC-Aware build time: {build_scc_time:.2f}s\")\n",
    "\n",
    "\n",
    "    # --- Evaluate Standard Search ---\n",
    "    print(\"\\nEvaluating standard search...\")\n",
    "    total_recall_std = 0\n",
    "    times_std = []\n",
    "    results_std = []\n",
    "    t_eval_std_start = time.perf_counter()\n",
    "    for i, q in enumerate(tqdm(queries, desc=\"Standard query\")):\n",
    "        ts = time.perf_counter()\n",
    "        out_indices = idx_std.search(q, k, ef_search=ef_search)\n",
    "        te = time.perf_counter()\n",
    "        times_std.append(te - ts)\n",
    "        if k > 0: # Avoid division by zero if k=0\n",
    "             recall = len(set(gt_i[i]) & set(out_indices)) / k\n",
    "             total_recall_std += recall\n",
    "        results_std.append(out_indices)\n",
    "    t_eval_std_end = time.perf_counter()\n",
    "\n",
    "    avg_time_std_ms = np.mean(times_std) * 1000 if times_std else 0\n",
    "    recall_std = total_recall_std / num_queries if num_queries > 0 else 0\n",
    "    total_query_time_std = t_eval_std_end - t_eval_std_start\n",
    "\n",
    "    # --- Evaluate SCC-Aware Search ---\n",
    "    print(\"\\nEvaluating SCC-Aware search...\")\n",
    "    total_recall_scc = 0\n",
    "    times_scc = []\n",
    "    results_scc = []\n",
    "    t_eval_scc_start = time.perf_counter()\n",
    "    for i, q in enumerate(tqdm(queries, desc=\"SCC-Aware query\")):\n",
    "        ts = time.perf_counter()\n",
    "        out_indices = idx_scc.search(q, k, ef_search=ef_search)\n",
    "        te = time.perf_counter()\n",
    "        times_scc.append(te - ts)\n",
    "        if k > 0: # Avoid division by zero if k=0\n",
    "            recall = len(set(gt_i[i]) & set(out_indices)) / k\n",
    "            total_recall_scc += recall\n",
    "        results_scc.append(out_indices)\n",
    "    t_eval_scc_end = time.perf_counter()\n",
    "\n",
    "    avg_time_scc_ms = np.mean(times_scc) * 1000 if times_scc else 0\n",
    "    recall_scc = total_recall_scc / num_queries if num_queries > 0 else 0\n",
    "    total_query_time_scc = t_eval_scc_end - t_eval_scc_start\n",
    "\n",
    "    # --- Output Results ---\n",
    "    print(\"\\n--- Comparison Summary ---\")\n",
    "    print(f\"Parameters: k={k}, M={M}, efConstruction={ef_construction}, efSearch={ef_search}, SCC_Alpha={scc_heuristic_alpha if idx_scc.scc_aware_yes else 'N/A'}\")\n",
    "    print(f\"Dataset size: {num_vectors} vectors, {num_queries} queries\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'Method':<16} | {'Build Time (s)':<15} | {'Recall@' + str(k):<10} | {'Total Query (s)':<15} | {'Avg Query (ms)':<15}\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'Standard HNSW':<16} | {build_std_time:<15.2f} | {recall_std:<10.4f} | {total_query_time_std:<15.2f} | {avg_time_std_ms:<15.2f}\")\n",
    "    print(f\"{'SCC-Aware HNSW':<16} | {build_scc_time:<15.2f} | {recall_scc:<10.4f} | {total_query_time_scc:<15.2f} | {avg_time_scc_ms:<15.2f}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    # Optional: Calculate speedup/recall difference\n",
    "    if avg_time_std_ms > 0 and avg_time_scc_ms > 0:\n",
    "         speedup = avg_time_std_ms / avg_time_scc_ms\n",
    "         print(f\"Avg Query Time Speedup (SCC vs Std): {speedup:.2f}x\")\n",
    "    recall_diff = recall_scc - recall_std\n",
    "    print(f\"Recall Difference (SCC - Std): {recall_diff:+.4f}\")\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93bd0f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a subset of 50000 vectors and 1000 queries.\n",
      "Computing ground truth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Initialized HNSW (dim=100, M=16, efC=200, Modifications: Standard)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth computed in 0.65s\n",
      "\n",
      "Building standard HNSW index (scc_aware_yes=False)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing standard HNSW: 100%|██████████| 50000/50000 [08:30<00:00, 97.90it/s] \n",
      "INFO:__main__:Initialized HNSW (dim=100, M=16, efC=200, Modifications: SCC-Aware (alpha=0.5))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard build time: 510.74s\n",
      "\n",
      "Building SCC-Aware HNSW index (scc_aware_yes=True)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing SCC-Aware HNSW: 100%|██████████| 50000/50000 [14:16<00:00, 58.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCC-Aware build time: 856.09s\n",
      "\n",
      "Evaluating standard search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Standard query: 100%|██████████| 1000/1000 [00:09<00:00, 101.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating SCC-Aware search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SCC-Aware query: 100%|██████████| 1000/1000 [00:13<00:00, 74.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Comparison Summary ---\n",
      "Parameters: k=10, M=16, efConstruction=200, efSearch=200, SCC_Alpha=0.5\n",
      "Dataset size: 50000 vectors, 1000 queries\n",
      "-------------------------------------------------------------------------------------\n",
      "Method           | Build Time (s)  | Recall@10  | Avg Neighbor Dist  | Total Query (s) | Avg Query (ms) \n",
      "-------------------------------------------------------------------------------------\n",
      "Standard HNSW    | 510.74          | 0.5909     | 3.3110             | 9.89            | 9.52           \n",
      "SCC-Aware HNSW   | 856.09          | 0.8151     | 3.0909             | 13.43           | 13.04          \n",
      "-------------------------------------------------------------------------------------\n",
      "Avg Query Time Speedup (SCC vs Std): 0.73x\n",
      "Recall Difference (SCC - Std): +0.2242\n",
      "Avg Distance Difference (SCC - Std): -0.2201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # --- Parameters ---\n",
    "    num_vectors = min(50_000, len(vectors)) # Use a smaller subset or max available\n",
    "    num_queries = min(1_000, num_vectors // 10)\n",
    "    if num_vectors <= 0:\n",
    "        print(\"Cannot proceed without data. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "    # Randomly sample data and queries if using a subset\n",
    "    if num_vectors < len(vectors):\n",
    "        data_indices = np.random.choice(len(vectors), num_vectors, replace=False)\n",
    "        data = vectors[data_indices]\n",
    "        query_indices = np.random.choice(num_vectors, num_queries, replace=False)\n",
    "        queries = data[query_indices]\n",
    "        print(f\"Using a subset of {num_vectors} vectors and {num_queries} queries.\")\n",
    "    else:\n",
    "        data = vectors\n",
    "        query_indices = np.random.choice(num_vectors, num_queries, replace=False)\n",
    "        queries = data[query_indices]\n",
    "        print(f\"Using the full dataset of {num_vectors} vectors and {num_queries} queries.\")\n",
    "\n",
    "\n",
    "    dim = data.shape[1]\n",
    "    k = 10 # Number of neighbors to retrieve\n",
    "    M = 16 # Max connections per node per layer\n",
    "    ef_construction = 200 # Candidate list size during construction, adjust as needed\n",
    "    ef_search = 200 # Candidate list size during search, adjust as needed\n",
    "\n",
    "    # SCC-Aware parameter (adjust this value to test different settings)\n",
    "    scc_heuristic_alpha = 0.5 # Example value from your notebook\n",
    "\n",
    "    # --- Compute Ground Truth ---\n",
    "    print(\"Computing ground truth...\")\n",
    "    t_gt_start = time.perf_counter()\n",
    "    nn_brute = NearestNeighbors(n_neighbors=k, algorithm='brute', metric='euclidean')\n",
    "    nn_brute.fit(data)\n",
    "    gt_d, gt_i = nn_brute.kneighbors(queries)\n",
    "    t_gt_end = time.perf_counter()\n",
    "    print(f\"Ground truth computed in {t_gt_end - t_gt_start:.2f}s\")\n",
    "\n",
    "    # --- Build Standard Index ---\n",
    "    print(\"\\nBuilding standard HNSW index (scc_aware_yes=False)...\")\n",
    "    idx_std = HNSW_SCC(\n",
    "        dim, num_vectors + 10, M, ef_construction,\n",
    "        scc_aware_yes=False # Standard HNSW\n",
    "    )\n",
    "    t0 = time.perf_counter()\n",
    "    for v in tqdm(data, desc=\"Indexing standard HNSW\"):\n",
    "        idx_std.insert(v)\n",
    "    build_std_time = time.perf_counter() - t0\n",
    "    print(f\"Standard build time: {build_std_time:.2f}s\")\n",
    "\n",
    "    # --- Build SCC-Aware Index ---\n",
    "    print(\"\\nBuilding SCC-Aware HNSW index (scc_aware_yes=True)...\")\n",
    "    idx_scc = HNSW_SCC(\n",
    "        dim, num_vectors + 10, M, ef_construction,\n",
    "        scc_aware_yes=True, # Enable SCC-Aware heuristic\n",
    "        scc_alpha=scc_heuristic_alpha\n",
    "    )\n",
    "    t0 = time.perf_counter()\n",
    "    for v in tqdm(data, desc=\"Indexing SCC-Aware HNSW\"):\n",
    "        idx_scc.insert(v)\n",
    "    build_scc_time = time.perf_counter() - t0\n",
    "    print(f\"SCC-Aware build time: {build_scc_time:.2f}s\")\n",
    "\n",
    "\n",
    "    # --- Initialize lists for average distances ---\n",
    "    all_avg_dists_std = [] # To store average distance per query for standard HNSW\n",
    "    all_avg_dists_scc = [] # To store average distance per query for SCC-aware HNSW\n",
    "\n",
    "    # --- Evaluate Standard Search ---\n",
    "    print(\"\\nEvaluating standard search...\")\n",
    "    total_recall_std = 0\n",
    "    times_std = []\n",
    "    results_std = []\n",
    "    t_eval_std_start = time.perf_counter()\n",
    "    for i, q in enumerate(tqdm(queries, desc=\"Standard query\")):\n",
    "        ts = time.perf_counter()\n",
    "        out_indices = idx_std.search(q, k, ef_search=ef_search)\n",
    "        te = time.perf_counter()\n",
    "        times_std.append(te - ts)\n",
    "\n",
    "        # Recall calculation\n",
    "        if k > 0: # Avoid division by zero if k=0\n",
    "            recall = len(set(gt_i[i]) & set(out_indices)) / k\n",
    "            total_recall_std += recall\n",
    "        results_std.append(out_indices)\n",
    "\n",
    "        # --- Calculate Average Distance for this query (Standard HNSW) ---\n",
    "        query_distances_std = []\n",
    "        if out_indices: # Check if any neighbors were found\n",
    "            try:\n",
    "                # Ensure indices are valid before accessing data\n",
    "                valid_indices = [idx for idx in out_indices if idx < len(data)]\n",
    "                if valid_indices:\n",
    "                    neighbor_vectors = data[valid_indices] # Get vectors using valid indices\n",
    "                    for neighbor_vec in neighbor_vectors:\n",
    "                        dist = np.linalg.norm(q - neighbor_vec) # Calculate Euclidean distance\n",
    "                        query_distances_std.append(dist)\n",
    "            except IndexError as e:\n",
    "                logger.error(f\"Index error during std distance calculation for query {i}: {e}. Indices: {out_indices}\")\n",
    "                # Handle error, e.g., append NaN or skip\n",
    "\n",
    "        # Store the average distance for *this* query\n",
    "        avg_query_dist_std = np.mean(query_distances_std) if query_distances_std else np.nan\n",
    "        all_avg_dists_std.append(avg_query_dist_std)\n",
    "        # --- End Average Distance Calculation ---\n",
    "\n",
    "    t_eval_std_end = time.perf_counter()\n",
    "\n",
    "    avg_time_std_ms = np.mean(times_std) * 1000 if times_std else 0\n",
    "    recall_std = total_recall_std / num_queries if num_queries > 0 else 0\n",
    "    total_query_time_std = t_eval_std_end - t_eval_std_start\n",
    "    # Calculate overall average distance for standard HNSW, ignoring NaNs\n",
    "    overall_avg_dist_std = np.nanmean(all_avg_dists_std) if all_avg_dists_std else 0\n",
    "\n",
    "    # --- Evaluate SCC-Aware Search ---\n",
    "    print(\"\\nEvaluating SCC-Aware search...\")\n",
    "    total_recall_scc = 0\n",
    "    times_scc = []\n",
    "    results_scc = []\n",
    "    t_eval_scc_start = time.perf_counter()\n",
    "    for i, q in enumerate(tqdm(queries, desc=\"SCC-Aware query\")):\n",
    "        ts = time.perf_counter()\n",
    "        out_indices = idx_scc.search(q, k, ef_search=ef_search)\n",
    "        te = time.perf_counter()\n",
    "        times_scc.append(te - ts)\n",
    "\n",
    "        # Recall calculation\n",
    "        if k > 0: # Avoid division by zero if k=0\n",
    "            recall = len(set(gt_i[i]) & set(out_indices)) / k\n",
    "            total_recall_scc += recall\n",
    "        results_scc.append(out_indices)\n",
    "\n",
    "        # --- Calculate Average Distance for this query (SCC-Aware HNSW) ---\n",
    "        query_distances_scc = []\n",
    "        if out_indices: # Check if any neighbors were found\n",
    "            try:\n",
    "                # Ensure indices are valid before accessing data\n",
    "                valid_indices = [idx for idx in out_indices if idx < len(data)]\n",
    "                if valid_indices:\n",
    "                    neighbor_vectors = data[valid_indices] # Get vectors using valid indices\n",
    "                    for neighbor_vec in neighbor_vectors:\n",
    "                        dist = np.linalg.norm(q - neighbor_vec) # Calculate Euclidean distance\n",
    "                        query_distances_scc.append(dist)\n",
    "            except IndexError as e:\n",
    "                logger.error(f\"Index error during scc distance calculation for query {i}: {e}. Indices: {out_indices}\")\n",
    "                # Handle error, e.g., append NaN or skip\n",
    "\n",
    "        # Store the average distance for *this* query\n",
    "        avg_query_dist_scc = np.mean(query_distances_scc) if query_distances_scc else np.nan\n",
    "        all_avg_dists_scc.append(avg_query_dist_scc)\n",
    "        # --- End Average Distance Calculation ---\n",
    "\n",
    "    t_eval_scc_end = time.perf_counter()\n",
    "\n",
    "    avg_time_scc_ms = np.mean(times_scc) * 1000 if times_scc else 0\n",
    "    recall_scc = total_recall_scc / num_queries if num_queries > 0 else 0\n",
    "    total_query_time_scc = t_eval_scc_end - t_eval_scc_start\n",
    "    # Calculate overall average distance for SCC-aware HNSW, ignoring NaNs\n",
    "    overall_avg_dist_scc = np.nanmean(all_avg_dists_scc) if all_avg_dists_scc else 0\n",
    "\n",
    "    # --- Output Results ---\n",
    "    print(\"\\n--- Comparison Summary ---\")\n",
    "    print(f\"Parameters: k={k}, M={M}, efConstruction={ef_construction}, efSearch={ef_search}, SCC_Alpha={scc_heuristic_alpha if idx_scc.scc_aware_yes else 'N/A'}\")\n",
    "    print(f\"Dataset size: {num_vectors} vectors, {num_queries} queries\")\n",
    "    print(\"-\" * 85) # Adjusted width for new column\n",
    "    # Updated Header\n",
    "    print(f\"{'Method':<16} | {'Build Time (s)':<15} | {'Recall@' + str(k):<10} | {'Avg Neighbor Dist':<18} | {'Total Query (s)':<15} | {'Avg Query (ms)':<15}\")\n",
    "    print(\"-\" * 85) # Adjusted width for new column\n",
    "    # Updated Data Rows\n",
    "    print(f\"{'Standard HNSW':<16} | {build_std_time:<15.2f} | {recall_std:<10.4f} | {overall_avg_dist_std:<18.4f} | {total_query_time_std:<15.2f} | {avg_time_std_ms:<15.2f}\")\n",
    "    print(f\"{'SCC-Aware HNSW':<16} | {build_scc_time:<15.2f} | {recall_scc:<10.4f} | {overall_avg_dist_scc:<18.4f} | {total_query_time_scc:<15.2f} | {avg_time_scc_ms:<15.2f}\")\n",
    "    print(\"-\" * 85) # Adjusted width for new column\n",
    "\n",
    "    # Optional: Calculate speedup/recall/distance difference\n",
    "    if avg_time_std_ms > 0 and avg_time_scc_ms > 0:\n",
    "        speedup = avg_time_std_ms / avg_time_scc_ms\n",
    "        print(f\"Avg Query Time Speedup (SCC vs Std): {speedup:.2f}x\")\n",
    "    recall_diff = recall_scc - recall_std\n",
    "    print(f\"Recall Difference (SCC - Std): {recall_diff:+.4f}\")\n",
    "    dist_diff = overall_avg_dist_scc - overall_avg_dist_std\n",
    "    print(f\"Avg Distance Difference (SCC - Std): {dist_diff:+.4f}\")\n",
    "\n",
    "# %% # Keep the cell marker if it exists in the original notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb34b87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a subset of 50000 vectors and 1000 queries.\n",
      "Computing ground truth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Initialized HNSW (dim=100, M=16, efC=200, Modifications: Standard)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth computed in 0.35s\n",
      "\n",
      "Building standard HNSW index (scc_aware_yes=False)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing standard HNSW: 100%|██████████| 50000/50000 [07:43<00:00, 107.80it/s]\n",
      "INFO:__main__:Initialized HNSW (dim=100, M=16, efC=200, Modifications: SCC-Aware (alpha=0.5))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard build time: 463.83s\n",
      "\n",
      "Building SCC-Aware HNSW index (scc_aware_yes=True)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing SCC-Aware HNSW: 100%|██████████| 50000/50000 [09:07<00:00, 91.37it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCC-Aware build time: 547.21s\n",
      "\n",
      "Evaluating standard search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Standard query: 100%|██████████| 1000/1000 [00:06<00:00, 165.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating SCC-Aware search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SCC-Aware query: 100%|██████████| 1000/1000 [00:08<00:00, 113.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Comparison Summary ---\n",
      "Parameters: k=100, M=16, efConstruction=200, efSearch=200, SCC_Alpha=0.5\n",
      "Dataset size: 50000 vectors, 1000 queries\n",
      "-------------------------------------------------------------------------------------\n",
      "Method           | Build Time (s)  | Recall@100 | Avg Neighbor Dist  | Total Query (s) | Avg Query (ms) \n",
      "-------------------------------------------------------------------------------------\n",
      "Standard HNSW    | 463.83          | 0.6484     | 3.6314             | 6.06            | 5.41           \n",
      "SCC-Aware HNSW   | 547.21          | 0.7700     | 3.5761             | 8.82            | 8.13           \n",
      "-------------------------------------------------------------------------------------\n",
      "Avg Query Time Speedup (SCC vs Std): 0.67x\n",
      "Recall Difference (SCC - Std): +0.1215\n",
      "Avg Distance Difference (SCC - Std): -0.0553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # --- Parameters ---\n",
    "    num_vectors = min(50_000, len(vectors)) # Use a smaller subset or max available\n",
    "    num_queries = min(1_000, num_vectors // 10)\n",
    "    if num_vectors <= 0:\n",
    "        print(\"Cannot proceed without data. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "    # Randomly sample data and queries if using a subset\n",
    "    if num_vectors < len(vectors):\n",
    "        data_indices = np.random.choice(len(vectors), num_vectors, replace=False)\n",
    "        data = vectors[data_indices]\n",
    "        query_indices = np.random.choice(num_vectors, num_queries, replace=False)\n",
    "        queries = data[query_indices]\n",
    "        print(f\"Using a subset of {num_vectors} vectors and {num_queries} queries.\")\n",
    "    else:\n",
    "        data = vectors\n",
    "        query_indices = np.random.choice(num_vectors, num_queries, replace=False)\n",
    "        queries = data[query_indices]\n",
    "        print(f\"Using the full dataset of {num_vectors} vectors and {num_queries} queries.\")\n",
    "\n",
    "\n",
    "    dim = data.shape[1]\n",
    "    k = 100 # Number of neighbors to retrieve\n",
    "    M = 16 # Max connections per node per layer\n",
    "    ef_construction = 200 # Candidate list size during construction, adjust as needed\n",
    "    ef_search = 200 # Candidate list size during search, adjust as needed\n",
    "\n",
    "    # SCC-Aware parameter (adjust this value to test different settings)\n",
    "    scc_heuristic_alpha = 0.5 # Example value from your notebook\n",
    "\n",
    "    # --- Compute Ground Truth ---\n",
    "    print(\"Computing ground truth...\")\n",
    "    t_gt_start = time.perf_counter()\n",
    "    nn_brute = NearestNeighbors(n_neighbors=k, algorithm='brute', metric='euclidean')\n",
    "    nn_brute.fit(data)\n",
    "    gt_d, gt_i = nn_brute.kneighbors(queries)\n",
    "    t_gt_end = time.perf_counter()\n",
    "    print(f\"Ground truth computed in {t_gt_end - t_gt_start:.2f}s\")\n",
    "\n",
    "    # --- Build Standard Index ---\n",
    "    print(\"\\nBuilding standard HNSW index (scc_aware_yes=False)...\")\n",
    "    idx_std = HNSW_SCC(\n",
    "        dim, num_vectors + 10, M, ef_construction,\n",
    "        scc_aware_yes=False # Standard HNSW\n",
    "    )\n",
    "    t0 = time.perf_counter()\n",
    "    for v in tqdm(data, desc=\"Indexing standard HNSW\"):\n",
    "        idx_std.insert(v)\n",
    "    build_std_time = time.perf_counter() - t0\n",
    "    print(f\"Standard build time: {build_std_time:.2f}s\")\n",
    "\n",
    "    # --- Build SCC-Aware Index ---\n",
    "    print(\"\\nBuilding SCC-Aware HNSW index (scc_aware_yes=True)...\")\n",
    "    idx_scc = HNSW_SCC(\n",
    "        dim, num_vectors + 10, M, ef_construction,\n",
    "        scc_aware_yes=True, # Enable SCC-Aware heuristic\n",
    "        scc_alpha=scc_heuristic_alpha\n",
    "    )\n",
    "    t0 = time.perf_counter()\n",
    "    for v in tqdm(data, desc=\"Indexing SCC-Aware HNSW\"):\n",
    "        idx_scc.insert(v)\n",
    "    build_scc_time = time.perf_counter() - t0\n",
    "    print(f\"SCC-Aware build time: {build_scc_time:.2f}s\")\n",
    "\n",
    "\n",
    "    # --- Initialize lists for average distances ---\n",
    "    all_avg_dists_std = [] # To store average distance per query for standard HNSW\n",
    "    all_avg_dists_scc = [] # To store average distance per query for SCC-aware HNSW\n",
    "\n",
    "    # --- Evaluate Standard Search ---\n",
    "    print(\"\\nEvaluating standard search...\")\n",
    "    total_recall_std = 0\n",
    "    times_std = []\n",
    "    results_std = []\n",
    "    t_eval_std_start = time.perf_counter()\n",
    "    for i, q in enumerate(tqdm(queries, desc=\"Standard query\")):\n",
    "        ts = time.perf_counter()\n",
    "        out_indices = idx_std.search(q, k, ef_search=ef_search)\n",
    "        te = time.perf_counter()\n",
    "        times_std.append(te - ts)\n",
    "\n",
    "        # Recall calculation\n",
    "        if k > 0: # Avoid division by zero if k=0\n",
    "            recall = len(set(gt_i[i]) & set(out_indices)) / k\n",
    "            total_recall_std += recall\n",
    "        results_std.append(out_indices)\n",
    "\n",
    "        # --- Calculate Average Distance for this query (Standard HNSW) ---\n",
    "        query_distances_std = []\n",
    "        if out_indices: # Check if any neighbors were found\n",
    "            try:\n",
    "                # Ensure indices are valid before accessing data\n",
    "                valid_indices = [idx for idx in out_indices if idx < len(data)]\n",
    "                if valid_indices:\n",
    "                    neighbor_vectors = data[valid_indices] # Get vectors using valid indices\n",
    "                    for neighbor_vec in neighbor_vectors:\n",
    "                        dist = np.linalg.norm(q - neighbor_vec) # Calculate Euclidean distance\n",
    "                        query_distances_std.append(dist)\n",
    "            except IndexError as e:\n",
    "                logger.error(f\"Index error during std distance calculation for query {i}: {e}. Indices: {out_indices}\")\n",
    "                # Handle error, e.g., append NaN or skip\n",
    "\n",
    "        # Store the average distance for *this* query\n",
    "        avg_query_dist_std = np.mean(query_distances_std) if query_distances_std else np.nan\n",
    "        all_avg_dists_std.append(avg_query_dist_std)\n",
    "        # --- End Average Distance Calculation ---\n",
    "\n",
    "    t_eval_std_end = time.perf_counter()\n",
    "\n",
    "    avg_time_std_ms = np.mean(times_std) * 1000 if times_std else 0\n",
    "    recall_std = total_recall_std / num_queries if num_queries > 0 else 0\n",
    "    total_query_time_std = t_eval_std_end - t_eval_std_start\n",
    "    # Calculate overall average distance for standard HNSW, ignoring NaNs\n",
    "    overall_avg_dist_std = np.nanmean(all_avg_dists_std) if all_avg_dists_std else 0\n",
    "\n",
    "    # --- Evaluate SCC-Aware Search ---\n",
    "    print(\"\\nEvaluating SCC-Aware search...\")\n",
    "    total_recall_scc = 0\n",
    "    times_scc = []\n",
    "    results_scc = []\n",
    "    t_eval_scc_start = time.perf_counter()\n",
    "    for i, q in enumerate(tqdm(queries, desc=\"SCC-Aware query\")):\n",
    "        ts = time.perf_counter()\n",
    "        out_indices = idx_scc.search(q, k, ef_search=ef_search)\n",
    "        te = time.perf_counter()\n",
    "        times_scc.append(te - ts)\n",
    "\n",
    "        # Recall calculation\n",
    "        if k > 0: # Avoid division by zero if k=0\n",
    "            recall = len(set(gt_i[i]) & set(out_indices)) / k\n",
    "            total_recall_scc += recall\n",
    "        results_scc.append(out_indices)\n",
    "\n",
    "        # --- Calculate Average Distance for this query (SCC-Aware HNSW) ---\n",
    "        query_distances_scc = []\n",
    "        if out_indices: # Check if any neighbors were found\n",
    "            try:\n",
    "                # Ensure indices are valid before accessing data\n",
    "                valid_indices = [idx for idx in out_indices if idx < len(data)]\n",
    "                if valid_indices:\n",
    "                    neighbor_vectors = data[valid_indices] # Get vectors using valid indices\n",
    "                    for neighbor_vec in neighbor_vectors:\n",
    "                        dist = np.linalg.norm(q - neighbor_vec) # Calculate Euclidean distance\n",
    "                        query_distances_scc.append(dist)\n",
    "            except IndexError as e:\n",
    "                logger.error(f\"Index error during scc distance calculation for query {i}: {e}. Indices: {out_indices}\")\n",
    "                # Handle error, e.g., append NaN or skip\n",
    "\n",
    "        # Store the average distance for *this* query\n",
    "        avg_query_dist_scc = np.mean(query_distances_scc) if query_distances_scc else np.nan\n",
    "        all_avg_dists_scc.append(avg_query_dist_scc)\n",
    "        # --- End Average Distance Calculation ---\n",
    "\n",
    "    t_eval_scc_end = time.perf_counter()\n",
    "\n",
    "    avg_time_scc_ms = np.mean(times_scc) * 1000 if times_scc else 0\n",
    "    recall_scc = total_recall_scc / num_queries if num_queries > 0 else 0\n",
    "    total_query_time_scc = t_eval_scc_end - t_eval_scc_start\n",
    "    # Calculate overall average distance for SCC-aware HNSW, ignoring NaNs\n",
    "    overall_avg_dist_scc = np.nanmean(all_avg_dists_scc) if all_avg_dists_scc else 0\n",
    "\n",
    "    # --- Output Results ---\n",
    "    print(\"\\n--- Comparison Summary ---\")\n",
    "    print(f\"Parameters: k={k}, M={M}, efConstruction={ef_construction}, efSearch={ef_search}, SCC_Alpha={scc_heuristic_alpha if idx_scc.scc_aware_yes else 'N/A'}\")\n",
    "    print(f\"Dataset size: {num_vectors} vectors, {num_queries} queries\")\n",
    "    print(\"-\" * 85) # Adjusted width for new column\n",
    "    # Updated Header\n",
    "    print(f\"{'Method':<16} | {'Build Time (s)':<15} | {'Recall@' + str(k):<10} | {'Avg Neighbor Dist':<18} | {'Total Query (s)':<15} | {'Avg Query (ms)':<15}\")\n",
    "    print(\"-\" * 85) # Adjusted width for new column\n",
    "    # Updated Data Rows\n",
    "    print(f\"{'Standard HNSW':<16} | {build_std_time:<15.2f} | {recall_std:<10.4f} | {overall_avg_dist_std:<18.4f} | {total_query_time_std:<15.2f} | {avg_time_std_ms:<15.2f}\")\n",
    "    print(f\"{'SCC-Aware HNSW':<16} | {build_scc_time:<15.2f} | {recall_scc:<10.4f} | {overall_avg_dist_scc:<18.4f} | {total_query_time_scc:<15.2f} | {avg_time_scc_ms:<15.2f}\")\n",
    "    print(\"-\" * 85) # Adjusted width for new column\n",
    "\n",
    "    # Optional: Calculate speedup/recall/distance difference\n",
    "    if avg_time_std_ms > 0 and avg_time_scc_ms > 0:\n",
    "        speedup = avg_time_std_ms / avg_time_scc_ms\n",
    "        print(f\"Avg Query Time Speedup (SCC vs Std): {speedup:.2f}x\")\n",
    "    recall_diff = recall_scc - recall_std\n",
    "    print(f\"Recall Difference (SCC - Std): {recall_diff:+.4f}\")\n",
    "    dist_diff = overall_avg_dist_scc - overall_avg_dist_std\n",
    "    print(f\"Avg Distance Difference (SCC - Std): {dist_diff:+.4f}\")\n",
    "\n",
    "# %% # Keep the cell marker if it exists in the original notebook\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
