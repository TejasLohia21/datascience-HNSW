{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import hnswlib\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from heapq import heappush, heappop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors of dimension 100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Path to your GloVe file (update this based on your downloaded version)\n",
    "glove_path = '/Users/tanishqchaudhari/Desktop/DataScience Proj  datasets/Dataset/glove.6B.100d.txt'\n",
    "\n",
    "# Load GloVe vectors\n",
    "word_to_vec = {}\n",
    "words = []\n",
    "vectors = []\n",
    "\n",
    "with open(glove_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]  # First token is the word\n",
    "        vector = np.array(values[1:], dtype=np.float32)  # Rest are vector values\n",
    "        word_to_vec[word] = vector\n",
    "        words.append(word)\n",
    "        vectors.append(vector)\n",
    "\n",
    "# Convert to numpy array\n",
    "vectors = np.array(vectors, dtype=np.float32)\n",
    "print(f\"Loaded {len(words)} word vectors of dimension {vectors.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400000, 100)\n"
     ]
    }
   ],
   "source": [
    "print(vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import math\n",
    "from heapq import heappush, heappop\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import pandas as pd\n",
    "\n",
    "# configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def precompute_all_query_pcas(pca_models, pca_applied_layers, query_vectors):\n",
    "    \"\"\"\n",
    "    Precomputes PCA projections for a batch of query vectors.\n",
    "    Returns a list where each index corresponds to the layer number, and contains\n",
    "    either the PCA-transformed queries (np.ndarray) or None if not used.\n",
    "    \"\"\"\n",
    "    max_layer = max(pca_applied_layers) + 1 if pca_applied_layers else 0\n",
    "    pca_queries = [None] * max_layer  # indexed list for O(1) access\n",
    "\n",
    "    query_vectors_np = np.asarray(query_vectors)\n",
    "\n",
    "    for l_idx in pca_applied_layers:\n",
    "        model = pca_models[l_idx]  # direct index, avoid .get()\n",
    "        pca_queries[l_idx] = model.transform(query_vectors_np)\n",
    "\n",
    "    return pca_queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_all_query_pcas(pca_models, pca_applied_layers, query_vectors):\n",
    "    \"\"\"\n",
    "    Precomputes PCA projections for a batch of query vectors.\n",
    "    Returns a list where each index = layer number, and value is\n",
    "    either the (n_queries, pca_components) array or None.\n",
    "    \"\"\"\n",
    "    if not pca_applied_layers:\n",
    "        return []\n",
    "\n",
    "    max_layer = max(pca_applied_layers) + 1\n",
    "    pca_queries = [None] * max_layer\n",
    "    Q = np.asarray(query_vectors)\n",
    "\n",
    "    for l in pca_applied_layers:\n",
    "        model = pca_models[l]\n",
    "        pca_queries[l] = model.transform(Q)\n",
    "\n",
    "    return pca_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import logging\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from heapq import heappush, heappop\n",
    "import time # Added for potential timing within methods if needed\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- precompute_all_query_pcas function remains the same ---\n",
    "def precompute_all_query_pcas(pca_models, pca_applied_layers, query_vectors):\n",
    "    \"\"\"\n",
    "    Precomputes PCA projections for a batch of query vectors.\n",
    "    Returns a list where each index = layer number, and value is\n",
    "    either the (n_queries, pca_components) array or None.\n",
    "    \"\"\"\n",
    "    if not pca_applied_layers:\n",
    "        return []\n",
    "\n",
    "    max_layer = max(pca_applied_layers) + 1\n",
    "    pca_queries = [None] * max_layer\n",
    "    Q = np.asarray(query_vectors)\n",
    "\n",
    "    for l in pca_applied_layers:\n",
    "        # Check if the model exists for the layer (robustness)\n",
    "        if l in pca_models:\n",
    "            model = pca_models[l]\n",
    "            pca_queries[l] = model.transform(Q)\n",
    "        # else: # Should not happen if pca_applied_layers is consistent\n",
    "        #    logger.warning(f\"Model for supposedly applied PCA layer {l} not found during query precomputation.\")\n",
    "\n",
    "\n",
    "    return pca_queries\n",
    "\n",
    "# --- Modified optHNSWPCA Class ---\n",
    "# --- Imports and Logger Setup ---\n",
    "import math\n",
    "import random\n",
    "import logging\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from heapq import heappush, heappop\n",
    "import time\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- optHNSWPCA Class (Global PCA Approach) ---\n",
    "\n",
    "class optHNSWPCA:\n",
    "    \"\"\"\n",
    "    HNSW with optional GLOBAL PCA-based acceleration (applied >= layer 1),\n",
    "    transforming queries ONCE at the start of search.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, max_elements, M=16, ef_construction=200,\n",
    "                 pca_yes=False, pca_components=50):\n",
    "        self.dim = dim\n",
    "        self.max_elements = max_elements\n",
    "        self.M = M\n",
    "        self.ef_construction = ef_construction\n",
    "        self.pca_yes = pca_yes and pca_components < dim\n",
    "        self.pca_components = min(pca_components, dim)\n",
    "\n",
    "        self.vectors = [] # Stores original vectors\n",
    "        self.layers = []\n",
    "        self.entry_point = None\n",
    "        self.entry_point_level = -1\n",
    "\n",
    "        # --- Global PCA Structures ---\n",
    "        self.global_pca_model = None      # Single PCA model for the whole dataset\n",
    "        self.global_reduced_vectors = None # Numpy array (N, pca_components)\n",
    "        # We still need to know if PCA is conceptually applicable (enough nodes) per layer for logic switching\n",
    "        # Let's reuse pca_applied_layers for this, but it now signifies \"use PCA distance metric if l >= 1\"\n",
    "        self.pca_logic_layers = set() # Layers (>=1) dense enough to *consider* using PCA logic\n",
    "\n",
    "        logger.info(f\"Initialized HNSW (dim={dim}, PCA={'on (GLOBAL, layers >= 1)' if self.pca_yes else 'off'}, PCA components={self.pca_components if self.pca_yes else 'N/A'})\")\n",
    "\n",
    "    # _get_layer, _distance, _search_layer_standard, insert remain the same as previous version\n",
    "    def _get_layer(self):\n",
    "        ml = 1 / math.log(self.M) if self.M > 1 else 1\n",
    "        return max(0, int(-math.log(random.random()) * ml))\n",
    "\n",
    "    def _distance(self, idx1, idx2):\n",
    "        if not (0 <= idx1 < len(self.vectors) and 0 <= idx2 < len(self.vectors)): return float('inf')\n",
    "        return np.linalg.norm(self.vectors[idx1] - self.vectors[idx2])\n",
    "\n",
    "    def _search_layer_standard(self, query_vec, layer_idx, ep_idx, ef):\n",
    "        if layer_idx >= len(self.layers): return []\n",
    "        graph = self.layers[layer_idx]\n",
    "        if not graph: return []\n",
    "        if ep_idx is None or ep_idx not in graph:\n",
    "             try: ep_idx = next(iter(graph))\n",
    "             except StopIteration: return []\n",
    "        if ep_idx >= len(self.vectors): return [] # Safety check\n",
    "\n",
    "        visited = {ep_idx}\n",
    "        dist0 = np.linalg.norm(query_vec - self.vectors[ep_idx])\n",
    "        candidates = [(dist0, ep_idx)]\n",
    "        results = [(-dist0, ep_idx)]\n",
    "\n",
    "        while candidates:\n",
    "            dist, cur = heappop(candidates)\n",
    "            if cur not in graph: continue\n",
    "            farthest_dist_in_results = -results[0][0]\n",
    "            if dist > farthest_dist_in_results and len(results) >= ef: break\n",
    "            for nb in graph.get(cur, []):\n",
    "                if nb in visited: continue\n",
    "                visited.add(nb)\n",
    "                if nb >= len(self.vectors): continue\n",
    "                d = np.linalg.norm(query_vec - self.vectors[nb])\n",
    "                if d < -results[0][0] or len(results) < ef:\n",
    "                    heappush(results, (-d, nb))\n",
    "                    if len(results) > ef: heappop(results)\n",
    "                    heappush(candidates, (d, nb))\n",
    "        return sorted([(-d, idx) for d, idx in results])[:ef]\n",
    "\n",
    "    def insert(self, vector):\n",
    "        # Simplified insert assuming it's called sequentially\n",
    "        vec = np.asarray(vector)\n",
    "        idx = len(self.vectors)\n",
    "        if idx >= self.max_elements: raise MemoryError(f\"Index full. Max elements: {self.max_elements}\")\n",
    "        self.vectors.append(vec)\n",
    "        level = self._get_layer()\n",
    "        while len(self.layers) <= level: self.layers.append({})\n",
    "\n",
    "        ep = self.entry_point\n",
    "        current_ep_level = self.entry_point_level\n",
    "\n",
    "        # Phase 1: Find entry points from top down\n",
    "        for l in range(current_ep_level, level, -1):\n",
    "            if ep is None: break\n",
    "            if l >= len(self.layers) or not self.layers[l]: continue\n",
    "            res = self._search_layer_standard(vec, l, ep, ef=1)\n",
    "            if res: ep = res[0][1]\n",
    "\n",
    "        # Phase 2: Insert node and connect\n",
    "        start_conn_level = min(level, current_ep_level if self.entry_point is not None else level)\n",
    "        current_ep_for_level = ep\n",
    "\n",
    "        for l in range(start_conn_level, -1, -1):\n",
    "            graph = self.layers[l]\n",
    "            if current_ep_for_level is not None and current_ep_for_level not in graph:\n",
    "                 if graph: current_ep_for_level = next(iter(graph))\n",
    "                 else: current_ep_for_level = None\n",
    "\n",
    "            neigh = []\n",
    "            if current_ep_for_level is not None:\n",
    "                neigh = self._search_layer_standard(vec, l, current_ep_for_level, self.ef_construction)\n",
    "                if neigh: current_ep_for_level = neigh[0][1]\n",
    "\n",
    "            conns = [nid for _, nid in neigh[:self.M]]\n",
    "            graph[idx] = conns\n",
    "\n",
    "            for nb in conns:\n",
    "                graph.setdefault(nb, []).append(idx)\n",
    "                if len(graph[nb]) > self.M:\n",
    "                    dists = [(self._distance(nb, c), c) for c in graph[nb]]\n",
    "                    dists.sort()\n",
    "                    graph[nb] = [c for _, c in dists[:self.M]]\n",
    "\n",
    "        if self.entry_point is None or level > self.entry_point_level:\n",
    "            self.entry_point = idx\n",
    "            self.entry_point_level = level\n",
    "\n",
    "\n",
    "    # --- MODIFIED finalize_pca for GLOBAL PCA ---\n",
    "    def finalize_pca(self):\n",
    "        if not self.pca_yes:\n",
    "            logger.info(\"PCA is disabled. Skipping PCA finalization.\")\n",
    "            return\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "        n_vectors = len(self.vectors)\n",
    "        if n_vectors == 0:\n",
    "             logger.warning(\"Cannot finalize PCA: No vectors inserted.\")\n",
    "             return\n",
    "        if n_vectors <= self.pca_components:\n",
    "             logger.warning(f\"Not enough vectors ({n_vectors}) to fit PCA with {self.pca_components} components. PCA disabled.\")\n",
    "             self.pca_yes = False # Disable PCA if not possible\n",
    "             return\n",
    "\n",
    "        logger.info(f\"Fitting GLOBAL PCA (components={self.pca_components}) on all {n_vectors} vectors...\")\n",
    "        try:\n",
    "            # Stack all vectors (consider memory for very large datasets)\n",
    "            all_vectors_stacked = np.vstack(self.vectors)\n",
    "\n",
    "            # Fit the single global model\n",
    "            self.global_pca_model = PCA(n_components=self.pca_components)\n",
    "            self.global_pca_model.fit(all_vectors_stacked)\n",
    "            logger.info(\"Global PCA model fitted.\")\n",
    "\n",
    "            # Transform ALL vectors and store them\n",
    "            logger.info(\"Transforming all vectors using global PCA model...\")\n",
    "            self.global_reduced_vectors = self.global_pca_model.transform(all_vectors_stacked).astype(np.float32)\n",
    "            logger.info(f\"Stored global reduced vectors with shape {self.global_reduced_vectors.shape}.\")\n",
    "\n",
    "            # Determine which layers (>=1) have enough nodes to warrant using PCA logic\n",
    "            self.pca_logic_layers.clear()\n",
    "            total_layers = len(self.layers)\n",
    "            for l in range(1, total_layers): # Check layers 1 and up\n",
    "                 if l < len(self.layers) and self.layers[l]:\n",
    "                      node_count = len([i for i in self.layers[l] if i < n_vectors])\n",
    "                      if node_count > self.pca_components: # Or some other threshold? For now, just > components.\n",
    "                           self.pca_logic_layers.add(l)\n",
    "\n",
    "            end_time = time.perf_counter()\n",
    "            logger.info(f\"Global PCA finalization complete in {end_time - start_time:.2f}s. PCA logic active for layers: {sorted(list(self.pca_logic_layers))}\")\n",
    "\n",
    "        except MemoryError:\n",
    "             logger.error(\"MemoryError during global PCA fitting/transformation. Disabling PCA.\")\n",
    "             self.pca_yes = False\n",
    "             self.global_pca_model = None\n",
    "             self.global_reduced_vectors = None\n",
    "             self.pca_logic_layers.clear()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during global PCA finalization: {e}. Disabling PCA.\")\n",
    "            self.pca_yes = False\n",
    "            self.global_pca_model = None\n",
    "            self.global_reduced_vectors = None\n",
    "            self.pca_logic_layers.clear()\n",
    "\n",
    "\n",
    "    # --- NEW Layer Search using GLOBAL PCA vectors ---\n",
    "    def _search_layer_global_pca(self, q_red_global, layer_idx, ep_idx, ef):\n",
    "        # Search using globally reduced query and globally reduced node vectors\n",
    "        if layer_idx >= len(self.layers): return []\n",
    "        graph = self.layers[layer_idx]\n",
    "        if not graph: return []\n",
    "\n",
    "        # Check if global reduced vectors exist\n",
    "        if self.global_reduced_vectors is None:\n",
    "             logger.error(\"Global reduced vectors not available for PCA search. Falling back.\")\n",
    "             # Fallback needs original query - this function doesn't have it easily.\n",
    "             # This indicates a fundamental issue if called when not ready.\n",
    "             # For now, return empty, but ideally, search_pca_opt checks readiness.\n",
    "             return []\n",
    "\n",
    "        # Ensure ep_idx is valid for graph and reduced vector array\n",
    "        if ep_idx is None or ep_idx not in graph or ep_idx >= len(self.global_reduced_vectors):\n",
    "            try: ep_idx = next(iter(graph)) # Try starting from *any* node in layer\n",
    "            except StopIteration: return []\n",
    "            # Check again if this arbitrary node is valid for reduced vectors\n",
    "            if ep_idx >= len(self.global_reduced_vectors):\n",
    "                 logger.error(f\"Cannot start global PCA search in layer {layer_idx}, even fallback ep {ep_idx} is invalid.\")\n",
    "                 return []\n",
    "\n",
    "        visited = {ep_idx}\n",
    "        # Initial distance using global PCA vectors\n",
    "        dist0 = np.linalg.norm(q_red_global - self.global_reduced_vectors[ep_idx])\n",
    "        candidates = [(dist0, ep_idx)]\n",
    "        results = [(-dist0, ep_idx)]\n",
    "\n",
    "        while candidates:\n",
    "            dist, cur = heappop(candidates)\n",
    "            if cur not in graph: continue\n",
    "            farthest_dist_in_results = -results[0][0]\n",
    "            if dist > farthest_dist_in_results and len(results) >= ef: break\n",
    "\n",
    "            for nb in graph.get(cur, []):\n",
    "                if nb in visited: continue\n",
    "                visited.add(nb)\n",
    "                # Check if neighbor index is valid for reduced vectors\n",
    "                if nb >= len(self.global_reduced_vectors): continue\n",
    "\n",
    "                # Calculate distance using GLOBAL reduced query and GLOBAL reduced neighbor\n",
    "                d = np.linalg.norm(q_red_global - self.global_reduced_vectors[nb])\n",
    "\n",
    "                if d < -results[0][0] or len(results) < ef:\n",
    "                    heappush(results, (-d, nb))\n",
    "                    if len(results) > ef: heappop(results)\n",
    "                    heappush(candidates, (d, nb))\n",
    "        return sorted([(-d, idx) for d, idx in results])[:ef]\n",
    "\n",
    "\n",
    "    # search (standard) remains the same\n",
    "    def search(self, query_vec, k=10):\n",
    "        ef = max(self.ef_construction, k)\n",
    "        ep = self.entry_point\n",
    "        if ep is None: return []\n",
    "        q = np.asarray(query_vec)\n",
    "        current_ep_level = self.entry_point_level\n",
    "        for l in range(current_ep_level, 0, -1):\n",
    "             if l >= len(self.layers) or not self.layers[l]: continue\n",
    "             res = self._search_layer_standard(q, l, ep, ef=1)\n",
    "             if res: ep = res[0][1]\n",
    "        res = self._search_layer_standard(q, 0, ep, ef)\n",
    "        return [i for _, i in res[:k]]\n",
    "\n",
    "\n",
    "    # --- MODIFIED search_pca_opt for GLOBAL PCA ---\n",
    "    def search_pca_opt(self, query_vec, k=10):\n",
    "        ef = max(self.ef_construction, k)\n",
    "        ep = self.entry_point\n",
    "        if ep is None: return []\n",
    "\n",
    "        q_orig = np.asarray(query_vec) # Keep original query\n",
    "        current_ep_level = self.entry_point_level\n",
    "\n",
    "        # Check if global PCA is ready and transform query ONCE\n",
    "        q_red_global = None\n",
    "        pca_active = self.pca_yes and self.global_pca_model is not None and self.global_reduced_vectors is not None\n",
    "        if pca_active:\n",
    "            try:\n",
    "                q_red_global = self.global_pca_model.transform(q_orig.reshape(1, -1))[0].astype(np.float32)\n",
    "            except Exception as e:\n",
    "                 logger.error(f\"Failed to transform query using global PCA model: {e}. Falling back to standard search.\")\n",
    "                 pca_active = False # Disable PCA for this query if transform fails\n",
    "\n",
    "        # Phase 1: Navigate top layers (down to layer 1)\n",
    "        for l in range(current_ep_level, 0, -1): # Down to 1\n",
    "            if l >= len(self.layers) or not self.layers[l]: continue\n",
    "\n",
    "            # Use PCA logic if active and layer is >= 1\n",
    "            # We don't strictly need to check pca_logic_layers if we always use PCA metric for l>=1 when active\n",
    "            if pca_active and q_red_global is not None:\n",
    "                 # Use the NEW global PCA layer search\n",
    "                 res = self._search_layer_global_pca(q_red_global, l, ep, ef=1)\n",
    "            else:\n",
    "                 # Use standard search\n",
    "                 res = self._search_layer_standard(q_orig, l, ep, ef=1)\n",
    "\n",
    "            if res:\n",
    "                ep = res[0][1]\n",
    "            # else: ep = ep # Keep same ep if layer search yields nothing\n",
    "\n",
    "        # Phase 2: Search in base layer (layer 0) - ALWAYS standard\n",
    "        res = self._search_layer_standard(q_orig, 0, ep, ef)\n",
    "\n",
    "        return [i for (_, i) in res[:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 400000 vectors, 100 dimensions\n",
      "Parameters: M=26, ef_construction=200, k=100\n",
      "PCA Settings: GLOBAL PCA, components=32 (logic active layers >= 1 if possible)\n",
      "\n",
      "Selecting query data...\n",
      "Using 5000 queries.\n",
      "\n",
      "Computing ground truth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Initialized HNSW (dim=100, PCA=off, PCA components=N/A)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth computed in 1.66s\n",
      "\n",
      "Building standard HNSW index (no PCA)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing standard HNSW: 100%|██████████| 400000/400000 [40:14<00:00, 165.64it/s] \n",
      "INFO:__main__:Initialized HNSW (dim=100, PCA=on (GLOBAL, layers >= 1), PCA components=32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard build time: 2414.87s\n",
      "Standard index height: 4\n",
      "\n",
      "Building PCA-enabled HNSW index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing PCA-enabled HNSW: 100%|██████████| 400000/400000 [37:51<00:00, 176.09it/s]\n",
      "INFO:__main__:Fitting GLOBAL PCA (components=32) on all 400000 vectors...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA-enabled index build time (before finalize): 2271.61s\n",
      "PCA-enabled index height: 4\n",
      "\n",
      "Finalizing GLOBAL PCA model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Global PCA model fitted.\n",
      "INFO:__main__:Transforming all vectors using global PCA model...\n",
      "INFO:__main__:Stored global reduced vectors with shape (400000, 32).\n",
      "INFO:__main__:Global PCA finalization complete in 0.72s. PCA logic active for layers: [1, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA finalize time: 0.73s\n",
      "\n",
      "Evaluating standard search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Standard query: 100%|██████████| 5000/5000 [00:10<00:00, 498.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating GLOBAL PCA-optimized search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global PCA-opt query: 100%|██████████| 5000/5000 [00:25<00:00, 198.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Results ---\n",
      "Parameters: M=26, ef_construction=200, k=100\n",
      "PCA Parameters: GLOBAL PCA, components=32, logic active for layers: [1, 2]\n",
      "\n",
      "Build Times:\n",
      "Standard Build: 2414.87s\n",
      "PCA Build (Index): 2271.61s\n",
      "PCA Build (Finalize Global): 0.73s\n",
      "Total PCA Build: 2272.33s\n",
      "\n",
      "Standard: recall@100=0.0174, total_query_time=10.04s, avg_query_time=1.98ms\n",
      "PCA-opt:  recall@100=0.4835, total_query_time=25.24s, avg_query_time=5.00ms\n",
      "\n",
      "--- Comparison Summary ---\n",
      "Method         | Recall@100 | TotalQuery(s) | AvgQuery(ms)\n",
      "---------------|------------|---------------|-------------\n",
      "Standard       | 0.0174     | 10.04         | 1.98       \n",
      "PCA-optimized  | 0.4835     | 25.24         | 5.00       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Main Execution Script (Mostly unchanged) ---\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assume 'vectors' is pre-loaded and logging is configured\n",
    "# logging.basicConfig(level=logging.INFO) # Ensure configured\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- parameters ---\n",
    "    num_vectors = vectors.shape[0]\n",
    "    num_queries = 5000  # Keep reasonable for testing\n",
    "    dim = vectors.shape[1]\n",
    "    k = 100\n",
    "    M = 26\n",
    "    ef_construction = 200\n",
    "    pca_components = 32 # Let's try 32 components\n",
    "\n",
    "    print(f\"Dataset: {num_vectors} vectors, {dim} dimensions\")\n",
    "    print(f\"Parameters: M={M}, ef_construction={ef_construction}, k={k}\")\n",
    "    print(f\"PCA Settings: GLOBAL PCA, components={pca_components} (logic active layers >= 1 if possible)\")\n",
    "\n",
    "    print(\"\\nSelecting query data...\")\n",
    "    actual_num_queries = min(num_queries, num_vectors)\n",
    "    query_indices = np.random.choice(num_vectors, actual_num_queries, replace=False)\n",
    "    queries = vectors[query_indices]\n",
    "    print(f\"Using {actual_num_queries} queries.\")\n",
    "\n",
    "    print(\"\\nComputing ground truth...\")\n",
    "    t_gt0 = time.perf_counter()\n",
    "    nn = NearestNeighbors(n_neighbors=k, algorithm='brute', metric='euclidean')\n",
    "    nn.fit(vectors)\n",
    "    gt_d, gt_i = nn.kneighbors(queries)\n",
    "    t_gt1 = time.perf_counter()\n",
    "    print(f\"Ground truth computed in {t_gt1 - t_gt0:.2f}s\")\n",
    "\n",
    "    # --- Build standard (non-PCA) index ---\n",
    "    print(\"\\nBuilding standard HNSW index (no PCA)...\")\n",
    "    idx_std = optHNSWPCA(dim, num_vectors + 10, M, ef_construction, pca_yes=False, pca_components=pca_components)\n",
    "    t0 = time.perf_counter()\n",
    "    # Seed random for potentially more comparable builds (optional)\n",
    "    # random.seed(42); np.random.seed(42)\n",
    "    for v in tqdm(vectors, desc=\"Indexing standard HNSW\"): idx_std.insert(v)\n",
    "    build_std = time.perf_counter() - t0\n",
    "    print(f\"Standard build time: {build_std:.2f}s\")\n",
    "    print(f\"Standard index height: {idx_std.entry_point_level}\")\n",
    "\n",
    "    # --- Build PCA-enabled index ---\n",
    "    print(\"\\nBuilding PCA-enabled HNSW index...\")\n",
    "    idx_pca = optHNSWPCA(dim, num_vectors + 10, M, ef_construction, pca_yes=True, pca_components=pca_components)\n",
    "    t0 = time.perf_counter()\n",
    "    # Seed random for potentially more comparable builds (optional)\n",
    "    # random.seed(42); np.random.seed(42) # Use same seed if comparing structures\n",
    "    for v in tqdm(vectors, desc=\"Indexing PCA-enabled HNSW\"): idx_pca.insert(v)\n",
    "    build_pca = time.perf_counter() - t0\n",
    "    print(f\"PCA-enabled index build time (before finalize): {build_pca:.2f}s\")\n",
    "    print(f\"PCA-enabled index height: {idx_pca.entry_point_level}\")\n",
    "\n",
    "    print(\"\\nFinalizing GLOBAL PCA model...\")\n",
    "    t0 = time.perf_counter()\n",
    "    idx_pca.finalize_pca() # Fits global PCA and transforms all vectors\n",
    "    fit_pca = time.perf_counter() - t0\n",
    "    print(f\"PCA finalize time: {fit_pca:.2f}s\")\n",
    "\n",
    "    # helper for recall calculation\n",
    "    def recall_at_k(ground_truth_indices, predicted_indices, k_val):\n",
    "        if predicted_indices is None: predicted_indices = []\n",
    "        k_actual_pred = min(k_val, len(predicted_indices))\n",
    "        k_actual_gt = min(k_val, len(ground_truth_indices))\n",
    "        gt_set = set(ground_truth_indices[:k_actual_gt])\n",
    "        pred_set = set(predicted_indices[:k_actual_pred])\n",
    "        return len(gt_set & pred_set) / k_val if k_val > 0 else 1.0\n",
    "\n",
    "    # --- Evaluate standard search ---\n",
    "    print(\"\\nEvaluating standard search...\")\n",
    "    results_std = [None] * actual_num_queries\n",
    "    times_std = []\n",
    "    total_r_std = 0.0\n",
    "    tstart_query_std = time.perf_counter()\n",
    "    for i, q in enumerate(tqdm(queries, desc=\"Standard query\")):\n",
    "        ts = time.perf_counter()\n",
    "        results_std[i] = idx_std.search(q, k)\n",
    "        te = time.perf_counter()\n",
    "        times_std.append(te - ts)\n",
    "        total_r_std += recall_at_k(gt_i[i], results_std[i], k)\n",
    "    tend_query_std = time.perf_counter()\n",
    "    tot_time_std = tend_query_std - tstart_query_std\n",
    "    avg_time_std = np.mean(times_std) * 1000 if times_std else 0\n",
    "    recall_std = total_r_std / actual_num_queries if actual_num_queries > 0 else 0.0\n",
    "\n",
    "    # --- Evaluate PCA-optimized search ---\n",
    "    print(\"\\nEvaluating GLOBAL PCA-optimized search...\")\n",
    "    results_pca = [None] * actual_num_queries\n",
    "    times_pca = []\n",
    "    total_r_pca = 0.0\n",
    "    avg_time_pca = 0.0\n",
    "    tot_time_pca = 0.0\n",
    "\n",
    "    # Only run if PCA was successfully finalized\n",
    "    if idx_pca.pca_yes and idx_pca.global_pca_model is not None:\n",
    "        tstart_query_pca = time.perf_counter()\n",
    "        for i, q in enumerate(tqdm(queries, desc=\"Global PCA-opt query\")):\n",
    "            ts = time.perf_counter()\n",
    "            results_pca[i] = idx_pca.search_pca_opt(q, k) # Call is simpler now\n",
    "            te = time.perf_counter()\n",
    "            times_pca.append(te - ts)\n",
    "            total_r_pca += recall_at_k(gt_i[i], results_pca[i], k)\n",
    "        tend_query_pca = time.perf_counter()\n",
    "        tot_time_pca = tend_query_pca - tstart_query_pca\n",
    "        avg_time_pca = np.mean(times_pca) * 1000 if times_pca else 0\n",
    "        recall_pca = total_r_pca / actual_num_queries if actual_num_queries > 0 else 0.0\n",
    "    else:\n",
    "        print(\"Skipping GLOBAL PCA-optimized search evaluation as PCA was not successfully finalized.\")\n",
    "        recall_pca = 0.0\n",
    "\n",
    "    # --- Output results ---\n",
    "    print(\"\\n--- Results ---\")\n",
    "    print(f\"Parameters: M={M}, ef_construction={ef_construction}, k={k}\")\n",
    "    if idx_pca.pca_yes and idx_pca.global_pca_model:\n",
    "        print(f\"PCA Parameters: GLOBAL PCA, components={pca_components}, logic active for layers: {sorted(list(idx_pca.pca_logic_layers)) if idx_pca.pca_logic_layers else 'None Dense Enough'}\")\n",
    "    else: print(\"PCA Parameters: PCA Disabled or Failed\")\n",
    "    print(\"\\nBuild Times:\")\n",
    "    print(f\"Standard Build: {build_std:.2f}s\")\n",
    "    print(f\"PCA Build (Index): {build_pca:.2f}s\")\n",
    "    print(f\"PCA Build (Finalize Global): {fit_pca:.2f}s\") # This might take longer now\n",
    "    print(f\"Total PCA Build: {build_pca + fit_pca:.2f}s\")\n",
    "    print(f\"\\nStandard: recall@{k}={recall_std:.4f}, total_query_time={tot_time_std:.2f}s, avg_query_time={avg_time_std:.2f}ms\")\n",
    "    if idx_pca.pca_yes and idx_pca.global_pca_model:\n",
    "        print(f\"PCA-opt:  recall@{k}={recall_pca:.4f}, total_query_time={tot_time_pca:.2f}s, avg_query_time={avg_time_pca:.2f}ms\")\n",
    "    else:\n",
    "        print(\"PCA-opt:  Not evaluated.\")\n",
    "\n",
    "    print(\"\\n--- Comparison Summary ---\")\n",
    "    print(f\"Method         | Recall@{k} | TotalQuery(s) | AvgQuery(ms)\")\n",
    "    print(f\"---------------|------------|---------------|-------------\")\n",
    "    print(f\"Standard       | {recall_std:<10.4f} | {tot_time_std:<13.2f} | {avg_time_std:<11.2f}\")\n",
    "    if idx_pca.pca_yes and idx_pca.global_pca_model:\n",
    "        print(f\"PCA-optimized  | {recall_pca:<10.4f} | {tot_time_pca:<13.2f} | {avg_time_pca:<11.2f}\")\n",
    "    else:\n",
    "        print(\"PCA-optimized  | N/A        | N/A           | N/A        \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Height of HNSW graph: 4\n"
     ]
    }
   ],
   "source": [
    "print(\"Height of HNSW graph:\", idx_pca.entry_point_level)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
