{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import hnswlib\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from heapq import heappush, heappop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors of dimension 100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Path to your GloVe file (update this based on your downloaded version)\n",
    "glove_path = '/Users/tanishqchaudhari/Desktop/DataScience Proj  datasets/Dataset/glove.6B.100d.txt'\n",
    "\n",
    "# Load GloVe vectors\n",
    "word_to_vec = {}\n",
    "words = []\n",
    "vectors = []\n",
    "\n",
    "with open(glove_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]  # First token is the word\n",
    "        vector = np.array(values[1:], dtype=np.float32)  # Rest are vector values\n",
    "        word_to_vec[word] = vector\n",
    "        words.append(word)\n",
    "        vectors.append(vector)\n",
    "\n",
    "# Convert to numpy array\n",
    "vectors = np.array(vectors, dtype=np.float32)\n",
    "print(f\"Loaded {len(words)} word vectors of dimension {vectors.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding nearest neighbors: 100%|██████████| 40000/40000 [00:00<00:00, 80071.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored nearest neighbors for 38026 queries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Generate 10^ random query indices\n",
    "num_queries = 40000\n",
    "query_indices = np.random.randint(0, len(words), size=num_queries)\n",
    "\n",
    "# Step 2: Get the corresponding query vectors\n",
    "query_vectors = vectors[query_indices]\n",
    "\n",
    "# Step 3: Use NearestNeighbors to find 100 nearest neighbors\n",
    "k = 100\n",
    "nn = NearestNeighbors(n_neighbors=k, algorithm='auto', metric='l2')\n",
    "nn.fit(vectors)\n",
    "\n",
    "# Step 4: For each query, get the indices of the 100 nearest neighbors\n",
    "distances, neighbor_indices = nn.kneighbors(query_vectors)\n",
    "\n",
    "# Step 5: Store results in a dictionary {query_word: [neighbor_words]}\n",
    "query_to_neighbors = {}\n",
    "\n",
    "\n",
    "\n",
    "# Step 5: Store results in a dictionary {query_word: [neighbor_words]}\n",
    "query_to_neighbors = {}\n",
    "for i in tqdm(range(len(query_indices)), desc=\"Finding nearest neighbors\"):\n",
    "    query_idx = query_indices[i]\n",
    "    query_word = words[query_idx]\n",
    "    neighbor_words = [words[idx] for idx in neighbor_indices[i]]\n",
    "    query_to_neighbors[query_word] = neighbor_words\n",
    "\n",
    "print(f\"Stored nearest neighbors for {len(query_to_neighbors)} queries.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# # Load the saved query-to-neighbors mapping\n",
    "# with open(\"query_neighbors.json\", \"r\") as f:\n",
    "#     query_to_neighbors = json.load(f)\n",
    "\n",
    "# print(f\"Loaded {len(query_to_neighbors)} queries from file.\")\n",
    "\n",
    "# # Step 6: Create a dictionary mapping each unique query_word to its vector\n",
    "# query_word_vectors = {word: word_to_vec[word] for word in query_to_neighbors}\n",
    "\n",
    "# print(f\"Stored vectors for {len(query_word_vectors)} unique query words.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding nearest neighbors: 100%|██████████| 40000/40000 [00:00<00:00, 1600650.29it/s]\n"
     ]
    }
   ],
   "source": [
    "query_to_neighbors_vectors = {}\n",
    "\n",
    "for i in tqdm(range(len(query_indices)), desc=\"Finding nearest neighbors\"):\n",
    "    query_idx = query_indices[i]\n",
    "    neighbor_indices_list = neighbor_indices[i]\n",
    "    query_to_neighbors_vectors[query_idx] = neighbor_indices_list  # All indices, not vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# Flatten all neighbor indices into a single list\n",
    "all_neighbors = [idx for neighbors in query_to_neighbors_vectors.values() for idx in neighbors]\n",
    "\n",
    "# Count frequency of each neighbor index\n",
    "freq_counter = Counter(all_neighbors)\n",
    "\n",
    "# Create DataFrame and sort\n",
    "freq_df = pd.DataFrame(freq_counter.items(), columns=[\"Index\", \"Frequency\"]).sort_values(\"Frequency\", ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average frequency of neighbors: 11.952186226037322\n"
     ]
    }
   ],
   "source": [
    "#printing the average\n",
    "average = freq_df[\"Frequency\"].mean()\n",
    "print(f\"Average frequency of neighbors: {average}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hubs(\n",
    "    neighbor_indices: np.ndarray,\n",
    "    distances:        np.ndarray,\n",
    "    freq_df:          pd.DataFrame,\n",
    "    alpha:            float = None,           # now optional\n",
    "    method:           str   = \"percentile\",\n",
    "    threshold:        float = 95.0,\n",
    "    top_k:            int   = None\n",
    ") -> (pd.Series, set):\n",
    "    \"\"\"Compute combined hub‐scores f_i and select hub items H.\"\"\"\n",
    "\n",
    "    # 1) Normalize frequency\n",
    "    freq_series = freq_df.set_index(\"Index\")[\"Frequency\"].astype(float)\n",
    "    c_min, c_max = freq_series.min(), freq_series.max()\n",
    "    if c_max > c_min:\n",
    "        f_freq = (freq_series - c_min) / (c_max - c_min)\n",
    "    else:\n",
    "        f_freq = pd.Series(0.0, index=freq_series.index)\n",
    "\n",
    "    # 2) Compute and normalize average distance (then flip)\n",
    "    df = pd.DataFrame({\n",
    "        \"Index\":    neighbor_indices.ravel(),\n",
    "        \"Distance\": distances.ravel()\n",
    "    })\n",
    "    d_bar = df.groupby(\"Index\")[\"Distance\"].mean()\n",
    "    d_min, d_max = d_bar.min(), d_bar.max()\n",
    "    if d_max > d_min:\n",
    "        d_tilde = (d_bar - d_min) / (d_max - d_min)\n",
    "    else:\n",
    "        d_tilde = pd.Series(0.0, index=d_bar.index)\n",
    "    f_dist = 1.0 - d_tilde\n",
    "\n",
    "    # 3) Align indices\n",
    "    common = f_freq.index.intersection(f_dist.index)\n",
    "    f_f = f_freq.loc[common]\n",
    "    f_d = f_dist.loc[common]\n",
    "\n",
    "    # 4) Auto‐compute alpha if not provided\n",
    "    if alpha is None:\n",
    "        var_f = f_f.var()\n",
    "        var_d = f_d.var()\n",
    "        total = var_f + var_d\n",
    "        alpha = var_f / total if total > 0 else 0.5\n",
    "\n",
    "    # 5) Combine\n",
    "    combined = alpha * f_f + (1 - alpha) * f_d\n",
    "    combined.name = \"f_i\"\n",
    "\n",
    "    # 6) Select hubs\n",
    "    if method == \"percentile\":\n",
    "        cutoff = np.percentile(combined.values, threshold)\n",
    "        hub_set = set(combined[combined >= cutoff].index)\n",
    "\n",
    "    elif method == \"stddev\":\n",
    "        mu, sigma = combined.mean(), combined.std()\n",
    "        cutoff = mu + threshold * sigma\n",
    "        hub_set = set(combined[combined >= cutoff].index)\n",
    "\n",
    "    elif method == \"top_k\":\n",
    "        if top_k is None:\n",
    "            raise ValueError(\"top_k must be provided when method='top_k'\")\n",
    "        hub_set = set(combined.nlargest(top_k).index)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"unknown method {method!r}\")\n",
    "\n",
    "    return combined, hub_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_scores, hubs = compute_hubs(\n",
    "    neighbor_indices,\n",
    "    distances,\n",
    "    freq_df,\n",
    "    alpha=None,\n",
    "    method=\"percentile\",\n",
    "    threshold=90.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from heapq import heappush, heappop\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HNSW CLASS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obj Of HNSW CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _distance(self, i, j):\n",
    "    return np.linalg.norm(self.vectors[i] - self.vectors[j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from heapq import heappush, heappop\n",
    "import random\n",
    "import math\n",
    "\n",
    "class HNSW:\n",
    "    \"\"\"\n",
    "    Standard HNSW implementation based on the provided code [1],\n",
    "    with hub-related modifications removed.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, max_elements, M=16, ef_construction=200):\n",
    "        \"\"\"\n",
    "        Initializes the HNSW index.\n",
    "\n",
    "        Args:\n",
    "            dim (int): Dimensionality of the vectors.\n",
    "            max_elements (int): Estimated maximum number of elements (informational).\n",
    "            M (int): Maximum number of connections per node per layer.\n",
    "            ef_construction (int): Size of the dynamic candidate list during construction.\n",
    "        \"\"\"\n",
    "        self.dim = dim\n",
    "        self.max_elements = max_elements # Informational\n",
    "        self.M = M\n",
    "        self.ef_construction = ef_construction\n",
    "\n",
    "        # Store graph layers: layers[l] is a dict {node_idx: [neighbor_indices]}\n",
    "        self.layers = []\n",
    "        # Store the actual vectors, index corresponds to node_idx\n",
    "        self.vectors = []\n",
    "        # Track the entry point (index of the node in the highest layer)\n",
    "        self.entry_point = None\n",
    "        # Precompute layer multiplier\n",
    "        self.ml = 1 / math.log(self.M) if self.M > 1 else 1\n",
    "\n",
    "\n",
    "    def _get_layer(self):\n",
    "        \"\"\"\n",
    "        Determine the layer for a new node based on exponential decay\n",
    "        using the precomputed multiplier (mL).\n",
    "\n",
    "        Returns:\n",
    "            int: The selected layer index (>= 0).\n",
    "        \"\"\"\n",
    "        # Calculate layer using the standard HNSW formula\n",
    "        layer = max(0, int(-math.log(random.random()) * self.ml))\n",
    "        return layer\n",
    "\n",
    "    # --- _distance method remains unchanged ---\n",
    "    def _distance(self, idx1, idx2):\n",
    "        \"\"\"Calculate Euclidean distance between two vectors by index.\"\"\"\n",
    "        if not (0 <= idx1 < len(self.vectors) and 0 <= idx2 < len(self.vectors)):\n",
    "            # Handle cases where one or both indices are out of bounds\n",
    "            # This might happen during pruning if a node was considered but\n",
    "            # doesn't actually exist in the current vector list state.\n",
    "            return float('inf')\n",
    "        try:\n",
    "            return np.linalg.norm(self.vectors[idx1] - self.vectors[idx2])\n",
    "        except IndexError:\n",
    "             # Should ideally not happen if bounds check passes, but as extra safety\n",
    "             return float('inf')\n",
    "\n",
    "\n",
    "    # --- _search_layer method remains unchanged ---\n",
    "    # (Includes robustness checks from original code [1])\n",
    "    def _search_layer(self, query_vec, layer_idx, ep_idx, ef):\n",
    "        \"\"\"Search within a specific layer starting from an entry point.\"\"\"\n",
    "        # Basic layer validity check\n",
    "        if layer_idx < 0 or layer_idx >= len(self.layers):\n",
    "            return []\n",
    "        layer_graph = self.layers[layer_idx]\n",
    "        if not layer_graph: # Layer exists but is empty\n",
    "            return []\n",
    "\n",
    "        # Robustness: Check if entry point is valid for this layer and vector list\n",
    "        if ep_idx not in layer_graph or ep_idx >= len(self.vectors):\n",
    "            # If invalid, try to find a random valid node in the layer as a fallback EP\n",
    "            try:\n",
    "                valid_indices = [idx for idx in layer_graph if idx < len(self.vectors)]\n",
    "                if not valid_indices:\n",
    "                    return [] # No valid nodes in this layer\n",
    "                ep_idx = random.choice(valid_indices) # Choose a random valid node\n",
    "            except IndexError: # Should not happen if valid_indices check passed\n",
    "                return []\n",
    "\n",
    "        visited = set()\n",
    "        candidates = [] # Min-heap: (distance, node_idx)\n",
    "        results = []    # Max-heap: (-distance, node_idx)\n",
    "\n",
    "        # Initialize search with the entry point\n",
    "        try:\n",
    "             # Ensure vector exists before calculating distance\n",
    "             if ep_idx >= len(self.vectors):\n",
    "                 # This case should be caught by the initial check, but for safety:\n",
    "                 return []\n",
    "             initial_dist = np.linalg.norm(query_vec - self.vectors[ep_idx])\n",
    "             heappush(candidates, (initial_dist, ep_idx))\n",
    "             heappush(results, (-initial_dist, ep_idx))\n",
    "             visited.add(ep_idx)\n",
    "        except IndexError:\n",
    "             # Fallback if vector access fails unexpectedly\n",
    "             # print(f\"Warning: IndexError accessing vector for entry point {ep_idx}...\") # Optional warning\n",
    "             return []\n",
    "\n",
    "        # Greedy search loop\n",
    "        while candidates:\n",
    "            try:\n",
    "                dist_candidate, current_idx = heappop(candidates)\n",
    "            except IndexError: # Heap is empty\n",
    "                break\n",
    "\n",
    "            # Get distance of the farthest node found so far\n",
    "            farthest_dist_in_results = -results[0][0] if results else float('inf')\n",
    "\n",
    "            # Early termination condition\n",
    "            if dist_candidate > farthest_dist_in_results and len(results) >= ef:\n",
    "                 break # All remaining candidates are farther than the worst result\n",
    "\n",
    "            # Check if current node is still valid (might be needed with concurrent modifications, less so here)\n",
    "            # Also check if it exists in the current layer graph\n",
    "            if current_idx not in layer_graph or current_idx >= len(self.vectors):\n",
    "                continue # Skip if node became invalid or is not in this layer\n",
    "\n",
    "            # Explore neighbors\n",
    "            for neighbor_idx in layer_graph.get(current_idx, []):\n",
    "                 # Check if neighbor is valid and not visited\n",
    "                 if neighbor_idx not in visited and neighbor_idx < len(self.vectors):\n",
    "                    visited.add(neighbor_idx)\n",
    "                    try:\n",
    "                        dist_neighbor = np.linalg.norm(query_vec - self.vectors[neighbor_idx])\n",
    "                    except IndexError:\n",
    "                        continue # Skip if neighbor vector access fails\n",
    "\n",
    "                    # Get updated farthest distance\n",
    "                    farthest_dist_in_results = -results[0][0] if results else float('inf')\n",
    "\n",
    "                    # If neighbor is closer than farthest result or results list is not full\n",
    "                    if dist_neighbor < farthest_dist_in_results or len(results) < ef:\n",
    "                        # Add to results (maintaining max-heap property)\n",
    "                        heappush(results, (-dist_neighbor, neighbor_idx))\n",
    "                        # If results exceed ef, remove the farthest\n",
    "                        if len(results) > ef:\n",
    "                            heappop(results)\n",
    "                        # Add neighbor to candidates for further exploration\n",
    "                        heappush(candidates, (dist_neighbor, neighbor_idx))\n",
    "\n",
    "        # Convert max-heap results to sorted list (distance, idx)\n",
    "        final_results = sorted([(-d, idx) for d, idx in results])\n",
    "        return final_results[:ef] # Return top ef results\n",
    "\n",
    "\n",
    "    def insert(self, vector):\n",
    "        \"\"\"Insert a vector into the HNSW index.\"\"\"\n",
    "        new_idx = len(self.vectors) # Index for the new vector\n",
    "        self.vectors.append(np.array(vector)) # Store the vector\n",
    "\n",
    "        # Determine the layers the new node will belong to\n",
    "        node_max_layer = self._get_layer() # Use standard layer selection\n",
    "\n",
    "        current_ep_idx = self.entry_point\n",
    "\n",
    "        # Ensure enough layers exist in the graph structure\n",
    "        while len(self.layers) <= node_max_layer:\n",
    "            self.layers.append(dict()) # Add new empty layer dictionaries\n",
    "\n",
    "        # --- Rest of insert method unchanged from original code [1] ---\n",
    "        # (Includes robustness checks)\n",
    "        current_insert_vec = self.vectors[new_idx] # Use the newly added vector\n",
    "\n",
    "        # Phase 1: Find entry points in upper layers (down to node_max_layer + 1)\n",
    "        for l in range(len(self.layers) - 1, node_max_layer, -1):\n",
    "             if current_ep_idx is None: break # Cannot proceed without an entry point\n",
    "             if not self.layers[l]: continue # Skip empty layers\n",
    "\n",
    "             # Ensure entry point validity before search in this layer\n",
    "             if current_ep_idx not in self.layers[l] or current_ep_idx >= len(self.vectors):\n",
    "                 try:\n",
    "                    # Fallback: Find a random valid node in the layer\n",
    "                    valid_indices = [idx for idx in self.layers[l] if idx < len(self.vectors)]\n",
    "                    if not valid_indices: continue # Skip layer if no valid nodes\n",
    "                    current_ep_idx = random.choice(valid_indices)\n",
    "                 except IndexError: continue # Should not happen\n",
    "\n",
    "             # Search for the closest node in layer l to the new vector (ef=1)\n",
    "             search_results = self._search_layer(current_insert_vec, l, current_ep_idx, ef=1)\n",
    "             if search_results:\n",
    "                 # Update the entry point for the next lower layer\n",
    "                 found_ep_idx = search_results[0][1]\n",
    "                 # Check validity before assignment\n",
    "                 if found_ep_idx < len(self.vectors):\n",
    "                      current_ep_idx = found_ep_idx\n",
    "\n",
    "        # Phase 2: Insert node in layers node_max_layer down to 0\n",
    "        for l in range(min(node_max_layer, len(self.layers) - 1), -1, -1):\n",
    "            layer_graph = self.layers[l]\n",
    "\n",
    "            # Ensure valid entry point for the search in this layer\n",
    "            if current_ep_idx is None or current_ep_idx not in layer_graph or current_ep_idx >= len(self.vectors):\n",
    "                 if layer_graph: # If the layer is not empty\n",
    "                     try:\n",
    "                         # Fallback: Find a random valid node\n",
    "                         valid_indices = [idx for idx in layer_graph if idx < len(self.vectors)]\n",
    "                         if not valid_indices: current_ep_idx = None # No valid EPs possible\n",
    "                         else: current_ep_idx = random.choice(valid_indices)\n",
    "                     except IndexError: current_ep_idx = None\n",
    "                 else: current_ep_idx = None # Layer is empty, no EP\n",
    "\n",
    "            # Find neighbors using search_layer with ef_construction\n",
    "            if current_ep_idx is None:\n",
    "                 neighbors = [] # No entry point, cannot search\n",
    "            else:\n",
    "                 # Search for ef_construction nearest neighbors\n",
    "                 neighbors = self._search_layer(current_insert_vec, l, current_ep_idx, self.ef_construction)\n",
    "                 # Update entry point for next layer (closest neighbor found)\n",
    "                 if neighbors:\n",
    "                      found_ep_idx = neighbors[0][1]\n",
    "                      # Check validity before assignment\n",
    "                      if found_ep_idx < len(self.vectors):\n",
    "                           current_ep_idx = found_ep_idx\n",
    "\n",
    "            # Select M best neighbors based on distance\n",
    "            connections = [idx for dist, idx in neighbors[:self.M]]\n",
    "            # Add connections for the new node in this layer\n",
    "            layer_graph[new_idx] = connections\n",
    "\n",
    "            # Add backlinks from neighbors to the new node, maintaining M limit\n",
    "            for neighbor_idx in connections:\n",
    "                 # Ensure neighbor exists and is valid before modifying its connections\n",
    "                 if neighbor_idx in layer_graph and neighbor_idx < len(self.vectors):\n",
    "                     neighbor_connections = layer_graph[neighbor_idx]\n",
    "                     if new_idx not in neighbor_connections: # Avoid duplicate connections\n",
    "                          neighbor_connections.append(new_idx)\n",
    "                          # Prune connections if exceeding M\n",
    "                          if len(neighbor_connections) > self.M:\n",
    "                               # Ensure all connections are valid before distance calculation for pruning\n",
    "                               valid_conns = [cidx for cidx in neighbor_connections if cidx < len(self.vectors)]\n",
    "                               if len(valid_conns) < len(neighbor_connections):\n",
    "                                   # If some connections became invalid, just update the list\n",
    "                                   layer_graph[neighbor_idx] = valid_conns\n",
    "                                   # Re-check length; might not need pruning anymore\n",
    "                                   if len(valid_conns) <= self.M:\n",
    "                                        continue # Skip pruning if count is now okay\n",
    "\n",
    "                               # Calculate distances only for valid connections\n",
    "                               distances = []\n",
    "                               for conn_idx in valid_conns:\n",
    "                                   dist = self._distance(neighbor_idx, conn_idx)\n",
    "                                   if dist != float('inf'): # Only consider valid distances\n",
    "                                       distances.append((dist, conn_idx))\n",
    "\n",
    "                               # Sort by distance and keep the M closest\n",
    "                               distances.sort()\n",
    "                               layer_graph[neighbor_idx] = [idx for dist, idx in distances[:self.M]]\n",
    "\n",
    "        # Update the global entry point if the new node is in a higher layer\n",
    "        current_ep_layer = self._get_node_layer(self.entry_point) # Find current EP's highest layer\n",
    "        if self.entry_point is None or node_max_layer > current_ep_layer:\n",
    "            self.entry_point = new_idx\n",
    "\n",
    "\n",
    "    # --- Helper method to find the highest layer a node exists in ---\n",
    "    def _get_node_layer(self, node_idx):\n",
    "        \"\"\"Helper to find the highest layer index a node exists in.\"\"\"\n",
    "        if node_idx is None or node_idx < 0: return -1\n",
    "        for l in range(len(self.layers) - 1, -1, -1):\n",
    "             # Check if layer exists and node is in the layer's keys\n",
    "             if l < len(self.layers) and node_idx in self.layers[l]:\n",
    "                 return l\n",
    "        return -1 # Node not found in any layer\n",
    "\n",
    "\n",
    "    # --- search method remains unchanged ---\n",
    "    # (Includes robustness checks from original code [1])\n",
    "    def search(self, query_vec, k=10):\n",
    "        \"\"\"Search for the k nearest neighbors of query_vec.\"\"\"\n",
    "        # Determine ef for search (at least k)\n",
    "        ef_search = max(self.ef_construction, k)\n",
    "        current_ep_idx = self.entry_point\n",
    "\n",
    "        # Check if entry point is valid at the start\n",
    "        if current_ep_idx is None or current_ep_idx >= len(self.vectors):\n",
    "            #print(\"Warning: Entry point is None or invalid at search start.\") # Optional warning\n",
    "            # Potentially try to find *any* valid node if layers exist?\n",
    "            # For now, return empty if no valid starting point.\n",
    "            return []\n",
    "\n",
    "        query_vec = np.array(query_vec) # Ensure query is a numpy array\n",
    "\n",
    "        # Phase 1: Navigate upper layers (down to layer 1) to find good entry point for layer 0\n",
    "        for l in range(len(self.layers) - 1, 0, -1): # Stop at layer 1\n",
    "            if not self.layers[l]: continue # Skip empty layers\n",
    "\n",
    "            # Ensure current entry point is valid for this layer\n",
    "            if current_ep_idx not in self.layers[l] or current_ep_idx >= len(self.vectors):\n",
    "                 try:\n",
    "                      # Fallback: Find a random valid node in the layer\n",
    "                      valid_indices = [idx for idx in self.layers[l] if idx < len(self.vectors)]\n",
    "                      if not valid_indices: continue # Skip layer if no valid nodes\n",
    "                      current_ep_idx = random.choice(valid_indices)\n",
    "                 except IndexError: continue # Should not happen\n",
    "\n",
    "            # Search layer l for the closest node (ef=1)\n",
    "            search_results = self._search_layer(query_vec, l, current_ep_idx, ef=1)\n",
    "            if search_results:\n",
    "                 # Update entry point for the next lower layer\n",
    "                 found_ep_idx = search_results[0][1]\n",
    "                 # Check validity before assignment\n",
    "                 if found_ep_idx < len(self.vectors):\n",
    "                      current_ep_idx = found_ep_idx\n",
    "\n",
    "        # Phase 2: Search layer 0 using the refined entry point\n",
    "        # Ensure the final entry point for layer 0 is valid before the final search\n",
    "        if current_ep_idx >= len(self.vectors) or \\\n",
    "           (0 < len(self.layers) and current_ep_idx not in self.layers[0]):\n",
    "             # If layer 0 exists and has nodes, try to find a fallback EP within it\n",
    "             if 0 < len(self.layers) and self.layers[0]:\n",
    "                  try:\n",
    "                      valid_indices_l0 = [idx for idx in self.layers[0] if idx < len(self.vectors)]\n",
    "                      if not valid_indices_l0: return [] # No valid nodes in layer 0\n",
    "                      current_ep_idx = random.choice(valid_indices_l0)\n",
    "                  except IndexError: return [] # Should not happen\n",
    "             else: return [] # Layer 0 doesn't exist or is empty\n",
    "\n",
    "        # Perform final search in layer 0 with ef_search\n",
    "        neighbors = self._search_layer(query_vec, 0, current_ep_idx, ef_search)\n",
    "\n",
    "        # Return the top k results by index\n",
    "        return [idx for dist, idx in neighbors[:k]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from heapq import heappush, heappop\n",
    "import random\n",
    "import math\n",
    "\n",
    "class HNSWfreqdistance:\n",
    "    \"\"\"\n",
    "    HNSW implementation based on paste.txt [1], modified to boost layer\n",
    "    probability for specified 'hub' nodes during insertion.\n",
    "    Search performance remains identical to the base HNSW implementation.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, max_elements, hubs, boost_const, M=16, ef_construction=200):\n",
    "        \"\"\"\n",
    "        Initializes the HNSWfreqdistance index.\n",
    "\n",
    "        Args:\n",
    "            dim (int): Dimensionality of the vectors.\n",
    "            max_elements (int): Estimated maximum number of elements (informational).\n",
    "            hubs (set): A set of integer indices representing 'hub' nodes.\n",
    "                        Nodes whose *future* index is in this set will have\n",
    "                        their layer assignment probability boosted.\n",
    "            boost_const (float): A small constant added to the layer calculation\n",
    "                                 multiplier (mL) for hub nodes.\n",
    "            M (int): Maximum number of connections per node per layer.\n",
    "            ef_construction (int): Size of the dynamic candidate list during construction.\n",
    "        \"\"\"\n",
    "        self.dim = dim\n",
    "        self.max_elements = max_elements # Informational\n",
    "        self.M = M\n",
    "        self.ef_construction = ef_construction\n",
    "        self.hubs = hubs if hubs is not None else set() # Ensure hubs is a set\n",
    "        self.boost_const = boost_const # Constant for boosting hub layer probability\n",
    "\n",
    "        # Store graph layers: layers[l] is a dict {node_idx: [neighbor_indices]}\n",
    "        self.layers = []\n",
    "        # Store the actual vectors, index corresponds to node_idx\n",
    "        self.vectors = []\n",
    "        # Track the entry point (index of the node in the highest layer)\n",
    "        self.entry_point = None\n",
    "        # Precompute layer multiplier\n",
    "        self.ml = 1 / math.log(self.M) if self.M > 1 else 1\n",
    "\n",
    "\n",
    "    # *** MODIFIED to accept node_idx and apply boost ***\n",
    "    def _get_layer(self, node_idx):\n",
    "        \"\"\"\n",
    "        Determine the layer for a new node based on exponential decay.\n",
    "        If node_idx is in self.hubs, boost the probability of higher layers.\n",
    "\n",
    "        Args:\n",
    "            node_idx (int): The index the new node *will* have upon insertion.\n",
    "\n",
    "        Returns:\n",
    "            int: The selected layer index (>= 0).\n",
    "        \"\"\"\n",
    "        # Use precomputed base multiplier\n",
    "        current_ml = self.ml\n",
    "\n",
    "        # Apply boost if the node index is designated as a hub\n",
    "        if node_idx in self.hubs:\n",
    "            current_ml += self.boost_const\n",
    "\n",
    "        # Calculate layer using the (potentially boosted) multiplier\n",
    "        layer = max(0, int(-math.log(random.random()) * current_ml))\n",
    "        return layer\n",
    "\n",
    "    # --- _distance method remains unchanged from paste.txt [1] ---\n",
    "    def _distance(self, idx1, idx2):\n",
    "        if not (0 <= idx1 < len(self.vectors) and 0 <= idx2 < len(self.vectors)):\n",
    "            return float('inf')\n",
    "        return np.linalg.norm(self.vectors[idx1] - self.vectors[idx2])\n",
    "\n",
    "    # --- _search_layer method remains unchanged from paste.txt [1] ---\n",
    "    # (Includes robustness checks from paste.txt [1])\n",
    "    def _search_layer(self, query_vec, layer_idx, ep_idx, ef):\n",
    "        if layer_idx < 0 or layer_idx >= len(self.layers): return []\n",
    "        layer_graph = self.layers[layer_idx]\n",
    "        if not layer_graph: return []\n",
    "\n",
    "        if ep_idx not in layer_graph or ep_idx >= len(self.vectors):\n",
    "            try:\n",
    "                valid_indices = [idx for idx in layer_graph if idx < len(self.vectors)]\n",
    "                if not valid_indices: return []\n",
    "                ep_idx = random.choice(valid_indices)\n",
    "            except IndexError: return []\n",
    "\n",
    "        visited = set()\n",
    "        candidates = []\n",
    "        results = []\n",
    "\n",
    "        try:\n",
    "            if ep_idx >= len(self.vectors): return []\n",
    "            initial_dist = np.linalg.norm(query_vec - self.vectors[ep_idx])\n",
    "            heappush(candidates, (initial_dist, ep_idx))\n",
    "            heappush(results, (-initial_dist, ep_idx))\n",
    "            visited.add(ep_idx)\n",
    "        except IndexError:\n",
    "             # print(f\"Warning: IndexError accessing vector for entry point {ep_idx}...\") # Optional warning\n",
    "             return []\n",
    "\n",
    "        while candidates:\n",
    "            try:\n",
    "                dist_candidate, current_idx = heappop(candidates)\n",
    "            except IndexError: break\n",
    "\n",
    "            farthest_dist_in_results = -results[0][0] if results else float('inf')\n",
    "            if dist_candidate > farthest_dist_in_results and len(results) >= ef:\n",
    "                 break\n",
    "\n",
    "            if current_idx not in layer_graph or current_idx >= len(self.vectors):\n",
    "                continue\n",
    "\n",
    "            for neighbor_idx in layer_graph.get(current_idx, []):\n",
    "                if neighbor_idx not in visited and neighbor_idx < len(self.vectors):\n",
    "                    visited.add(neighbor_idx)\n",
    "                    dist_neighbor = np.linalg.norm(query_vec - self.vectors[neighbor_idx])\n",
    "                    farthest_dist_in_results = -results[0][0] if results else float('inf')\n",
    "\n",
    "                    if dist_neighbor < farthest_dist_in_results or len(results) < ef:\n",
    "                        heappush(results, (-dist_neighbor, neighbor_idx))\n",
    "                        if len(results) > ef: heappop(results)\n",
    "                        heappush(candidates, (dist_neighbor, neighbor_idx))\n",
    "\n",
    "        final_results = sorted([(-d, idx) for d, idx in results])\n",
    "        return final_results[:ef]\n",
    "\n",
    "    # *** MODIFIED to pass new_idx to _get_layer ***\n",
    "    def insert(self, vector):\n",
    "        \"\"\"Insert a vector into the HNSW index.\"\"\"\n",
    "        new_idx = len(self.vectors) # Determine index before layer calculation\n",
    "        self.vectors.append(np.array(vector))\n",
    "\n",
    "        # *** MODIFIED CALL ***: Pass the node's future index to _get_layer\n",
    "        node_max_layer = self._get_layer(new_idx)\n",
    "\n",
    "        current_ep_idx = self.entry_point\n",
    "\n",
    "        while len(self.layers) <= node_max_layer:\n",
    "            self.layers.append(dict())\n",
    "\n",
    "        # --- Rest of insert method unchanged from paste.txt [1] ---\n",
    "        # (Includes robustness checks)\n",
    "        current_insert_vec = self.vectors[new_idx] # Use stored vector for search\n",
    "\n",
    "        for l in range(len(self.layers) - 1, node_max_layer, -1):\n",
    "             if current_ep_idx is None: break\n",
    "             if not self.layers[l]: continue\n",
    "\n",
    "             # Ensure entry point validity before search\n",
    "             if current_ep_idx not in self.layers[l] or current_ep_idx >= len(self.vectors):\n",
    "                 try:\n",
    "                    valid_indices = [idx for idx in self.layers[l] if idx < len(self.vectors)]\n",
    "                    if not valid_indices: continue\n",
    "                    current_ep_idx = random.choice(valid_indices)\n",
    "                 except IndexError: continue\n",
    "\n",
    "             search_results = self._search_layer(current_insert_vec, l, current_ep_idx, ef=1)\n",
    "             if search_results:\n",
    "                 found_ep_idx = search_results[0][1]\n",
    "                 if found_ep_idx < len(self.vectors): # Check validity before assignment\n",
    "                      current_ep_idx = found_ep_idx\n",
    "\n",
    "        for l in range(min(node_max_layer, len(self.layers) - 1), -1, -1):\n",
    "            layer_graph = self.layers[l]\n",
    "\n",
    "            # Ensure valid entry point for search\n",
    "            if current_ep_idx is None or current_ep_idx not in layer_graph or current_ep_idx >= len(self.vectors):\n",
    "                 if layer_graph:\n",
    "                     try:\n",
    "                         valid_indices = [idx for idx in layer_graph if idx < len(self.vectors)]\n",
    "                         if not valid_indices: current_ep_idx = None\n",
    "                         else: current_ep_idx = random.choice(valid_indices)\n",
    "                     except IndexError: current_ep_idx = None\n",
    "                 else: current_ep_idx = None\n",
    "\n",
    "            if current_ep_idx is None:\n",
    "                 neighbors = []\n",
    "            else:\n",
    "                 neighbors = self._search_layer(current_insert_vec, l, current_ep_idx, self.ef_construction)\n",
    "                 if neighbors:\n",
    "                      found_ep_idx = neighbors[0][1]\n",
    "                      if found_ep_idx < len(self.vectors): # Check validity\n",
    "                           current_ep_idx = found_ep_idx\n",
    "\n",
    "            connections = [idx for dist, idx in neighbors[:self.M]]\n",
    "            layer_graph[new_idx] = connections\n",
    "\n",
    "            for neighbor_idx in connections:\n",
    "                 # Check validity before adding backlink\n",
    "                 if neighbor_idx in layer_graph and neighbor_idx < len(self.vectors):\n",
    "                     neighbor_connections = layer_graph[neighbor_idx]\n",
    "                     if new_idx not in neighbor_connections:\n",
    "                          neighbor_connections.append(new_idx)\n",
    "                          if len(neighbor_connections) > self.M:\n",
    "                               # Check connection validity before distance calc for pruning\n",
    "                               valid_conns = [cidx for cidx in neighbor_connections if cidx < len(self.vectors)]\n",
    "                               if len(valid_conns) < len(neighbor_connections):\n",
    "                                   layer_graph[neighbor_idx] = valid_conns\n",
    "                                   continue\n",
    "\n",
    "                               distances = [(self._distance(neighbor_idx, conn_idx), conn_idx) for conn_idx in valid_conns]\n",
    "                               distances.sort()\n",
    "                               layer_graph[neighbor_idx] = [idx for dist, idx in distances[:self.M]]\n",
    "\n",
    "        # Update global entry point logic (unchanged from paste.txt [1])\n",
    "        current_ep_layer = self._get_node_layer(self.entry_point) # Use helper\n",
    "        if self.entry_point is None or node_max_layer > current_ep_layer:\n",
    "            self.entry_point = new_idx\n",
    "\n",
    "\n",
    "    # --- Helper method from paste-2.txt [2] added for clarity ---\n",
    "    def _get_node_layer(self, node_idx):\n",
    "        # Helper to find highest layer a node exists in\n",
    "        if node_idx is None or node_idx < 0: return -1\n",
    "        for l in range(len(self.layers) - 1, -1, -1):\n",
    "             if node_idx in self.layers[l]:\n",
    "                 return l\n",
    "        return -1 # Node not found\n",
    "\n",
    "    # --- search method remains unchanged from paste.txt [1] ---\n",
    "    # (Includes robustness checks)\n",
    "    def search(self, query_vec, k=10):\n",
    "        ef_search = max(self.ef_construction, k)\n",
    "        current_ep_idx = self.entry_point\n",
    "\n",
    "        if current_ep_idx is None or current_ep_idx >= len(self.vectors):\n",
    "            #print(\"Warning: Entry point is None or invalid...\") # Optional\n",
    "            return []\n",
    "\n",
    "        query_vec = np.array(query_vec)\n",
    "\n",
    "        for l in range(len(self.layers) - 1, 0, -1): # Down to layer 1\n",
    "            if not self.layers[l]: continue\n",
    "\n",
    "            # Ensure entry point validity\n",
    "            if current_ep_idx not in self.layers[l] or current_ep_idx >= len(self.vectors):\n",
    "                 try:\n",
    "                     valid_indices = [idx for idx in self.layers[l] if idx < len(self.vectors)]\n",
    "                     if not valid_indices: continue\n",
    "                     current_ep_idx = random.choice(valid_indices)\n",
    "                 except IndexError: continue\n",
    "\n",
    "            search_results = self._search_layer(query_vec, l, current_ep_idx, ef=1)\n",
    "            if search_results:\n",
    "                 found_ep_idx = search_results[0][1]\n",
    "                 if found_ep_idx < len(self.vectors): # Check validity\n",
    "                      current_ep_idx = found_ep_idx\n",
    "\n",
    "        # Ensure final entry point for layer 0 is valid\n",
    "        if current_ep_idx >= len(self.vectors) or (0 < len(self.layers) and current_ep_idx not in self.layers[0]):\n",
    "             if 0 < len(self.layers) and self.layers[0]:\n",
    "                  try:\n",
    "                      valid_indices_l0 = [idx for idx in self.layers[0] if idx < len(self.vectors)]\n",
    "                      if not valid_indices_l0: return []\n",
    "                      current_ep_idx = random.choice(valid_indices_l0)\n",
    "                  except IndexError: return []\n",
    "             else: return []\n",
    "\n",
    "        neighbors = self._search_layer(query_vec, 0, current_ep_idx, ef_search)\n",
    "        return [idx for dist, idx in neighbors[:k]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- User parameters ---\n",
    "K = 100                   # Number of neighbors for recall\n",
    "NUM_QUERIES = 40000       # Number of queries to use\n",
    "\n",
    "\n",
    "DIMENSION = vectors.shape[1]\n",
    "NUM_ELEMENTS = vectors.shape[0]\n",
    "\n",
    "# --- Select query vectors ---\n",
    "idxs = np.random.choice(NUM_ELEMENTS, size=min(40000, NUM_ELEMENTS), replace=False)\n",
    "query_vectors = vectors[idxs]\n",
    "num_actual_queries = len(query_vectors)\n",
    "\n",
    "# --- Helper functions ---\n",
    "def calculate_recall(ground_truth, found, k):\n",
    "    total_found = 0\n",
    "    num_q = len(ground_truth)\n",
    "    expected = num_q * k\n",
    "\n",
    "    for i in range(num_q):\n",
    "        gt_set = set(ground_truth[i][:k])\n",
    "        res = found[i]\n",
    "        found_set = set(res[:k]) if res is not None else set()\n",
    "        total_found += len(gt_set & found_set)\n",
    "\n",
    "    return total_found / expected if expected > 0 else 1.0\n",
    "\n",
    "# --- Ground Truth Calculation (Corrected) ---\n",
    "import time # Ensure time is imported if not already globally\n",
    "from sklearn.neighbors import NearestNeighbors # Ensure imported\n",
    "import numpy as np # Ensure imported\n",
    "\n",
    "def compute_ground_truth(data, queries, k):\n",
    "    \"\"\"\n",
    "    Computes ground truth returning both distances and indices.\n",
    "    \"\"\"\n",
    "    print(f\"Computing ground truth with brute-force for {len(queries)} queries and k={k}...\")\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    # Ensure k is valid for the number of data points\n",
    "    # We need at least k+1 points to find k neighbors (excluding the point itself if it's in data)\n",
    "    # However, kneighbors handles finding fewer if necessary. Let's check k against shape[0].\n",
    "    if k >= data.shape[0]:\n",
    "        print(f\"Warning: k={k} is >= number of data points ({data.shape[0]}). Reducing k to {data.shape[0] - 1}.\")\n",
    "        k = max(1, data.shape[0] - 1) # Find at least 1 neighbor if possible\n",
    "\n",
    "    if k <= 0:\n",
    "       print(\"Error: Cannot compute neighbors with k <= 0.\")\n",
    "       return np.array([]), np.array([]) # Return empty arrays\n",
    "\n",
    "    nn = NearestNeighbors(n_neighbors=k, algorithm='brute', metric='euclidean')\n",
    "    nn.fit(data)\n",
    "\n",
    "    # Capture BOTH distances and indices from kneighbors\n",
    "    gt_distances, gt_indices = nn.kneighbors(queries, n_neighbors=k)\n",
    "\n",
    "    t1 = time.perf_counter()\n",
    "    print(f\"Done computing ground truth in {t1 - t0:.2f}s\")\n",
    "\n",
    "    # Return BOTH distances and indices\n",
    "    return gt_distances, gt_indices\n",
    "\n",
    "# --- Evaluation for base HNSW ---\n",
    "def evaluate_hnsw_base(all_vecs, queries, gt, k, M=16, ef_const=200):\n",
    "    print(\"\\nEvaluating HNSW (base)\")\n",
    "    index = HNSW(dim=all_vecs.shape[1], max_elements=all_vecs.shape[0], M=M, ef_construction=ef_const)\n",
    "\n",
    "    # Indexing with progress bar\n",
    "    t0 = time.perf_counter()\n",
    "    for v in tqdm(all_vecs, total=all_vecs.shape[0], desc=\"HNSW (base) indexing\"):\n",
    "        index.insert(v)\n",
    "    t1 = time.perf_counter()\n",
    "\n",
    "    # Querying with progress bar\n",
    "    results = []\n",
    "    t_q0 = time.perf_counter()\n",
    "    for q in tqdm(queries, total=len(queries), desc=\"HNSW (base) querying\"):\n",
    "        results.append(index.search(q, k=k))\n",
    "    t_q1 = time.perf_counter()\n",
    "\n",
    "    recall = calculate_recall(gt, results, k)\n",
    "    total_q = t_q1 - t_q0\n",
    "    avg_q = total_q / len(queries)\n",
    "\n",
    "    print(f\"Index time: {t1 - t0:.2f}s\")\n",
    "    print(f\"Total query time: {total_q:.2f}s, Avg/query: {avg_q*1000:.2f}ms\")\n",
    "    print(f\"Recall@{k}: {recall:.4f}\")\n",
    "\n",
    "    return (t1 - t0), total_q, avg_q, recall\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this new helper function (e.g., after calculate_recall)\n",
    "\n",
    "def calculate_recall_dist(gt_distances, found_results, k):\n",
    "    \"\"\"\n",
    "    Calculates recall based on distance.\n",
    "\n",
    "    For each query, determines the distance to the k-th ground truth neighbor (radius).\n",
    "    Counts how many of the k found neighbors fall within this radius.\n",
    "    Averages this count over all queries and divides by k.\n",
    "\n",
    "    Args:\n",
    "        gt_distances (np.ndarray): Array of shape (num_queries, k) containing\n",
    "                                   distances to ground truth neighbors.\n",
    "        found_results (list): List of lists. Each inner list contains\n",
    "                              tuples (distance, index) for found neighbors.\n",
    "        k (int): Number of neighbors considered.\n",
    "\n",
    "    Returns:\n",
    "        float: The distance-based recall score.\n",
    "    \"\"\"\n",
    "    total_within_radius = 0\n",
    "    num_queries = len(gt_distances)\n",
    "\n",
    "    if num_queries == 0 or k == 0:\n",
    "        return 1.0 # Or 0.0, depending on definition for empty cases\n",
    "\n",
    "    for i in range(num_queries):\n",
    "        if i >= len(found_results) or not found_results[i]:\n",
    "            continue # Skip if no results found for this query\n",
    "\n",
    "        # Ground truth distance for the k-th neighbor (defines the radius)\n",
    "        # Ensure k-1 is a valid index\n",
    "        if k > gt_distances.shape[1]:\n",
    "             print(f\"Warning: k={k} exceeds ground truth neighbors found ({gt_distances.shape[1]})\")\n",
    "             continue # Or handle differently\n",
    "        radius = gt_distances[i, k-1]\n",
    "\n",
    "        # Distances of the *found* neighbors (up to k)\n",
    "        # found_results[i] is like [(dist1, idx1), (dist2, idx2), ...]\n",
    "        found_dists = [dist for dist, idx in found_results[i][:k]]\n",
    "\n",
    "        # Count how many found neighbors are within the radius\n",
    "        count_within = sum(1 for dist in found_dists if dist <= radius)\n",
    "        total_within_radius += count_within\n",
    "\n",
    "    # Average recall: (total found within radius) / (total possible = num_queries * k)\n",
    "    expected = num_queries * k\n",
    "    return total_within_radius / expected if expected > 0 else 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluation for base HNSW (Modified to work with index-only search results) ---\n",
    "def evaluate_hnsw_base(all_vecs, queries, gt_distances, gt_indices, k, M=16, ef_const=200):\n",
    "    print(\"\\nEvaluating HNSW (base)\")\n",
    "    # Assume index object is already created and populated elsewhere if we skip indexing\n",
    "    # For this specific request, we NEED to run indexing if the index isn't already built and available globally\n",
    "    # If index IS already built, we'd load/reuse it. Assuming here we run the whole process.\n",
    "    index = HNSW(dim=all_vecs.shape[1], max_elements=all_vecs.shape[0], M=M, ef_construction=ef_const)\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    # Check if index is already populated (e.g., if run previously)\n",
    "    # This is tricky without proper state management; let's assume re-indexing for now\n",
    "    # If you have a pre-built index object, you'd skip this loop.\n",
    "    # For this example, we still show the indexing time calculation.\n",
    "    if not index.vectors: # Simple check if index appears empty\n",
    "         print(\"Index seems empty, running indexing...\")\n",
    "         for v in tqdm(all_vecs, total=all_vecs.shape[0], desc=\"HNSW (base) indexing\"):\n",
    "             index.insert(v)\n",
    "    else:\n",
    "        print(\"Skipping indexing, assuming index is pre-built.\") # Placeholder message\n",
    "    t1 = time.perf_counter()\n",
    "    index_time = t1 - t0 # Record indexing time even if skipped conceptually\n",
    "\n",
    "    # Querying with progress bar - Gets lists of indices\n",
    "    results_indices = [] # Store results from index.search (list of lists of indices)\n",
    "    t_q0 = time.perf_counter()\n",
    "    for q in tqdm(queries, total=len(queries), desc=\"HNSW (base) querying\"):\n",
    "        results_indices.append(index.search(q, k=k)) # Returns list[int]\n",
    "    t_q1 = time.perf_counter()\n",
    "    query_time = t_q1 - t_q0\n",
    "\n",
    "    # --- Post-processing for recall calculations ---\n",
    "\n",
    "    # 1. Standard Recall: Uses the indices directly\n",
    "    # results_indices is already [[idx1, idx2,...], [idx3, idx4,...], ...]\n",
    "    recall_std = calculate_recall(gt_indices, results_indices, k)\n",
    "\n",
    "    # 2. Distance Recall: Manually compute distances for found indices\n",
    "    results_with_distances = [] # Will store [[(d1,i1), (d2,i2),..], ...]\n",
    "    for i, q_vec in enumerate(queries):\n",
    "        found_idxs = results_indices[i] # Get the indices found for this query\n",
    "        query_results_dist = []\n",
    "        for idx in found_idxs:\n",
    "            if 0 <= idx < len(all_vecs): # Check validity\n",
    "                # Calculate distance between query and the vector at found index\n",
    "                dist = np.linalg.norm(q_vec - all_vecs[idx])\n",
    "                query_results_dist.append((dist, idx))\n",
    "            else:\n",
    "                 # Handle case where index might be invalid (shouldn't happen often)\n",
    "                 query_results_dist.append((float('inf'), idx))\n",
    "        # Sort by distance just in case search didn't guarantee order (though it should)\n",
    "        query_results_dist.sort(key=lambda x: x[0])\n",
    "        results_with_distances.append(query_results_dist)\n",
    "\n",
    "    recall_dist = calculate_recall_dist(gt_distances, results_with_distances, k)\n",
    "\n",
    "    avg_q = query_time / len(queries) if len(queries) > 0 else 0\n",
    "\n",
    "    print(f\"Index time: {index_time:.2f}s\") # Show calculated/placeholder index time\n",
    "    print(f\"Total query time: {query_time:.2f}s, Avg/query: {avg_q*1000:.2f}ms\")\n",
    "    print(f\"Recall@{k} (std): {recall_std:.4f}\")\n",
    "    print(f\"Recall@{k} (dist): {recall_dist:.4f}\")\n",
    "\n",
    "    return index_time, query_time, avg_q, recall_std, recall_dist\n",
    "\n",
    "\n",
    "# --- Evaluation for HNSWfreqdistance (Modified to work with index-only search results) ---\n",
    "def evaluate_hnsw_freq(all_vecs, queries, gt_distances, gt_indices, k, hubs, boost_scaling_factor, M=16, ef_const=200):\n",
    "    print(\"\\nEvaluating HNSWfreqdistance\")\n",
    "\n",
    "    # Dynamic boost calculation (remains the same)\n",
    "    if all_vecs.shape[0] > 0 and hubs is not None:\n",
    "        hub_ratio = len(hubs) / all_vecs.shape[0]\n",
    "        boost_const = boost_scaling_factor * hub_ratio\n",
    "        print(f\"  Hub ratio: {hub_ratio:.4f}, Calculated boost_const: {boost_const:.4f}\")\n",
    "    else:\n",
    "        boost_const = 0\n",
    "        print(\"  Hub ratio: N/A or 0, boost_const set to 0\")\n",
    "\n",
    "    # Again, assuming index needs to be built or reused correctly.\n",
    "    index = HNSWfreqdistance(dim=all_vecs.shape[1], max_elements=all_vecs.shape[0],\n",
    "                              hubs=hubs, boost_const=boost_const,\n",
    "                              M=M, ef_construction=ef_const)\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    # Similar check/assumption about indexing as in evaluate_hnsw_base\n",
    "    if not index.vectors:\n",
    "         print(\"Index seems empty, running indexing...\")\n",
    "         for v in tqdm(all_vecs, total=all_vecs.shape[0], desc=\"HNSWfreqdistance indexing\"):\n",
    "             index.insert(v)\n",
    "    else:\n",
    "         print(\"Skipping indexing, assuming index is pre-built.\")\n",
    "    t1 = time.perf_counter()\n",
    "    index_time = t1 - t0\n",
    "\n",
    "    # Querying - Gets lists of indices\n",
    "    results_indices = []\n",
    "    t_q0 = time.perf_counter()\n",
    "    for q in tqdm(queries, total=len(queries), desc=\"HNSWfreqdistance querying\"):\n",
    "        results_indices.append(index.search(q, k=k)) # Returns list[int]\n",
    "    t_q1 = time.perf_counter()\n",
    "    query_time = t_q1 - t_q0\n",
    "\n",
    "    # --- Post-processing for recall calculations ---\n",
    "\n",
    "    # 1. Standard Recall\n",
    "    recall_std = calculate_recall(gt_indices, results_indices, k)\n",
    "\n",
    "    # 2. Distance Recall (Manual distance calculation)\n",
    "    results_with_distances = []\n",
    "    for i, q_vec in enumerate(queries):\n",
    "        found_idxs = results_indices[i]\n",
    "        query_results_dist = []\n",
    "        for idx in found_idxs:\n",
    "             if 0 <= idx < len(all_vecs):\n",
    "                dist = np.linalg.norm(q_vec - all_vecs[idx])\n",
    "                query_results_dist.append((dist, idx))\n",
    "             else:\n",
    "                query_results_dist.append((float('inf'), idx))\n",
    "        query_results_dist.sort(key=lambda x: x[0])\n",
    "        results_with_distances.append(query_results_dist)\n",
    "\n",
    "    recall_dist = calculate_recall_dist(gt_distances, results_with_distances, k)\n",
    "\n",
    "    avg_q = query_time / len(queries) if len(queries) > 0 else 0\n",
    "\n",
    "    print(f\"Index time: {index_time:.2f}s\")\n",
    "    print(f\"Total query time: {query_time:.2f}s, Avg/query: {avg_q*1000:.2f}ms\")\n",
    "    print(f\"Recall@{k} (std): {recall_std:.4f}\")\n",
    "    print(f\"Recall@{k} (dist): {recall_dist:.4f}\")\n",
    "\n",
    "    return index_time, query_time, avg_q, recall_std, recall_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HNSW with HUB BOOSTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing ground truth with brute-force for 40000 queries and k=100...\n",
      "Done computing ground truth in 7.00s\n",
      "\n",
      "Evaluating HNSW (base)\n",
      "Index seems empty, running indexing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HNSW (base) indexing: 100%|██████████| 400000/400000 [39:34<00:00, 168.44it/s]   \n",
      "HNSW (base) querying: 100%|██████████| 40000/40000 [01:39<00:00, 402.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index time: 1131.41s\n",
      "Total query time: 99.35s, Avg/query: 2.48ms\n",
      "Recall@100 (std): 0.5034\n",
      "Recall@100 (dist): 0.5030\n",
      "\n",
      "Evaluating HNSWfreqdistance\n",
      "  Hub ratio: 0.0795, Calculated boost_const: 0.0795\n",
      "Index seems empty, running indexing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HNSWfreqdistance indexing: 100%|██████████| 400000/400000 [22:03<00:00, 302.20it/s]  \n",
      "HNSWfreqdistance querying: 100%|██████████| 40000/40000 [02:25<00:00, 274.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index time: 1234.91s\n",
      "Total query time: 145.77s, Avg/query: 3.64ms\n",
      "Recall@100 (std): 0.3868\n",
      "Recall@100 (dist): 0.3865\n",
      "\n",
      "--- Summary ---\n",
      "Parameters: K=100, Num Queries=40000, M=16, ef_const=200, boost_scaling=1.0\n",
      "--------------------------------------------------------------------------------\n",
      "Method            | Index(s) | Query(s) | AvgQuery(ms) | Recall@100(std) | Recall@100(dist)\n",
      "--------------------------------------------------------------------------------\n",
      "HNSW (base)       | 1131.41  | 99.35    | 2.48         | 0.5034        | 0.5030        \n",
      "HNSWfreqdistance  | 1234.91  | 145.77   | 3.64         | 0.3868        | 0.3865        \n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# In the cell with \"Main Comparison\" (Cell 19 originally)\n",
    "# --- Main Comparison ---\n",
    "if __name__ == '__main__':\n",
    "    # K and NUM_QUERIES defined earlier\n",
    "    K = 100\n",
    "    NUM_QUERIES = 40000 # Reduced from 40000 for potentially faster testing, adjust as needed\n",
    "\n",
    "    # Select query vectors (make sure this uses the NUM_QUERIES variable)\n",
    "    # Ensure we don't request more queries than available vectors\n",
    "    actual_num_queries = min(NUM_QUERIES, vectors.shape[0])\n",
    "    if actual_num_queries < NUM_QUERIES:\n",
    "        print(f\"Warning: Requested {NUM_QUERIES} queries, but only {vectors.shape[0]} vectors available. Using {actual_num_queries}.\")\n",
    "\n",
    "    idxs = np.random.choice(vectors.shape[0], size=actual_num_queries, replace=False)\n",
    "    query_vectors_main = vectors[idxs] # Use a different variable name if needed to avoid conflict\n",
    "\n",
    "\n",
    "    # *** Get GT distances and indices ***\n",
    "    gt_distances, gt_indices = compute_ground_truth(vectors, query_vectors_main, K)\n",
    "\n",
    "    # Base HNSW\n",
    "    idx_time_base, q_time_base, avg_q_base, recall_std_base, recall_dist_base = evaluate_hnsw_base(\n",
    "        vectors, query_vectors_main, gt_distances, gt_indices, K, # Pass GT distances\n",
    "        M=16, ef_const=200\n",
    "    )\n",
    "\n",
    "    # HNSW with hub boosting\n",
    "    # *** Choose a scaling factor for the dynamic boost ***\n",
    "    # E.g., 1.0 means boost_const = hub_ratio\n",
    "    # E.g., 0.5 means boost_const = 0.5 * hub_ratio\n",
    "    BOOST_SCALING_FACTOR = 1.0 # You can tune this hyperparameter\n",
    "\n",
    "    # *** Pass hubs, boost_scaling_factor, and ef_const correctly ***\n",
    "    idx_time_freq, q_time_freq, avg_q_freq, recall_std_freq, recall_dist_freq = evaluate_hnsw_freq(\n",
    "        vectors, query_vectors_main, gt_distances, gt_indices, K, # Pass GT distances\n",
    "        hubs=hubs, boost_scaling_factor=BOOST_SCALING_FACTOR, # Pass scaling factor\n",
    "        M=16, ef_const=200 # Pass ef_const\n",
    "    )\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\n--- Summary ---\")\n",
    "    print(f\"Parameters: K={K}, Num Queries={actual_num_queries}, M=16, ef_const=200, boost_scaling={BOOST_SCALING_FACTOR if 'BOOST_SCALING_FACTOR' in locals() else 'N/A'}\")\n",
    "    print(\"-\" * 80)\n",
    "    # Updated header to include both recalls\n",
    "    print(f\"Method            | Index(s) | Query(s) | AvgQuery(ms) | Recall@{K}(std) | Recall@{K}(dist)\")\n",
    "    print(\"-\" * 80)\n",
    "    # Updated print statements\n",
    "    print(f\"HNSW (base)       | {idx_time_base:<8.2f} | {q_time_base:<8.2f} | {avg_q_base*1000:<12.2f} | {recall_std_base:<13.4f} | {recall_dist_base:<14.4f}\")\n",
    "    print(f\"HNSWfreqdistance  | {idx_time_freq:<8.2f} | {q_time_freq:<8.2f} | {avg_q_freq*1000:<12.2f} | {recall_std_freq:<13.4f} | {recall_dist_freq:<14.4f}\")\n",
    "    print(\"-\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
