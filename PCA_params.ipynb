{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import hnswlib\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from heapq import heappush, heappop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors of dimension 100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Path to your GloVe file (update this based on your downloaded version)\n",
    "glove_path = '/Users/tanishqchaudhari/Desktop/DataScience Proj  datasets/Dataset/glove.6B.100d.txt'\n",
    "\n",
    "# Load GloVe vectors\n",
    "word_to_vec = {}\n",
    "words = []\n",
    "vectors = []\n",
    "\n",
    "with open(glove_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]  # First token is the word\n",
    "        vector = np.array(values[1:], dtype=np.float32)  # Rest are vector values\n",
    "        word_to_vec[word] = vector\n",
    "        words.append(word)\n",
    "        vectors.append(vector)\n",
    "\n",
    "# Convert to numpy array\n",
    "vectors = np.array(vectors, dtype=np.float32)\n",
    "print(f\"Loaded {len(words)} word vectors of dimension {vectors.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400000, 100)\n"
     ]
    }
   ],
   "source": [
    "print(vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import math\n",
    "from heapq import heappush, heappop\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import pandas as pd\n",
    "\n",
    "# configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def precompute_all_query_pcas(pca_models, pca_applied_layers, query_vectors):\n",
    "    \"\"\"\n",
    "    Precomputes PCA projections for a batch of query vectors.\n",
    "    Returns a list where each index corresponds to the layer number, and contains\n",
    "    either the PCA-transformed queries (np.ndarray) or None if not used.\n",
    "    \"\"\"\n",
    "    max_layer = max(pca_applied_layers) + 1 if pca_applied_layers else 0\n",
    "    pca_queries = [None] * max_layer  # indexed list for O(1) access\n",
    "\n",
    "    query_vectors_np = np.asarray(query_vectors)\n",
    "\n",
    "    for l_idx in pca_applied_layers:\n",
    "        model = pca_models[l_idx]  # direct index, avoid .get()\n",
    "        pca_queries[l_idx] = model.transform(query_vectors_np)\n",
    "\n",
    "    return pca_queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_all_query_pcas(pca_models, pca_applied_layers, query_vectors):\n",
    "    \"\"\"\n",
    "    Precomputes PCA projections for a batch of query vectors.\n",
    "    Returns a list where each index = layer number, and value is\n",
    "    either the (n_queries, pca_components) array or None.\n",
    "    \"\"\"\n",
    "    if not pca_applied_layers:\n",
    "        return []\n",
    "\n",
    "    max_layer = max(pca_applied_layers) + 1\n",
    "    pca_queries = [None] * max_layer\n",
    "    Q = np.asarray(query_vectors)\n",
    "\n",
    "    for l in pca_applied_layers:\n",
    "        model = pca_models[l]\n",
    "        pca_queries[l] = model.transform(Q)\n",
    "\n",
    "    return pca_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import logging\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from heapq import heappush, heappop\n",
    "import time # Added for potential timing within methods if needed\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- precompute_all_query_pcas function remains the same ---\n",
    "def precompute_all_query_pcas(pca_models, pca_applied_layers, query_vectors):\n",
    "    \"\"\"\n",
    "    Precomputes PCA projections for a batch of query vectors.\n",
    "    Returns a list where each index = layer number, and value is\n",
    "    either the (n_queries, pca_components) array or None.\n",
    "    \"\"\"\n",
    "    if not pca_applied_layers:\n",
    "        return []\n",
    "\n",
    "    max_layer = max(pca_applied_layers) + 1\n",
    "    pca_queries = [None] * max_layer\n",
    "    Q = np.asarray(query_vectors)\n",
    "\n",
    "    for l in pca_applied_layers:\n",
    "        # Check if the model exists for the layer (robustness)\n",
    "        if l in pca_models:\n",
    "            model = pca_models[l]\n",
    "            pca_queries[l] = model.transform(Q)\n",
    "        # else: # Should not happen if pca_applied_layers is consistent\n",
    "        #    logger.warning(f\"Model for supposedly applied PCA layer {l} not found during query precomputation.\")\n",
    "\n",
    "\n",
    "    return pca_queries\n",
    "\n",
    "# --- Modified optHNSWPCA Class ---\n",
    "class optHNSWPCA:\n",
    "    \"\"\"\n",
    "    HNSW with optional PCA-based acceleration, applied only to layers >= 2.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, max_elements, M=16, ef_construction=200,\n",
    "                 pca_yes=False, pca_components=50): # Removed pca_top_layers\n",
    "        self.dim = dim\n",
    "        self.max_elements = max_elements\n",
    "        self.M = M\n",
    "        self.ef_construction = ef_construction\n",
    "        self.pca_yes = pca_yes and pca_components < dim\n",
    "        # pca_top_layers parameter is removed\n",
    "        self.pca_components = min(pca_components, dim)\n",
    "\n",
    "        self.vectors = [] # List to store full-dimensional vectors\n",
    "        self.layers = [] # List of dictionaries representing graph layers\n",
    "        self.entry_point = None # Index of the entry point node\n",
    "        self.entry_point_level = -1 # Highest layer the entry point exists in\n",
    "\n",
    "        # PCA internals - populated by finalize_pca\n",
    "        self.pca_models = {} # Dictionary {layer_idx: fitted_pca_model}\n",
    "        self.pca_applied_layers = set() # Set of layer indices where PCA is active\n",
    "\n",
    "        # Optimized lookup structures for PCA search - populated by finalize_pca\n",
    "        self._rv_arr = None      # Numpy array shape (max_elements, pca_components) storing reduced vectors\n",
    "        self._rv_mask = None     # Boolean mask (max_elements,) indicating if a node has a reduced vector\n",
    "        self._use_pca_layer = None  # Boolean mask (num_layers,) indicating if PCA logic should be used for a layer\n",
    "\n",
    "        logger.info(f\"Initialized HNSW (dim={dim}, PCA={'on (layers >= 2)' if self.pca_yes else 'off'}, PCA components={self.pca_components if self.pca_yes else 'N/A'})\")\n",
    "\n",
    "    def _get_layer(self):\n",
    "        # Determine the layer for a new node using exponential decay based on M\n",
    "        ml = 1 / math.log(self.M) if self.M > 1 else 1\n",
    "        # Ensure level is at least 0\n",
    "        return max(0, int(-math.log(random.random()) * ml))\n",
    "\n",
    "    def _distance(self, idx1, idx2):\n",
    "        # Calculate Euclidean distance between two vectors using their indices\n",
    "        # Assumes indices are valid and vectors exist\n",
    "        return np.linalg.norm(self.vectors[idx1] - self.vectors[idx2])\n",
    "\n",
    "    def _search_layer_standard(self, query_vec, layer_idx, ep_idx, ef):\n",
    "        # Standard HNSW search within a single layer using full-dimensional vectors\n",
    "        graph = self.layers[layer_idx]\n",
    "        if not graph: # Layer is empty\n",
    "            return []\n",
    "\n",
    "        # Ensure the entry point exists in this layer's graph\n",
    "        if ep_idx not in graph:\n",
    "            # If ep_idx isn't valid, try starting from the first node in the layer\n",
    "            try:\n",
    "                ep_idx = next(iter(graph))\n",
    "            except StopIteration: # Layer exists but is empty (shouldn't happen if !graph check passes)\n",
    "                 return []\n",
    "\n",
    "        visited = {ep_idx} # Keep track of visited nodes to avoid cycles/redundancy\n",
    "        dist0 = np.linalg.norm(query_vec - self.vectors[ep_idx]) # Distance to entry point\n",
    "        candidates = [(dist0, ep_idx)] # Min-heap storing (distance, node_idx) candidates to visit\n",
    "        results = [(-dist0, ep_idx)] # Max-heap storing (-distance, node_idx) of potential neighbors found\n",
    "\n",
    "        while candidates:\n",
    "            dist, cur = heappop(candidates) # Get the closest candidate node\n",
    "            farthest_dist_in_results = -results[0][0] # Distance to the farthest node currently in results\n",
    "\n",
    "            # Optimization: If the closest candidate is farther than the farthest result\n",
    "            # and we already have enough results (ef), we can stop exploring this path.\n",
    "            if dist > farthest_dist_in_results and len(results) >= ef:\n",
    "                break\n",
    "\n",
    "            # Explore neighbors of the current node\n",
    "            # Use .get(cur, []) to handle nodes that might have no outgoing links (shouldn't happen with bidirectional links)\n",
    "            for nb in graph.get(cur, []):\n",
    "                if nb in visited:\n",
    "                    continue # Skip already visited nodes\n",
    "                visited.add(nb)\n",
    "\n",
    "                # Check if neighbor index is valid (robustness for potential graph inconsistencies)\n",
    "                if nb >= len(self.vectors):\n",
    "                    logger.warning(f\"Neighbor index {nb} out of bounds (max: {len(self.vectors)-1}) in layer {layer_idx}. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                d = np.linalg.norm(query_vec - self.vectors[nb]) # Calculate distance to neighbor\n",
    "\n",
    "                # If the neighbor is closer than the farthest result, or we don't have ef results yet\n",
    "                if d < farthest_dist_in_results or len(results) < ef:\n",
    "                    heappush(results, (-d, nb)) # Add to results (max-heap, so use negative distance)\n",
    "                    # If results exceed ef, remove the farthest one\n",
    "                    if len(results) > ef:\n",
    "                        heappop(results)\n",
    "                    # Add the neighbor to candidates for further exploration\n",
    "                    heappush(candidates, (d, nb))\n",
    "\n",
    "        # Return the top 'ef' results, sorted by actual distance (ascending)\n",
    "        return sorted([(-d, idx) for d, idx in results])[:ef]\n",
    "\n",
    "    def insert(self, vector):\n",
    "        # Insert a new vector into the HNSW graph\n",
    "        vec = np.asarray(vector)\n",
    "        idx = len(self.vectors) # Index for the new vector\n",
    "\n",
    "        if idx >= self.max_elements:\n",
    "            logger.error(f\"Attempted to insert vector {idx}, but index capacity ({self.max_elements}) reached.\")\n",
    "            raise MemoryError(\"Index full\")\n",
    "\n",
    "        self.vectors.append(vec) # Store the full-dimensional vector\n",
    "        level = self._get_layer() # Determine the maximum layer this node will exist in\n",
    "\n",
    "        # Ensure the layers list is large enough\n",
    "        while len(self.layers) <= level:\n",
    "            self.layers.append({}) # Add new empty layers if needed\n",
    "\n",
    "        ep = self.entry_point # Start search from the global entry point\n",
    "\n",
    "        # Phase 1: Find entry points for each layer from top down to target 'level' + 1\n",
    "        # This navigates the upper layers to find a good starting point for insertion in the layers below.\n",
    "        for l in range(self.entry_point_level, level, -1): # Iterate downwards from highest layer down to level+1\n",
    "            if ep is None: break # Should not happen if entry_point_level >= 0\n",
    "            if not self.layers[l]: continue # Skip empty layers\n",
    "\n",
    "            # Search in layer 'l' to find the closest node to the new vector 'vec'\n",
    "            # Use ef=1 as we only need the single best entry point for the next layer down.\n",
    "            res = self._search_layer_standard(vec, l, ep, ef=1)\n",
    "            if res:\n",
    "                # Update entry point for the next lower layer search\n",
    "                ep = res[0][1]\n",
    "            # else: # No node found in this layer? Continue with the current ep.\n",
    "\n",
    "        # Phase 2: Insert the node into layers from 'level' down to 0\n",
    "        # At each layer, find neighbors and establish connections.\n",
    "        for l in range(min(level, self.entry_point_level if self.entry_point is not None else -1), -1, -1):\n",
    "            graph = self.layers[l]\n",
    "\n",
    "            # If entry point 'ep' is None (e.g., first node) and graph isn't empty, pick a random node as ep\n",
    "            if ep is None and graph:\n",
    "                 try: ep = next(iter(graph))\n",
    "                 except StopIteration: pass # Graph became empty?\n",
    "\n",
    "            neigh = [] # List to store neighbors found in this layer\n",
    "            if ep is not None:\n",
    "                # Search for ef_construction nearest neighbors to 'vec' in layer 'l', starting from 'ep'\n",
    "                neigh = self._search_layer_standard(vec, l, ep, self.ef_construction)\n",
    "                if neigh:\n",
    "                    # Update the entry point 'ep' for the next layer down (layer l-1)\n",
    "                    # The closest node found in this layer is a good starting point for the layer below.\n",
    "                    ep = neigh[0][1]\n",
    "                # else: ep remains the same if no neighbors found\n",
    "\n",
    "            # Select the top M neighbors based on distance\n",
    "            conns = [nid for _, nid in neigh[:self.M]]\n",
    "            # Add outgoing connections from the new node 'idx' to its neighbors\n",
    "            graph[idx] = conns\n",
    "\n",
    "            # Add incoming connections from neighbors back to the new node 'idx'\n",
    "            for nb in conns:\n",
    "                # Ensure neighbor exists in the graph dict\n",
    "                graph.setdefault(nb, [])\n",
    "                neighbor_conns = graph[nb]\n",
    "                neighbor_conns.append(idx)\n",
    "\n",
    "                # Pruning: If neighbor now has > M connections, remove the farthest one\n",
    "                if len(neighbor_conns) > self.M:\n",
    "                    dists = [(self._distance(nb, c), c) for c in neighbor_conns]\n",
    "                    dists.sort() # Sort by distance\n",
    "                    # Keep only the M closest connections\n",
    "                    graph[nb] = [c for _, c in dists[:self.M]]\n",
    "\n",
    "        # Update global entry point if the new node is inserted at a higher level than the current entry point\n",
    "        if self.entry_point is None or level > self.entry_point_level:\n",
    "            self.entry_point = idx\n",
    "            self.entry_point_level = level\n",
    "\n",
    "\n",
    "    def finalize_pca(self):\n",
    "        # Fit PCA models for specified layers and prepare optimized structures for PCA search.\n",
    "        if not self.pca_yes:\n",
    "            logger.info(\"PCA is disabled (pca_yes=False). Skipping PCA finalization.\")\n",
    "            return\n",
    "\n",
    "        total_layers = len(self.layers)\n",
    "        self.pca_models.clear()\n",
    "        self.pca_applied_layers.clear()\n",
    "\n",
    "        # --- Define the starting layer for PCA ---\n",
    "        # Apply PCA only to layers 2 and above.\n",
    "        start_pca_consideration_layer = 2\n",
    "\n",
    "        logger.info(f\"Attempting to apply PCA (components={self.pca_components}) to layers >= {start_pca_consideration_layer}...\")\n",
    "\n",
    "        layers_fitted = []\n",
    "        # Iterate through layers starting from layer 2 up to the highest layer\n",
    "        for l in range(start_pca_consideration_layer, total_layers):\n",
    "            if l not in self.layers: # Should not happen, but defensive check\n",
    "                continue\n",
    "\n",
    "            graph = self.layers[l]\n",
    "            # Get valid indices of nodes present in this layer's graph\n",
    "            idxs = [i for i in graph if i < len(self.vectors)] # Ensure index is within bounds of stored vectors\n",
    "\n",
    "            # Check if there are enough nodes in the layer to perform PCA\n",
    "            if len(idxs) <= self.pca_components:\n",
    "                logger.info(f\"Skipping PCA on layer {l}: Not enough nodes ({len(idxs)}) for {self.pca_components} components.\")\n",
    "                continue\n",
    "\n",
    "            logger.info(f\"Fitting PCA for layer {l} on {len(idxs)} nodes...\")\n",
    "            try:\n",
    "                # Stack the full-dimensional vectors for nodes in this layer\n",
    "                arr = np.vstack([self.vectors[i] for i in idxs])\n",
    "\n",
    "                # Fit PCA model\n",
    "                pca = PCA(n_components=self.pca_components)\n",
    "                pca.fit(arr)\n",
    "\n",
    "                # Store the fitted model and mark the layer as having PCA applied\n",
    "                self.pca_models[l] = pca\n",
    "                self.pca_applied_layers.add(l)\n",
    "                layers_fitted.append(l)\n",
    "                logger.info(f\"PCA successfully fitted for layer {l}.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error applying PCA to layer {l}: {e}. Skipping PCA for this layer.\")\n",
    "                # Ensure layer is not marked if PCA failed\n",
    "                if l in self.pca_applied_layers:\n",
    "                     self.pca_applied_layers.remove(l)\n",
    "                if l in self.pca_models:\n",
    "                     del self.pca_models[l]\n",
    "                continue\n",
    "\n",
    "        # If no layers ended up having PCA applied, log and exit finalize_pca\n",
    "        if not self.pca_applied_layers:\n",
    "             logger.warning(\"PCA was enabled, but no layers met the criteria (layer >= 2 and sufficient nodes). PCA search will not be used.\")\n",
    "             # Initialize mask for search function robustness, although it won't be used\n",
    "             self._use_pca_layer = np.zeros(total_layers, dtype=bool)\n",
    "             return\n",
    "\n",
    "        # --- Prepare optimized structures only if PCA was applied to at least one layer ---\n",
    "        logger.info(f\"Preparing optimized PCA lookup structures for layers: {sorted(list(self.pca_applied_layers))}\")\n",
    "\n",
    "        # Allocate arrays for reduced vectors and masks\n",
    "        # Using float32 for reduced vectors can save memory\n",
    "        self._rv_arr = np.zeros((self.max_elements, self.pca_components), dtype=np.float32)\n",
    "        self._rv_mask = np.zeros(self.max_elements, dtype=bool) # Tracks which nodes have a reduced vector stored\n",
    "        self._use_pca_layer = np.zeros(total_layers, dtype=bool) # Tracks which layers use PCA search logic\n",
    "\n",
    "        # Fill the reduced vectors array and update masks\n",
    "        for l in self.pca_applied_layers:\n",
    "            # Get node indices again for this layer\n",
    "            idxs = [i for i in self.layers[l] if i < len(self.vectors)]\n",
    "            if not idxs: continue # Skip if layer somehow became empty\n",
    "\n",
    "            # Get the corresponding PCA model\n",
    "            pca_model = self.pca_models[l]\n",
    "\n",
    "            # Stack vectors and transform them using the layer's PCA model\n",
    "            # Note: It might be slightly more efficient to transform only vectors\n",
    "            # not already transformed by a higher layer's PCA, but this is simpler.\n",
    "            arr = np.vstack([self.vectors[i] for i in idxs])\n",
    "            red = pca_model.transform(arr).astype(np.float32) # Transform and cast to float32\n",
    "\n",
    "            # Store the reduced vectors in the pre-allocated array and set the mask\n",
    "            for node_idx, vec_red in zip(idxs, red):\n",
    "                if node_idx < self.max_elements: # Bounds check\n",
    "                    self._rv_arr[node_idx] = vec_red\n",
    "                    self._rv_mask[node_idx] = True # Mark this node as having a reduced vector\n",
    "                else:\n",
    "                     logger.warning(f\"Node index {node_idx} >= max_elements ({self.max_elements}) during PCA finalization. Skipping.\")\n",
    "\n",
    "\n",
    "            self._use_pca_layer[l] = True # Mark this layer for using PCA search logic\n",
    "\n",
    "        logger.info(f\"PCA finalization complete. Applied to layers: {sorted(list(self.pca_applied_layers))}\")\n",
    "\n",
    "\n",
    "    def _search_layer_pca_precomputed(self, query_idx, query_vec, pca_queries, layer_idx, ep_idx, ef):\n",
    "        # PCA-accelerated search within a single layer using precomputed PCA-transformed queries\n",
    "        graph = self.layers[layer_idx]\n",
    "        if not graph: return []\n",
    "\n",
    "        # Check if the precomputed PCA query exists for this layer and query index\n",
    "        if pca_queries[layer_idx] is None or query_idx >= len(pca_queries[layer_idx]):\n",
    "             logger.warning(f\"PCA query vector missing for query {query_idx} in layer {layer_idx}. Falling back to standard search for this layer.\")\n",
    "             # Fallback to standard search for this specific layer call\n",
    "             return self._search_layer_standard(query_vec, layer_idx, ep_idx, ef)\n",
    "\n",
    "        q_red = pca_queries[layer_idx][query_idx] # Get the precomputed reduced query vector\n",
    "\n",
    "        # Ensure entry point exists and has a reduced vector\n",
    "        if ep_idx not in graph or not self._rv_mask[ep_idx]:\n",
    "             logger.warning(f\"Entry point {ep_idx} missing or lacks reduced vector in PCA layer {layer_idx}. Trying graph iterator.\")\n",
    "             try:\n",
    "                 # Try to find a valid starting node in the graph that *does* have a reduced vector\n",
    "                 valid_eps = [idx for idx in graph if self._rv_mask[idx]]\n",
    "                 if not valid_eps:\n",
    "                      logger.error(f\"No nodes with reduced vectors found in PCA layer {layer_idx}. Cannot start PCA search. Falling back.\")\n",
    "                      return self._search_layer_standard(query_vec, layer_idx, ep_idx if ep_idx in graph else next(iter(graph)), ef)\n",
    "                 ep_idx = random.choice(valid_eps) # Start from a random valid node\n",
    "                 logger.info(f\"Restarting PCA search in layer {layer_idx} from new entry point {ep_idx}.\")\n",
    "             except StopIteration:\n",
    "                 logger.error(f\"Graph iterator failed in PCA layer {layer_idx} fallback. Returning empty.\")\n",
    "                 return [] # Layer is effectively empty or unusable\n",
    "\n",
    "        visited = {ep_idx}\n",
    "        # Calculate initial distance using reduced vectors\n",
    "        init_dist = np.linalg.norm(q_red - self._rv_arr[ep_idx])\n",
    "        candidates = [(init_dist, ep_idx)] # Min-heap for candidates\n",
    "        results = [(-init_dist, ep_idx)] # Max-heap for results\n",
    "\n",
    "        while candidates:\n",
    "            dist, cur = heappop(candidates)\n",
    "            farthest_dist_in_results = -results[0][0]\n",
    "\n",
    "            if dist > farthest_dist_in_results and len(results) >= ef:\n",
    "                break\n",
    "\n",
    "            for nb in graph.get(cur, []):\n",
    "                if nb in visited:\n",
    "                    continue\n",
    "                visited.add(nb)\n",
    "\n",
    "                # Check if neighbor index is valid (bounds check)\n",
    "                if nb >= self.max_elements:\n",
    "                     logger.warning(f\"Neighbor index {nb} out of bounds ({self.max_elements}) in PCA layer {layer_idx} search. Skipping.\")\n",
    "                     continue\n",
    "\n",
    "                # Check if the neighbor has a precomputed reduced vector\n",
    "                if self._rv_mask[nb]:\n",
    "                    # Calculate distance in reduced PCA space\n",
    "                    d = np.linalg.norm(q_red - self._rv_arr[nb])\n",
    "                else:\n",
    "                    # Fallback: Neighbor exists in this layer but somehow doesn't have a reduced vector\n",
    "                    # This shouldn't happen if finalize_pca worked correctly for all nodes in the layer.\n",
    "                    # Calculate distance using original vectors as a fallback.\n",
    "                    logger.warning(f\"Node {nb} in PCA layer {layer_idx} missing reduced vector. Using full distance calculation.\")\n",
    "                    # Ensure the original vector exists\n",
    "                    if nb >= len(self.vectors):\n",
    "                         logger.error(f\"Node {nb} index out of bounds for original vectors too. Skipping.\")\n",
    "                         continue\n",
    "                    d = np.linalg.norm(query_vec - self.vectors[nb])\n",
    "\n",
    "                # Add to results/candidates if it's a potential neighbor\n",
    "                if d < farthest_dist_in_results or len(results) < ef:\n",
    "                    heappush(results, (-d, nb))\n",
    "                    if len(results) > ef:\n",
    "                        heappop(results)\n",
    "                    heappush(candidates, (d, nb))\n",
    "\n",
    "        # Return results sorted by distance\n",
    "        return sorted([(-d, idx) for d, idx in results])[:ef]\n",
    "\n",
    "\n",
    "    def search(self, query_vec, k=10):\n",
    "        # Standard HNSW search (no PCA acceleration used)\n",
    "        ef = max(self.ef_construction, k) # Use ef_construction or k, whichever is larger, for search quality\n",
    "        ep = self.entry_point\n",
    "        if ep is None: # Handle empty index\n",
    "            return []\n",
    "\n",
    "        q = np.asarray(query_vec) # Ensure query is a numpy array\n",
    "\n",
    "        current_ep_level = self.entry_point_level\n",
    "        # Phase 1: Navigate top layers (down to layer 1) to find entry point for layer 0\n",
    "        for l in range(current_ep_level, 0, -1): # Iterate from highest layer down to layer 1\n",
    "             if not self.layers[l]: continue # Skip empty layers\n",
    "\n",
    "             # Find the single closest node in layer 'l' to use as entry point for layer 'l-1'\n",
    "             res = self._search_layer_standard(q, l, ep, ef=1)\n",
    "             if res:\n",
    "                 ep = res[0][1] # Update entry point for the next layer down\n",
    "\n",
    "        # Phase 2: Perform exhaustive search in the base layer (layer 0)\n",
    "        # Use the entry point 'ep' found from navigating upper layers.\n",
    "        # Use the larger 'ef' value for higher recall in the base layer.\n",
    "        res = self._search_layer_standard(q, 0, ep, ef)\n",
    "\n",
    "        # Return the indices of the top k results\n",
    "        return [i for _, i in res[:k]]\n",
    "\n",
    "    def search_pca_opt(self, query_idx, query_vec, pca_queries, k=10):\n",
    "        # HNSW search potentially accelerated by PCA in layers >= 2.\n",
    "        ef = max(self.ef_construction, k)\n",
    "        ep = self.entry_point\n",
    "        if ep is None: return []\n",
    "\n",
    "        q = np.asarray(query_vec) # Original query vector (needed for fallbacks)\n",
    "\n",
    "        current_ep_level = self.entry_point_level\n",
    "\n",
    "        # Check if PCA structures are initialized (robustness)\n",
    "        pca_ready = self.pca_yes and self._use_pca_layer is not None and pca_queries is not None\n",
    "\n",
    "        # Phase 1: Navigate top layers (down to layer 1)\n",
    "        for l in range(current_ep_level, 0, -1):\n",
    "            if not self.layers[l]: continue\n",
    "\n",
    "            use_pca_for_this_layer = False\n",
    "            # Determine if PCA should be used for this layer's search step\n",
    "            if pca_ready and l < len(self._use_pca_layer) and self._use_pca_layer[l] and l < len(pca_queries):\n",
    "                  use_pca_for_this_layer = True\n",
    "\n",
    "            # Perform search in layer 'l' (either PCA or standard) to find entry point for 'l-1'\n",
    "            if use_pca_for_this_layer:\n",
    "                 # Use PCA search, ef=1 as we only need the best entry point\n",
    "                 res = self._search_layer_pca_precomputed(query_idx, q, pca_queries, l, ep, ef=1)\n",
    "            else:\n",
    "                 # Use standard search, ef=1\n",
    "                 res = self._search_layer_standard(q, l, ep, ef=1)\n",
    "\n",
    "            if res:\n",
    "                ep = res[0][1] # Update entry point for next layer down\n",
    "\n",
    "        # Phase 2: Search in base layer (layer 0) - *Always use standard search*\n",
    "        # Layer 0 is intentionally excluded from PCA application in finalize_pca.\n",
    "        # We use the larger 'ef' here for quality.\n",
    "        res = self._search_layer_standard(q, 0, ep, ef)\n",
    "\n",
    "        # Return top k results\n",
    "        return [i for (_, i) in res[:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting query data...\n",
      "Using 40000 queries.\n",
      "Computing ground truth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Initialized HNSW (dim=100, PCA=off, PCA components=N/A)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth computed in 9.37s\n",
      "\n",
      "Building standard HNSW index (no PCA)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing standard HNSW: 100%|██████████| 400000/400000 [47:04<00:00, 141.60it/s]  \n",
      "INFO:__main__:Initialized HNSW (dim=100, PCA=on (layers >= 2), PCA components=25)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard build time: 2307.32s\n",
      "Standard index height: 4\n",
      "\n",
      "Building PCA-enabled HNSW index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing PCA-enabled HNSW: 100%|██████████| 400000/400000 [38:37<00:00, 172.62it/s] \n",
      "INFO:__main__:Attempting to apply PCA (components=25) to layers >= 2...\n",
      "WARNING:__main__:PCA was enabled, but no layers met the criteria (layer >= 2 and sufficient nodes). PCA search will not be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA-enabled index build time (before finalize): 2317.24s\n",
      "PCA-enabled index height: 5\n",
      "\n",
      "Finalizing PCA models on index...\n",
      "PCA finalize time: 0.00s\n",
      "\n",
      "Precomputing PCA projections for queries...\n",
      "Skipping query PCA precomputation as PCA was not applied to any layers.\n",
      "Query PCA precomputation time: 0.0000s\n",
      "\n",
      "Evaluating standard search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Standard query: 100%|██████████| 40000/40000 [03:58<00:00, 167.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating PCA-optimized search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PCA-opt query: 100%|██████████| 40000/40000 [03:47<00:00, 175.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Results ---\n",
      "Parameters: M=20, ef_construction=300, k=100\n",
      "PCA Parameters: components=25, applied_to_layers=None\n",
      "\n",
      "Build Times:\n",
      "Standard Build: 2307.32s\n",
      "PCA Build (Index): 2317.24s\n",
      "PCA Build (Finalize): 0.00s\n",
      "Total PCA Build: 2317.24s\n",
      "\n",
      "Standard: recall@100=0.6762, total_query_time=238.64s, avg_query_time=5.90ms\n",
      "PCA-opt:  recall@100=0.6546, total_query_time=227.66s, avg_query_time=5.64ms\n",
      "\n",
      "--- Comparison Summary ---\n",
      "Method         | Recall@100 | TotalQuery(s) | AvgQuery(ms)\n",
      "---------------|------------|---------------|-------------\n",
      "Standard       | 0.6762     | 238.64        | 5.90       \n",
      "PCA-optimized  | 0.6546     | 227.66        | 5.64       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from tqdm import tqdm\n",
    "import logging # Make sure logging is configured\n",
    "\n",
    "# Configure logging if not already done\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Assume 'vectors' is pre-loaded (e.g., from GloVe)\n",
    "# Example Placeholder:\n",
    "# NUM_ELEMENTS = 400000\n",
    "# DIMENSION = 100\n",
    "# vectors = np.random.rand(NUM_ELEMENTS, DIMENSION).astype(np.float32)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- parameters ---\n",
    "    num_vectors = vectors.shape[0] # Use actual shape\n",
    "    num_queries = 40_000\n",
    "    dim = vectors.shape[1]         # Use actual shape\n",
    "    k = 100\n",
    "    M = 20\n",
    "    ef_construction = 300\n",
    "    # pca_top_layers REMOVED from parameters here\n",
    "    pca_components = 25 # Example: Use a less aggressive value\n",
    "\n",
    "    # load or generate data\n",
    "    print(\"Selecting query data...\")\n",
    "    # Ensure num_queries isn't larger than num_vectors\n",
    "    actual_num_queries = min(num_queries, num_vectors)\n",
    "    query_indices = np.random.choice(num_vectors, actual_num_queries, replace=False)\n",
    "    queries = vectors[query_indices]\n",
    "    print(f\"Using {actual_num_queries} queries.\")\n",
    "\n",
    "\n",
    "    # compute ground truth once\n",
    "    print(\"Computing ground truth...\")\n",
    "    t_gt0 = time.perf_counter()\n",
    "    nn = NearestNeighbors(n_neighbors=k, algorithm='brute', metric='euclidean')\n",
    "    nn.fit(vectors) # Use the full dataset 'vectors'\n",
    "    gt_d, gt_i = nn.kneighbors(queries)\n",
    "    t_gt1 = time.perf_counter()\n",
    "    print(f\"Ground truth computed in {t_gt1 - t_gt0:.2f}s\")\n",
    "\n",
    "\n",
    "    # --- Build standard (non-PCA) index ---\n",
    "    print(\"\\nBuilding standard HNSW index (no PCA)...\")\n",
    "    idx_std = optHNSWPCA(\n",
    "        dim, num_vectors + 10, M, ef_construction, # Add buffer to max_elements\n",
    "        pca_yes=False,\n",
    "        # No pca_top_layers argument needed\n",
    "        pca_components=pca_components # Pass components even if pca_yes=False (ignored)\n",
    "    )\n",
    "    t0 = time.perf_counter()\n",
    "    for v in tqdm(vectors, desc=\"Indexing standard HNSW\"): # Iterate over 'vectors'\n",
    "        idx_std.insert(v)\n",
    "    build_std = time.perf_counter() - t0\n",
    "    print(f\"Standard build time: {build_std:.2f}s\")\n",
    "    print(f\"Standard index height: {idx_std.entry_point_level}\")\n",
    "\n",
    "\n",
    "    # --- Build PCA-enabled index ---\n",
    "    print(\"\\nBuilding PCA-enabled HNSW index...\")\n",
    "    idx_pca = optHNSWPCA(\n",
    "        dim, num_vectors + 10, M, ef_construction, # Add buffer\n",
    "        pca_yes=True,\n",
    "        # No pca_top_layers argument needed\n",
    "        pca_components=pca_components # Specify desired components\n",
    "    )\n",
    "    t0 = time.perf_counter()\n",
    "    # Build the PCA index using the *same* data\n",
    "    for v in tqdm(vectors, desc=\"Indexing PCA-enabled HNSW\"): # Iterate over 'vectors'\n",
    "        idx_pca.insert(v)\n",
    "    build_pca = time.perf_counter() - t0\n",
    "    print(f\"PCA-enabled index build time (before finalize): {build_pca:.2f}s\")\n",
    "    print(f\"PCA-enabled index height: {idx_pca.entry_point_level}\")\n",
    "\n",
    "\n",
    "    # finalize PCA on PCA-enabled index\n",
    "    print(\"\\nFinalizing PCA models on index...\")\n",
    "    t0 = time.perf_counter()\n",
    "    idx_pca.finalize_pca() # This now applies PCA only to layers >= 2\n",
    "    fit_pca = time.perf_counter() - t0\n",
    "    print(f\"PCA finalize time: {fit_pca:.2f}s\")\n",
    "\n",
    "    # Precompute PCA projections for queries for the PCA-enabled index\n",
    "    print(\"\\nPrecomputing PCA projections for queries...\")\n",
    "    t0 = time.perf_counter()\n",
    "    # Ensure pca_models and pca_applied_layers exist before calling\n",
    "    pca_q = []\n",
    "    if idx_pca.pca_yes and idx_pca.pca_applied_layers:\n",
    "         pca_q = precompute_all_query_pcas(idx_pca.pca_models, idx_pca.pca_applied_layers, queries)\n",
    "    else:\n",
    "         print(\"Skipping query PCA precomputation as PCA was not applied to any layers.\")\n",
    "    t_pca_q = time.perf_counter()-t0\n",
    "    print(f\"Query PCA precomputation time: {t_pca_q:.4f}s\")\n",
    "\n",
    "\n",
    "    # helper for recall calculation\n",
    "    def recall_at_k(ground_truth_indices, predicted_indices, k_val):\n",
    "        \"\"\" Calculates recall for a single query \"\"\"\n",
    "        # Ensure predicted_indices is iterable and not None\n",
    "        if predicted_indices is None:\n",
    "             predicted_indices = []\n",
    "        return len(set(ground_truth_indices[:k_val]) & set(predicted_indices[:k_val])) / k_val\n",
    "\n",
    "    # --- Evaluate standard search ---\n",
    "    print(\"\\nEvaluating standard search...\")\n",
    "    results_std = [None] * actual_num_queries\n",
    "    times_std = []\n",
    "    total_r_std = 0\n",
    "    tstart_query_std = time.perf_counter()\n",
    "    for i, q in enumerate(tqdm(queries, desc=\"Standard query\")):\n",
    "        ts = time.perf_counter()\n",
    "        results_std[i] = idx_std.search(q, k) # Store results\n",
    "        te = time.perf_counter()\n",
    "        times_std.append(te - ts)\n",
    "        total_r_std += recall_at_k(gt_i[i], results_std[i], k) # Use the helper\n",
    "    tend_query_std = time.perf_counter()\n",
    "    tot_time_std = tend_query_std - tstart_query_std\n",
    "    avg_time_std = np.mean(times_std) * 1000 if times_std else 0\n",
    "    recall_std = total_r_std / actual_num_queries if actual_num_queries > 0 else 0\n",
    "\n",
    "\n",
    "    # --- Evaluate PCA-optimized search ---\n",
    "    print(\"\\nEvaluating PCA-optimized search...\")\n",
    "    results_pca = [None] * actual_num_queries\n",
    "    times_pca = []\n",
    "    total_r_pca = 0\n",
    "    tstart_query_pca = time.perf_counter()\n",
    "    for i, q in enumerate(tqdm(queries, desc=\"PCA-opt query\")):\n",
    "        ts = time.perf_counter()\n",
    "        # Call search_pca_opt, passing the query index 'i' and original query 'q'\n",
    "        results_pca[i] = idx_pca.search_pca_opt(i, q, pca_q, k) # Store results\n",
    "        te = time.perf_counter()\n",
    "        times_pca.append(te - ts)\n",
    "        # Calculate recall using ground truth for query i and predictions for query i\n",
    "        total_r_pca += recall_at_k(gt_i[i], results_pca[i], k) # Use the helper\n",
    "    tend_query_pca = time.perf_counter()\n",
    "    tot_time_pca = tend_query_pca - tstart_query_pca\n",
    "    avg_time_pca = np.mean(times_pca) * 1000 if times_pca else 0\n",
    "    recall_pca = total_r_pca / actual_num_queries if actual_num_queries > 0 else 0\n",
    "\n",
    "\n",
    "    # --- Output results ---\n",
    "    print(\"\\n--- Results ---\")\n",
    "    print(f\"Parameters: M={M}, ef_construction={ef_construction}, k={k}\")\n",
    "    if idx_pca.pca_yes:\n",
    "        print(f\"PCA Parameters: components={pca_components}, applied_to_layers={sorted(list(idx_pca.pca_applied_layers)) if idx_pca.pca_applied_layers else 'None'}\")\n",
    "    else:\n",
    "         print(\"PCA Parameters: PCA Disabled\")\n",
    "\n",
    "    print(\"\\nBuild Times:\")\n",
    "    print(f\"Standard Build: {build_std:.2f}s\")\n",
    "    print(f\"PCA Build (Index): {build_pca:.2f}s\")\n",
    "    print(f\"PCA Build (Finalize): {fit_pca:.2f}s\")\n",
    "    print(f\"Total PCA Build: {build_pca + fit_pca:.2f}s\")\n",
    "\n",
    "\n",
    "    print(f\"\\nStandard: recall@{k}={recall_std:.4f}, total_query_time={tot_time_std:.2f}s, avg_query_time={avg_time_std:.2f}ms\")\n",
    "    print(f\"PCA-opt:  recall@{k}={recall_pca:.4f}, total_query_time={tot_time_pca:.2f}s, avg_query_time={avg_time_pca:.2f}ms\")\n",
    "\n",
    "    print(\"\\n--- Comparison Summary ---\")\n",
    "    print(f\"Method         | Recall@{k} | TotalQuery(s) | AvgQuery(ms)\")\n",
    "    print(f\"---------------|------------|---------------|-------------\")\n",
    "    print(f\"Standard       | {recall_std:<10.4f} | {tot_time_std:<13.2f} | {avg_time_std:<11.2f}\")\n",
    "    # Only show PCA line if PCA was actually attempted\n",
    "    if idx_pca.pca_yes:\n",
    "        print(f\"PCA-optimized  | {recall_pca:<10.4f} | {tot_time_pca:<13.2f} | {avg_time_pca:<11.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating standard search…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Standard query:   0%|          | 0/40000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "recall_at_k() missing 1 required positional argument: 'k_val'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m     te \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m     12\u001b[0m     times_std\u001b[38;5;241m.\u001b[39mappend(te \u001b[38;5;241m-\u001b[39m ts)\n\u001b[0;32m---> 13\u001b[0m     total_r_std \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mrecall_at_k\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgt_i\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m tot_time_std \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m tstart\n\u001b[1;32m     15\u001b[0m avg_time_std \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(times_std) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: recall_at_k() missing 1 required positional argument: 'k_val'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "\n",
    "    # evaluate standard search\n",
    "    print(\"Evaluating standard search…\")\n",
    "    total_r_std, times_std = 0, []\n",
    "    tstart = time.perf_counter()\n",
    "    for i, q in enumerate(tqdm(queries, desc=\"Standard query\")):\n",
    "        ts = time.perf_counter()\n",
    "        out = idx_std.search(q, k)\n",
    "        te = time.perf_counter()\n",
    "        times_std.append(te - ts)\n",
    "        total_r_std += recall_at_k(gt_i[i], out)\n",
    "    tot_time_std = time.perf_counter() - tstart\n",
    "    avg_time_std = np.mean(times_std) * 1000\n",
    "    recall_std = total_r_std / num_queries\n",
    "\n",
    "    # evaluate PCA-optimized search\n",
    "    print(\"Evaluating PCA-optimized search…\")\n",
    "    total_r_pca, times_pca = 0, []\n",
    "    tstart = time.perf_counter()\n",
    "    for i, q in enumerate(tqdm(queries, desc=\"PCA-opt query\")):\n",
    "        ts = time.perf_counter()\n",
    "        out = idx_pca.search_pca_opt(i, q, pca_q, k)\n",
    "        te = time.perf_counter()\n",
    "        times_pca.append(te - ts)\n",
    "        total_r_pca += recall_at_k(gt_i[i], out)\n",
    "    tot_time_pca = time.perf_counter() - tstart\n",
    "    avg_time_pca = np.mean(times_pca) * 1000\n",
    "    recall_pca = total_r_pca / num_queries\n",
    "\n",
    "    # output results\n",
    "    print(f\"Standard: recall={recall_std:.4f}, total_query_time={tot_time_std:.2f}s, avg_query_time={avg_time_std:.2f}ms\")\n",
    "    print(f\"PCA-opt: recall={recall_pca:.4f}, total_query_time={tot_time_pca:.2f}s, avg_query_time={avg_time_pca:.2f}ms\")\n",
    "\n",
    "    print(\"\\n--- Comparison Summary ---\")\n",
    "    print(f\"Method         | Recall@{k} | TotalQuery(s) | AvgQuery(ms)\")\n",
    "    print(f\"Standard       | {recall_std:.4f}    | {tot_time_std:.2f}       | {avg_time_std:.2f}\")\n",
    "    print(f\"PCA-optimized  | {recall_pca:.4f}    | {tot_time_pca:.2f}       | {avg_time_pca:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Height of HNSW graph: 5\n"
     ]
    }
   ],
   "source": [
    "print(\"Height of HNSW graph:\", idx_pca.entry_point_level)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
